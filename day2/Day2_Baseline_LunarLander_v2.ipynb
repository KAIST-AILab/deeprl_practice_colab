{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wP3KREMIG196",
    "colab_type": "text"
   },
   "source": [
    "# RLLAB setup scripts for google colab\n",
    "Install packages with compatible versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "2J5ROkw6Gcxq",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!apt-get -qq install -y xvfb python-opengl > /dev/null 2>&1\n",
    "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
    "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tvacEH-tGtI1",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89.0
    },
    "outputId": "64c043d9-39e0-4595-bd2d-db36060b51e3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.547627398571E12,
     "user_tz": -540.0,
     "elapsed": 64075.0,
     "user": {
      "displayName": "Haanvid Lee",
      "photoUrl": "",
      "userId": "02061776945051468424"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mstable-baselines 2.2.1 has requirement gym[atari,classic_control]>=0.10.9, but you'll have gym 0.7.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mdopamine-rl 1.0.5 has requirement gym>=0.10.5, but you'll have gym 0.7.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mpymc3 3.6 has requirement joblib<0.13.0, but you'll have joblib 0.13.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mpymc3 3.6 has requirement theano>=1.0.0, but you'll have theano 0.8.2 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q path.py\n",
    "!pip install -q pyprind\n",
    "!pip install -q cached_property\n",
    "!pip install -q gym==0.7.4\n",
    "!pip install -q theano==0.8.2\n",
    "!pip install -q git+https://github.com/neocxi/Lasagne.git@484866cf8b38d878e92d521be445968531646bb8#egg=Lasagne\n",
    "  \n",
    "!pip install -q PyOpenGL piglet pyglet pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TWKA_0xQHGAS",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469.0
    },
    "outputId": "e2e693ca-3910-41ee-9fbd-65d3d1245628",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.547627407153E12,
     "user_tz": -540.0,
     "elapsed": 72654.0,
     "user": {
      "displayName": "Haanvid Lee",
      "photoUrl": "",
      "userId": "02061776945051468424"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting box2d-py\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
      "\u001b[K    100% |████████████████████████████████| 450kB 14.5MB/s \n",
      "\u001b[?25hCollecting mako==1.0.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/f3/67579bb486517c0d49547f9697e36582cd19dafb5df9e687ed8e22de57fa/Mako-1.0.7.tar.gz (564kB)\n",
      "\u001b[K    100% |████████████████████████████████| 573kB 30.0MB/s \n",
      "\u001b[?25hCollecting Pygame\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/5e/fb7c85304ad1fd52008fd25fce97a7f59e6147ae97378afc86cf0f5d9146/pygame-1.9.4-cp36-cp36m-manylinux1_x86_64.whl (12.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.1MB 3.0MB/s \n",
      "\u001b[?25hCollecting JSAnimation\n",
      "  Downloading https://files.pythonhosted.org/packages/3c/e6/a93a578400c38a43af8b4271334ed2444b42d65580f1d6721c9fe32e9fd8/JSAnimation-0.1.tar.gz\n",
      "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako==1.0.7) (1.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.14.6)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (4.0.0)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->imageio) (0.46)\n",
      "Building wheels for collected packages: mako, JSAnimation\n",
      "  Running setup.py bdist_wheel for mako ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/15/35/25/dbcb848832ccb1a4b4ad23f529badfd3bce9bf88017f7ca510\n",
      "  Running setup.py bdist_wheel for JSAnimation ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3c/c2/b2/b444dffc3eed9c78139288d301c4009a42c0dd061d3b62cead\n",
      "Successfully built mako JSAnimation\n",
      "Installing collected packages: box2d-py, mako, Pygame, JSAnimation\n",
      "Successfully installed JSAnimation-0.1 Pygame-1.9.4 box2d-py-2.3.8 mako-1.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install box2d-py mako==1.0.7 Pygame JSAnimation imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iDxIzxg9HJhp",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143.0
    },
    "outputId": "ebd1f620-519e-4c86-8df9-0f8eb6daf90d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.547627409302E12,
     "user_tz": -540.0,
     "elapsed": 74802.0,
     "user": {
      "displayName": "Haanvid Lee",
      "photoUrl": "",
      "userId": "02061776945051468424"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'rllab-git'...\n",
      "remote: Enumerating objects: 45, done.\u001b[K\n",
      "remote: Counting objects:   2% (1/45)   \u001b[K\rremote: Counting objects:   4% (2/45)   \u001b[K\rremote: Counting objects:   6% (3/45)   \u001b[K\rremote: Counting objects:   8% (4/45)   \u001b[K\rremote: Counting objects:  11% (5/45)   \u001b[K\rremote: Counting objects:  13% (6/45)   \u001b[K\rremote: Counting objects:  15% (7/45)   \u001b[K\rremote: Counting objects:  17% (8/45)   \u001b[K\rremote: Counting objects:  20% (9/45)   \u001b[K\rremote: Counting objects:  22% (10/45)   \u001b[K\rremote: Counting objects:  24% (11/45)   \u001b[K\rremote: Counting objects:  26% (12/45)   \u001b[K\rremote: Counting objects:  28% (13/45)   \u001b[K\rremote: Counting objects:  31% (14/45)   \u001b[K\rremote: Counting objects:  33% (15/45)   \u001b[K\rremote: Counting objects:  35% (16/45)   \u001b[K\rremote: Counting objects:  37% (17/45)   \u001b[K\rremote: Counting objects:  40% (18/45)   \u001b[K\rremote: Counting objects:  42% (19/45)   \u001b[K\rremote: Counting objects:  44% (20/45)   \u001b[K\rremote: Counting objects:  46% (21/45)   \u001b[K\rremote: Counting objects:  48% (22/45)   \u001b[K\rremote: Counting objects:  51% (23/45)   \u001b[K\rremote: Counting objects:  53% (24/45)   \u001b[K\rremote: Counting objects:  55% (25/45)   \u001b[K\rremote: Counting objects:  57% (26/45)   \u001b[K\rremote: Counting objects:  60% (27/45)   \u001b[K\rremote: Counting objects:  62% (28/45)   \u001b[K\rremote: Counting objects:  64% (29/45)   \u001b[K\rremote: Counting objects:  66% (30/45)   \u001b[K\rremote: Counting objects:  68% (31/45)   \u001b[K\rremote: Counting objects:  71% (32/45)   \u001b[K\rremote: Counting objects:  73% (33/45)   \u001b[K\rremote: Counting objects:  75% (34/45)   \u001b[K\rremote: Counting objects:  77% (35/45)   \u001b[K\rremote: Counting objects:  80% (36/45)   \u001b[K\rremote: Counting objects:  82% (37/45)   \u001b[K\rremote: Counting objects:  84% (38/45)   \u001b[K\rremote: Counting objects:  86% (39/45)   \u001b[K\rremote: Counting objects:  88% (40/45)   \u001b[K\rremote: Counting objects:  91% (41/45)   \u001b[K\rremote: Counting objects:  93% (42/45)   \u001b[K\rremote: Counting objects:  95% (43/45)   \u001b[K\rremote: Counting objects:  97% (44/45)   \u001b[K\rremote: Counting objects: 100% (45/45)   \u001b[K\rremote: Counting objects: 100% (45/45), done.\u001b[K\n",
      "remote: Compressing objects:   2% (1/36)   \u001b[K\rremote: Compressing objects:   5% (2/36)   \u001b[K\rremote: Compressing objects:   8% (3/36)   \u001b[K\rremote: Compressing objects:  11% (4/36)   \u001b[K\rremote: Compressing objects:  13% (5/36)   \u001b[K\rremote: Compressing objects:  16% (6/36)   \u001b[K\rremote: Compressing objects:  19% (7/36)   \u001b[K\rremote: Compressing objects:  22% (8/36)   \u001b[K\rremote: Compressing objects:  25% (9/36)   \u001b[K\rremote: Compressing objects:  27% (10/36)   \u001b[K\rremote: Compressing objects:  30% (11/36)   \u001b[K\rremote: Compressing objects:  33% (12/36)   \u001b[K\rremote: Compressing objects:  36% (13/36)   \u001b[K\rremote: Compressing objects:  38% (14/36)   \u001b[K\rremote: Compressing objects:  41% (15/36)   \u001b[K\rremote: Compressing objects:  44% (16/36)   \u001b[K\rremote: Compressing objects:  47% (17/36)   \u001b[K\rremote: Compressing objects:  50% (18/36)   \u001b[K\rremote: Compressing objects:  52% (19/36)   \u001b[K\rremote: Compressing objects:  55% (20/36)   \u001b[K\rremote: Compressing objects:  58% (21/36)   \u001b[K\rremote: Compressing objects:  61% (22/36)   \u001b[K\rremote: Compressing objects:  63% (23/36)   \u001b[K\rremote: Compressing objects:  66% (24/36)   \u001b[K\rremote: Compressing objects:  69% (25/36)   \u001b[K\rremote: Compressing objects:  72% (26/36)   \u001b[K\rremote: Compressing objects:  75% (27/36)   \u001b[K\rremote: Compressing objects:  77% (28/36)   \u001b[K\rremote: Compressing objects:  80% (29/36)   \u001b[K\rremote: Compressing objects:  83% (30/36)   \u001b[K\rremote: Compressing objects:  86% (31/36)   \u001b[K\rremote: Compressing objects:  88% (32/36)   \u001b[K\rremote: Compressing objects:  91% (33/36)   \u001b[K\rremote: Compressing objects:  94% (34/36)   \u001b[K\rremote: Compressing objects:  97% (35/36)   \u001b[K\rremote: Compressing objects: 100% (36/36)   \u001b[K\rremote: Compressing objects: 100% (36/36), done.\u001b[K\n",
      "Receiving objects:   0% (1/1440)   \rReceiving objects:   1% (15/1440)   \rReceiving objects:   2% (29/1440)   \rReceiving objects:   3% (44/1440)   \rReceiving objects:   4% (58/1440)   \rReceiving objects:   5% (72/1440)   \rReceiving objects:   6% (87/1440)   \rReceiving objects:   7% (101/1440)   \rReceiving objects:   8% (116/1440)   \rReceiving objects:   9% (130/1440)   \rReceiving objects:  10% (144/1440)   \rReceiving objects:  11% (159/1440)   \rReceiving objects:  12% (173/1440)   \rReceiving objects:  13% (188/1440)   \rReceiving objects:  14% (202/1440)   \rReceiving objects:  15% (216/1440)   \rReceiving objects:  16% (231/1440)   \rReceiving objects:  17% (245/1440)   \rReceiving objects:  18% (260/1440)   \rReceiving objects:  19% (274/1440)   \rReceiving objects:  20% (288/1440)   \rReceiving objects:  21% (303/1440)   \rReceiving objects:  22% (317/1440)   \rReceiving objects:  23% (332/1440)   \rReceiving objects:  24% (346/1440)   \rReceiving objects:  25% (360/1440)   \rReceiving objects:  26% (375/1440)   \rReceiving objects:  27% (389/1440)   \rReceiving objects:  28% (404/1440)   \rReceiving objects:  29% (418/1440)   \rReceiving objects:  30% (432/1440)   \rReceiving objects:  31% (447/1440)   \rReceiving objects:  32% (461/1440)   \rReceiving objects:  33% (476/1440)   \rReceiving objects:  34% (490/1440)   \rReceiving objects:  35% (504/1440)   \rReceiving objects:  36% (519/1440)   \rReceiving objects:  37% (533/1440)   \rReceiving objects:  38% (548/1440)   \rReceiving objects:  39% (562/1440)   \rReceiving objects:  40% (576/1440)   \rReceiving objects:  41% (591/1440)   \rReceiving objects:  42% (605/1440)   \rReceiving objects:  43% (620/1440)   \rReceiving objects:  44% (634/1440)   \rReceiving objects:  45% (648/1440)   \rReceiving objects:  46% (663/1440)   \rReceiving objects:  47% (677/1440)   \rReceiving objects:  48% (692/1440)   \rReceiving objects:  49% (706/1440)   \rReceiving objects:  50% (720/1440)   \rReceiving objects:  51% (735/1440)   \rReceiving objects:  52% (749/1440)   \rReceiving objects:  53% (764/1440)   \rReceiving objects:  54% (778/1440)   \rReceiving objects:  55% (792/1440)   \rReceiving objects:  56% (807/1440)   \rReceiving objects:  57% (821/1440)   \rReceiving objects:  58% (836/1440)   \rReceiving objects:  59% (850/1440)   \rReceiving objects:  60% (864/1440)   \rReceiving objects:  61% (879/1440)   \rReceiving objects:  62% (893/1440)   \rReceiving objects:  63% (908/1440)   \rReceiving objects:  64% (922/1440)   \rReceiving objects:  65% (936/1440)   \rReceiving objects:  66% (951/1440)   \rReceiving objects:  67% (965/1440)   \rReceiving objects:  68% (980/1440)   \rReceiving objects:  69% (994/1440)   \rReceiving objects:  70% (1008/1440)   \rReceiving objects:  71% (1023/1440)   \rReceiving objects:  72% (1037/1440)   \rReceiving objects:  73% (1052/1440)   \rReceiving objects:  74% (1066/1440)   \rReceiving objects:  75% (1080/1440)   \rReceiving objects:  76% (1095/1440)   \rReceiving objects:  77% (1109/1440)   \rReceiving objects:  78% (1124/1440)   \rReceiving objects:  79% (1138/1440)   \rReceiving objects:  80% (1152/1440)   \rReceiving objects:  81% (1167/1440)   \rReceiving objects:  82% (1181/1440)   \rReceiving objects:  83% (1196/1440)   \rReceiving objects:  84% (1210/1440)   \rReceiving objects:  85% (1224/1440)   \rReceiving objects:  86% (1239/1440)   \rReceiving objects:  87% (1253/1440)   \rReceiving objects:  88% (1268/1440)   \rReceiving objects:  89% (1282/1440)   \rReceiving objects:  90% (1296/1440)   \rReceiving objects:  91% (1311/1440)   \rReceiving objects:  92% (1325/1440)   \rReceiving objects:  93% (1340/1440)   \rReceiving objects:  94% (1354/1440)   \rReceiving objects:  95% (1368/1440)   \rReceiving objects:  96% (1383/1440)   \rremote: Total 1440 (delta 9), reused 32 (delta 7), pack-reused 1395\u001b[K\n",
      "Receiving objects:  97% (1397/1440)   \rReceiving objects:  98% (1412/1440)   \rReceiving objects:  99% (1426/1440)   \rReceiving objects: 100% (1440/1440)   \rReceiving objects: 100% (1440/1440), 1.55 MiB | 8.55 MiB/s, done.\n",
      "Resolving deltas:   0% (0/769)   \rResolving deltas:   1% (11/769)   \rResolving deltas:   2% (16/769)   \rResolving deltas:   3% (28/769)   \rResolving deltas:   4% (33/769)   \rResolving deltas:   5% (39/769)   \rResolving deltas:   6% (47/769)   \rResolving deltas:   7% (56/769)   \rResolving deltas:   8% (65/769)   \rResolving deltas:   9% (70/769)   \rResolving deltas:  10% (79/769)   \rResolving deltas:  11% (85/769)   \rResolving deltas:  12% (98/769)   \rResolving deltas:  13% (102/769)   \rResolving deltas:  14% (110/769)   \rResolving deltas:  15% (118/769)   \rResolving deltas:  16% (124/769)   \rResolving deltas:  18% (139/769)   \rResolving deltas:  19% (150/769)   \rResolving deltas:  20% (155/769)   \rResolving deltas:  21% (162/769)   \rResolving deltas:  22% (170/769)   \rResolving deltas:  23% (177/769)   \rResolving deltas:  24% (185/769)   \rResolving deltas:  25% (193/769)   \rResolving deltas:  26% (200/769)   \rResolving deltas:  27% (208/769)   \rResolving deltas:  28% (217/769)   \rResolving deltas:  29% (230/769)   \rResolving deltas:  30% (234/769)   \rResolving deltas:  31% (239/769)   \rResolving deltas:  33% (260/769)   \rResolving deltas:  34% (263/769)   \rResolving deltas:  35% (274/769)   \rResolving deltas:  36% (278/769)   \rResolving deltas:  37% (286/769)   \rResolving deltas:  38% (293/769)   \rResolving deltas:  39% (301/769)   \rResolving deltas:  40% (309/769)   \rResolving deltas:  41% (316/769)   \rResolving deltas:  42% (323/769)   \rResolving deltas:  43% (332/769)   \rResolving deltas:  44% (341/769)   \rResolving deltas:  45% (347/769)   \rResolving deltas:  46% (354/769)   \rResolving deltas:  47% (362/769)   \rResolving deltas:  48% (370/769)   \rResolving deltas:  49% (377/769)   \rResolving deltas:  50% (385/769)   \rResolving deltas:  51% (395/769)   \rResolving deltas:  52% (405/769)   \rResolving deltas:  53% (408/769)   \rResolving deltas:  54% (417/769)   \rResolving deltas:  58% (450/769)   \rResolving deltas:  59% (457/769)   \rResolving deltas:  60% (465/769)   \rResolving deltas:  61% (470/769)   \rResolving deltas:  62% (480/769)   \rResolving deltas:  63% (487/769)   \rResolving deltas:  64% (495/769)   \rResolving deltas:  65% (502/769)   \rResolving deltas:  66% (515/769)   \rResolving deltas:  68% (530/769)   \rResolving deltas:  71% (553/769)   \rResolving deltas:  72% (554/769)   \rResolving deltas:  73% (567/769)   \rResolving deltas:  74% (571/769)   \rResolving deltas:  75% (581/769)   \rResolving deltas:  78% (602/769)   \rResolving deltas:  82% (633/769)   \rResolving deltas:  83% (642/769)   \rResolving deltas:  86% (665/769)   \rResolving deltas:  88% (678/769)   \rResolving deltas:  91% (703/769)   \rResolving deltas:  92% (709/769)   \rResolving deltas:  93% (718/769)   \rResolving deltas:  94% (728/769)   \rResolving deltas:  95% (734/769)   \rResolving deltas:  96% (740/769)   \rResolving deltas:  97% (751/769)   \rResolving deltas:  98% (754/769)   \rResolving deltas:  99% (762/769)   \rResolving deltas: 100% (769/769)   \rResolving deltas: 100% (769/769), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/kekim/rllab.git rllab-git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Hq4UosxRHMkq",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!cp -a ./rllab-git/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUHGgASMNg2N",
    "colab_type": "text"
   },
   "source": [
    "# Baseline on LunarLanderContinous-v2 (OpenAI Gym version)\n",
    "**Important!**\n",
    "Before running the following cell, make sure rllab is set up properly in your current runtime by executing codes in RLLAB setup scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XA0g5MBZHQ3o",
    "colab_type": "text"
   },
   "source": [
    "**1. Implement Baseline Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJcqSePquDny",
    "colab_type": "text"
   },
   "source": [
    "- Import necessary packages\n",
    "(Execute **once again** if you encounter an error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "C49pvNj5HooC",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from rllab.envs.gym_env import GymEnv\n",
    "from rllab.baselines.linear_feature_baseline import LinearFeatureBaseline\n",
    "from rllab.policies.gaussian_mlp_policy import GaussianMLPPolicy\n",
    "from rllab.envs.normalized_env import normalize\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as TT\n",
    "from lasagne.updates import adam\n",
    "from rllab.misc.instrument import run_experiment_lite\n",
    "import rllab.misc.logger as logger\n",
    "\n",
    "########## Baseline_GYM_Lunvar_v2_With_RUN_EXP_LITE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThinxFhruY1K",
    "colab_type": "text"
   },
   "source": [
    "- Implement Baseline algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wpx46tkruXqt",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408.0
    },
    "outputId": "02cce16f-8459-45c1-8d70-387fa8cb8d82",
    "executionInfo": {
     "status": "error",
     "timestamp": 1.547627572049E12,
     "user_tz": -540.0,
     "elapsed": 13522.0,
     "user": {
      "displayName": "Haanvid Lee",
      "photoUrl": "",
      "userId": "02061776945051468424"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-16 08:32:38.880435 UTC | Warning: skipping Gym environment monitoring since snapshot_dir not configured.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-01-16 08:32:38,882] Making new env: LunarLanderContinuous-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-16 08:32:38.911229 UTC | observation space: Box(8,)\n",
      "2019-01-16 08:32:38.912513 UTC | action space: Box(2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
      "[2019-01-16 08:32:44,413] We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5dc0a9bfdb98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Get the list of trainable parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m f_train = theano.function(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'surr' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def Baseline(*_):\n",
    "\n",
    "    # normalize() makes sure that the actions for the environment lies\n",
    "    # within the range [-1, 1] (only works for environments with continuous actions)\n",
    "\n",
    "    # normalize() makes sure that the actions for the environment lies\n",
    "    # within the range [-1, 1] (only works for environments with continuous actions)\n",
    "    env = normalize(GymEnv(env_name = \"LunarLanderContinuous-v2\", force_reset=True))\n",
    "    # Initialize a neural network policy with a single hidden layer of 8 hidden units\n",
    "    policy = GaussianMLPPolicy(env.spec, hidden_sizes=(32,32))\n",
    "    # Initialize a linear baseline estimator using default hand-crafted features\n",
    "    baseline = LinearFeatureBaseline(env.spec)\n",
    "\n",
    "    # We will collect 100 trajectories per iteration\n",
    "    N = 3\n",
    "    # Each trajectory will have at most 100 time steps\n",
    "    T = 400\n",
    "    # Number of iterations\n",
    "    n_itr = 1000\n",
    "    # Set the discount factor for the problem\n",
    "    discount = 0.99\n",
    "    # Learning rate for the gradient update\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # Construct the computation graph\n",
    "\n",
    "    # Create a Theano variable for storing the observations\n",
    "    # We could have simply written `observations_var = TT.matrix('observations')` instead for this example. However,\n",
    "    # doing it in a slightly more abstract way allows us to delegate to the environment for handling the correct data\n",
    "    # type for the variable. For instance, for an environment with discrete observations, we might want to use integer\n",
    "    # types if the observations are represented as one-hot vectors.\n",
    "    observations_var = env.observation_space.new_tensor_variable(\n",
    "        'observations',\n",
    "        # It should have 1 extra dimension since we want to represent a list of observations\n",
    "        extra_dims=1\n",
    "    )\n",
    "    actions_var = env.action_space.new_tensor_variable(\n",
    "        'actions',\n",
    "        extra_dims=1\n",
    "    )\n",
    "    advantages_var = TT.vector('advantages')\n",
    "\n",
    "    # policy.dist_info_sym returns a dictionary, whose values are symbolic expressions for quantities related to the\n",
    "    # distribution of the actions. For a Gaussian policy, it contains the mean and (log) standard deviation.\n",
    "    dist_info_vars = policy.dist_info_sym(observations_var)\n",
    "\n",
    "    # policy.distribution returns a distribution object under rllab.distributions. It contains many utilities for computing\n",
    "    # distribution-related quantities, given the computed dist_info_vars. Below we use dist.log_likelihood_sym to compute\n",
    "    # the symbolic log-likelihood. For this example, the corresponding distribution is an instance of the class\n",
    "    # rllab.distributions.DiagonalGaussian\n",
    "    dist = policy.distribution\n",
    "\n",
    "    # Exp. #2, Prob.1: Define a proper loss function for Baseline\n",
    "    ###########################################################################\n",
    "    # Note that we negate the objective, since most optimizers assume a minimization problem\n",
    "\n",
    "\n",
    "    # Get the list of trainable parameters.\n",
    "    params = policy.get_params(trainable=True)\n",
    "    grads = theano.grad(surr, params)\n",
    "\n",
    "    f_train = theano.function(\n",
    "        inputs=[observations_var, actions_var, advantages_var],\n",
    "        outputs=None,\n",
    "        updates=adam(grads, params, learning_rate=learning_rate),\n",
    "        allow_input_downcast=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(n_itr):\n",
    "        logger.push_prefix('epoch #%d | ' % epoch)\n",
    "        logger.log(\"Training started\")\n",
    "        paths = []\n",
    "\n",
    "        for _ in range(N):\n",
    "            observations = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "\n",
    "            observation = env.reset()\n",
    "\n",
    "            for _ in range(T):\n",
    "                # policy.get_action() returns a pair of values. The second one returns a dictionary, whose values contains\n",
    "                # sufficient statistics for the action distribution. It should at least contain entries that would be\n",
    "                # returned by calling policy.dist_info(), which is the non-symbolic analog of policy.dist_info_sym().\n",
    "                # Storing these statistics is useful, e.g., when forming importance sampling ratios. In our case it is\n",
    "                # not needed.\n",
    "                action, _ = policy.get_action(observation)\n",
    "                # Recall that the last entry of the tuple stores diagnostic information about the environment. In our\n",
    "                # case it is not needed.\n",
    "                next_observation, reward, terminal, _ = env.step(action)\n",
    "                observations.append(observation)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                observation = next_observation\n",
    "                if terminal:\n",
    "                    # Finish rollout if terminal state reached\n",
    "                    break\n",
    "\n",
    "            # We need to compute the empirical return for each time step along the\n",
    "            # trajectory\n",
    "            path = dict(\n",
    "                observations=np.array(observations),\n",
    "                actions=np.array(actions),\n",
    "                rewards=np.array(rewards),\n",
    "            )\n",
    "            path_baseline = baseline.predict(path)\n",
    "\n",
    "            # Exp. #2, Prob.2: Calculate advantages and returns.\n",
    "            ###########################################################################\n",
    "            advantages = []\n",
    "            returns = []\n",
    "            return_so_far = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # The advantages are stored backwards in time, so we need to revert it\n",
    "\n",
    "\n",
    "            # And we need to do the same thing for the list of returns\n",
    "\n",
    "\n",
    "            # Normalizing advantages is known trick for reducing variance, similar to normalizing reward.\n",
    "            advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)\n",
    "\n",
    "            path[\"advantages\"] = advantages\n",
    "            path[\"returns\"] = returns\n",
    "\n",
    "            paths.append(path)\n",
    "\n",
    "        baseline.fit(paths)\n",
    "\n",
    "        #### Dim. of observation: [Sum of length of all traj, Dim(observation)] ####\n",
    "        observations = np.concatenate([p[\"observations\"] for p in paths])\n",
    "\n",
    "        #### Dim. of actions    : [Sum of length of all traj, Dim(action)]      ####\n",
    "        actions = np.concatenate([p[\"actions\"] for p in paths])\n",
    "\n",
    "        #### Dim. of advantages : [sSm of length of all traj, ]                 ####\n",
    "        advantages = np.concatenate([p[\"advantages\"] for p in paths])\n",
    "\n",
    "        f_train(observations, actions, advantages)\n",
    "        returns_to_check = [sum(p[\"rewards\"]) for p in paths]\n",
    "        print('Average Return:', np.mean(returns_to_check))\n",
    "\n",
    "        ############################################################################\n",
    "        logger.log(\"Training finished\")\n",
    "        logger.save_itr_params(epoch, params)\n",
    "        logger.dump_tabular(with_prefix=False)\n",
    "        logger.pop_prefix()\n",
    "\n",
    "\n",
    "        logger.record_tabular('Epoch', epoch)\n",
    "        logger.record_tabular('Steps', epoch*N*T)\n",
    "        logger.record_tabular('AverageReturn', np.mean(returns_to_check))\n",
    "        logger.record_tabular('StdReturn', np.std(returns_to_check))\n",
    "        logger.record_tabular('MaxReturn', np.max(returns_to_check))\n",
    "        logger.record_tabular('MinReturn', np.min(returns_to_check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xF6kAwo57hb6",
    "colab_type": "text"
   },
   "source": [
    "**2. Execute Your Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjHQm_6t7wK1",
    "colab_type": "text"
   },
   "source": [
    "- Activate a virtual display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "oIUuJMGv7u8O",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "import os\n",
    "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6yM_2LavPBR",
    "colab_type": "text"
   },
   "source": [
    "- Create & Run a RL task for Acrobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "74oGATzdIO9b",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "mypath='./log/vpg_lunar_baseline/'\n",
    "\n",
    "run_experiment_lite(\n",
    "    Baseline,\n",
    "    # Number of parallel workers for sampling\n",
    "    log_dir=mypath,\n",
    "    n_parallel=1,\n",
    "    # Only keep the snapshot parameters for the last iteration\n",
    "    snapshot_mode=\"last\",\n",
    "    # Specifies the seed for the experiment. If this is not provided, a random seed\n",
    "    # will be used\n",
    "    seed=1,\n",
    "    # plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdDOU5hwKlW4",
    "colab_type": "text"
   },
   "source": [
    "**3. Average Reward Plotting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ip1bXHMtvtWy",
    "colab_type": "text"
   },
   "source": [
    "- You can evaluate how your agent is being trained with reward it gets in every iteration. \n",
    "- Whenever you execute the code 'run_experiment_lite', it will generate a experiment directory.\n",
    "- (/content/log/vpg_lunar_baseline/)\n",
    "- Please update the value of '**mypath**' and specify your new experiment directory name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "RlIgTMPOKqHX",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import joblib\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "mypath = './log/vpg_lunar_baseline/'\n",
    "\n",
    "plots = []\n",
    "legends = []\n",
    "returns = []\n",
    "with open(osp.join(mypath, 'progress.csv'), 'rt') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if row['AverageReturn']:\n",
    "            returns.append(float(row['AverageReturn']))\n",
    "returns = np.array(returns)\n",
    "plots.append(plt.plot(returns)[0])\n",
    "legends.append('AverageReturn')\n",
    "plt.legend(plots, legends)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWDQUoPePLDx",
    "colab_type": "text"
   },
   "source": [
    "**4. Play Videos of your Agent Behavior**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhdxUQuozcRv",
    "colab_type": "text"
   },
   "source": [
    "- You can watch how your agent's behavior improves.\n",
    "- If you haven't update 'mypath' in the code above, you need to update it here.\n",
    "- (/content/log/vpg_lunar_baseline/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "TPjtJ3IjPY5A",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from IPython import display as pythondisplay\n",
    "# from pyvirtualdisplay import Display\n",
    "\n",
    "# from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from JSAnimation import IPython_display\n",
    "from IPython.display import HTML\n",
    "\n",
    "import imageio\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def plot_movie_js(image_array, filename):\n",
    "    dpi = 10.0\n",
    "    xpixels, ypixels = image_array[0].shape[0], image_array[0].shape[1]\n",
    "    fig = plt.figure(figsize=(ypixels/(dpi), xpixels/(dpi)), dpi=dpi)\n",
    "    fig.suptitle(filename, fontsize=160)\n",
    "    # fig.set_xlabel(filename, fontsize=160)\n",
    "    # fig.xlabel(filename, fontsize=160)\n",
    "    im = plt.figimage(image_array[0])\n",
    "\n",
    "    def animate(i):\n",
    "        im.set_array(image_array[i])\n",
    "        return (im,)\n",
    "    \n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(image_array))\n",
    "    pythondisplay.display(IPython_display.display_animation(anim))\n",
    "\n",
    "# mypath = './log/vpg_lunar_baseline/'\n",
    "mypath += 'gym_log/'\n",
    "mp4files = [f for f in listdir(mypath) if f.endswith(\".mp4\")]\n",
    "mp4files.sort()\n",
    "\n",
    "\n",
    "for filename in mp4files:\n",
    "    vid = imageio.get_reader(join(mypath, filename),  'ffmpeg')\n",
    "    # print(len(vid))\n",
    "    # print(vid.get_data(0).shape)\n",
    "\n",
    "    screenlist = []\n",
    "    for i in range(len(vid)):\n",
    "        image = vid.get_data(i)\n",
    "        screenlist.append(image)\n",
    "        # fig = plt.figure()\n",
    "        # fig.suptitle('image #{}'.format(i), fontsize=20)\n",
    "        # plt.imshow(image)\n",
    "\n",
    "    plot_movie_js(screenlist, filename)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Day2_Baseline_LunarLander_v2.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
