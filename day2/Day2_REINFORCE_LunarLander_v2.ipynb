{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day2_REINFORCE_LunarLander_v2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"wP3KREMIG196","colab_type":"text"},"cell_type":"markdown","source":["# RLLAB setup scripts for google colab\n","Install packages with compatible versions"]},{"metadata":{"id":"2J5ROkw6Gcxq","colab_type":"code","colab":{}},"cell_type":"code","source":["!apt-get -qq install -y xvfb python-opengl > /dev/null 2>&1\n","!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n","!apt-get -qq -y install xvfb freeglut3-dev ffmpeg > /dev/null"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tvacEH-tGtI1","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install -q path.py\n","!pip install -q pyprind\n","!pip install -q cached_property\n","!pip install -q gym==0.7.4\n","!pip install -q theano==0.8.2\n","!pip install -q git+https://github.com/neocxi/Lasagne.git@484866cf8b38d878e92d521be445968531646bb8#egg=Lasagne\n","  \n","!pip install -q PyOpenGL piglet pyglet pyvirtualdisplay"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TWKA_0xQHGAS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":179},"outputId":"62551ff6-f05a-422c-f605-7b61c54dadd7","executionInfo":{"status":"ok","timestamp":1547624595676,"user_tz":-540,"elapsed":18030,"user":{"displayName":"Haanvid Lee","photoUrl":"","userId":"02061776945051468424"}}},"cell_type":"code","source":["!pip install box2d-py mako==1.0.7 Pygame JSAnimation imageio"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: box2d-py in /usr/local/lib/python3.6/dist-packages (2.3.8)\n","Requirement already satisfied: mako==1.0.7 in /usr/local/lib/python3.6/dist-packages (1.0.7)\n","Requirement already satisfied: Pygame in /usr/local/lib/python3.6/dist-packages (1.9.4)\n","Requirement already satisfied: JSAnimation in /usr/local/lib/python3.6/dist-packages (0.1)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako==1.0.7) (1.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.14.6)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (4.0.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->imageio) (0.46)\n"],"name":"stdout"}]},{"metadata":{"id":"iDxIzxg9HJhp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":143},"outputId":"5ca96f59-12da-4a40-cf87-36c80a213ab7","executionInfo":{"status":"ok","timestamp":1547624352848,"user_tz":-540,"elapsed":77314,"user":{"displayName":"Haanvid Lee","photoUrl":"","userId":"02061776945051468424"}}},"cell_type":"code","source":["!git clone https://github.com/kekim/rllab.git rllab-git"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Cloning into 'rllab-git'...\n","remote: Enumerating objects: 45, done.\u001b[K\n","remote: Counting objects:   2% (1/45)   \u001b[K\rremote: Counting objects:   4% (2/45)   \u001b[K\rremote: Counting objects:   6% (3/45)   \u001b[K\rremote: Counting objects:   8% (4/45)   \u001b[K\rremote: Counting objects:  11% (5/45)   \u001b[K\rremote: Counting objects:  13% (6/45)   \u001b[K\rremote: Counting objects:  15% (7/45)   \u001b[K\rremote: Counting objects:  17% (8/45)   \u001b[K\rremote: Counting objects:  20% (9/45)   \u001b[K\rremote: Counting objects:  22% (10/45)   \u001b[K\rremote: Counting objects:  24% (11/45)   \u001b[K\rremote: Counting objects:  26% (12/45)   \u001b[K\rremote: Counting objects:  28% (13/45)   \u001b[K\rremote: Counting objects:  31% (14/45)   \u001b[K\rremote: Counting objects:  33% (15/45)   \u001b[K\rremote: Counting objects:  35% (16/45)   \u001b[K\rremote: Counting objects:  37% (17/45)   \u001b[K\rremote: Counting objects:  40% (18/45)   \u001b[K\rremote: Counting objects:  42% (19/45)   \u001b[K\rremote: Counting objects:  44% (20/45)   \u001b[K\rremote: Counting objects:  46% (21/45)   \u001b[K\rremote: Counting objects:  48% (22/45)   \u001b[K\rremote: Counting objects:  51% (23/45)   \u001b[K\rremote: Counting objects:  53% (24/45)   \u001b[K\rremote: Counting objects:  55% (25/45)   \u001b[K\rremote: Counting objects:  57% (26/45)   \u001b[K\rremote: Counting objects:  60% (27/45)   \u001b[K\rremote: Counting objects:  62% (28/45)   \u001b[K\rremote: Counting objects:  64% (29/45)   \u001b[K\rremote: Counting objects:  66% (30/45)   \u001b[K\rremote: Counting objects:  68% (31/45)   \u001b[K\rremote: Counting objects:  71% (32/45)   \u001b[K\rremote: Counting objects:  73% (33/45)   \u001b[K\rremote: Counting objects:  75% (34/45)   \u001b[K\rremote: Counting objects:  77% (35/45)   \u001b[K\rremote: Counting objects:  80% (36/45)   \u001b[K\rremote: Counting objects:  82% (37/45)   \u001b[K\rremote: Counting objects:  84% (38/45)   \u001b[K\rremote: Counting objects:  86% (39/45)   \u001b[K\rremote: Counting objects:  88% (40/45)   \u001b[K\rremote: Counting objects:  91% (41/45)   \u001b[K\rremote: Counting objects:  93% (42/45)   \u001b[K\rremote: Counting objects:  95% (43/45)   \u001b[K\rremote: Counting objects:  97% (44/45)   \u001b[K\rremote: Counting objects: 100% (45/45)   \u001b[K\rremote: Counting objects: 100% (45/45), done.\u001b[K\n","remote: Compressing objects:   2% (1/36)   \u001b[K\rremote: Compressing objects:   5% (2/36)   \u001b[K\rremote: Compressing objects:   8% (3/36)   \u001b[K\rremote: Compressing objects:  11% (4/36)   \u001b[K\rremote: Compressing objects:  13% (5/36)   \u001b[K\rremote: Compressing objects:  16% (6/36)   \u001b[K\rremote: Compressing objects:  19% (7/36)   \u001b[K\rremote: Compressing objects:  22% (8/36)   \u001b[K\rremote: Compressing objects:  25% (9/36)   \u001b[K\rremote: Compressing objects:  27% (10/36)   \u001b[K\rremote: Compressing objects:  30% (11/36)   \u001b[K\rremote: Compressing objects:  33% (12/36)   \u001b[K\rremote: Compressing objects:  36% (13/36)   \u001b[K\rremote: Compressing objects:  38% (14/36)   \u001b[K\rremote: Compressing objects:  41% (15/36)   \u001b[K\rremote: Compressing objects:  44% (16/36)   \u001b[K\rremote: Compressing objects:  47% (17/36)   \u001b[K\rremote: Compressing objects:  50% (18/36)   \u001b[K\rremote: Compressing objects:  52% (19/36)   \u001b[K\rremote: Compressing objects:  55% (20/36)   \u001b[K\rremote: Compressing objects:  58% (21/36)   \u001b[K\rremote: Compressing objects:  61% (22/36)   \u001b[K\rremote: Compressing objects:  63% (23/36)   \u001b[K\rremote: Compressing objects:  66% (24/36)   \u001b[K\rremote: Compressing objects:  69% (25/36)   \u001b[K\rremote: Compressing objects:  72% (26/36)   \u001b[K\rremote: Compressing objects:  75% (27/36)   \u001b[K\rremote: Compressing objects:  77% (28/36)   \u001b[K\rremote: Compressing objects:  80% (29/36)   \u001b[K\rremote: Compressing objects:  83% (30/36)   \u001b[K\rremote: Compressing objects:  86% (31/36)   \u001b[K\rremote: Compressing objects:  88% (32/36)   \u001b[K\rremote: Compressing objects:  91% (33/36)   \u001b[K\rremote: Compressing objects:  94% (34/36)   \u001b[K\rremote: Compressing objects:  97% (35/36)   \u001b[K\rremote: Compressing objects: 100% (36/36)   \u001b[K\rremote: Compressing objects: 100% (36/36), done.\u001b[K\n","Receiving objects:   0% (1/1440)   \rReceiving objects:   1% (15/1440)   \rReceiving objects:   2% (29/1440)   \rReceiving objects:   3% (44/1440)   \rReceiving objects:   4% (58/1440)   \rReceiving objects:   5% (72/1440)   \rReceiving objects:   6% (87/1440)   \rReceiving objects:   7% (101/1440)   \rReceiving objects:   8% (116/1440)   \rReceiving objects:   9% (130/1440)   \rReceiving objects:  10% (144/1440)   \rReceiving objects:  11% (159/1440)   \rReceiving objects:  12% (173/1440)   \rReceiving objects:  13% (188/1440)   \rReceiving objects:  14% (202/1440)   \rReceiving objects:  15% (216/1440)   \rReceiving objects:  16% (231/1440)   \rReceiving objects:  17% (245/1440)   \rReceiving objects:  18% (260/1440)   \rReceiving objects:  19% (274/1440)   \rReceiving objects:  20% (288/1440)   \rReceiving objects:  21% (303/1440)   \rReceiving objects:  22% (317/1440)   \rReceiving objects:  23% (332/1440)   \rReceiving objects:  24% (346/1440)   \rReceiving objects:  25% (360/1440)   \rReceiving objects:  26% (375/1440)   \rReceiving objects:  27% (389/1440)   \rReceiving objects:  28% (404/1440)   \rReceiving objects:  29% (418/1440)   \rReceiving objects:  30% (432/1440)   \rReceiving objects:  31% (447/1440)   \rReceiving objects:  32% (461/1440)   \rReceiving objects:  33% (476/1440)   \rReceiving objects:  34% (490/1440)   \rReceiving objects:  35% (504/1440)   \rReceiving objects:  36% (519/1440)   \rReceiving objects:  37% (533/1440)   \rReceiving objects:  38% (548/1440)   \rReceiving objects:  39% (562/1440)   \rReceiving objects:  40% (576/1440)   \rReceiving objects:  41% (591/1440)   \rReceiving objects:  42% (605/1440)   \rReceiving objects:  43% (620/1440)   \rReceiving objects:  44% (634/1440)   \rReceiving objects:  45% (648/1440)   \rReceiving objects:  46% (663/1440)   \rReceiving objects:  47% (677/1440)   \rReceiving objects:  48% (692/1440)   \rReceiving objects:  49% (706/1440)   \rReceiving objects:  50% (720/1440)   \rReceiving objects:  51% (735/1440)   \rReceiving objects:  52% (749/1440)   \rReceiving objects:  53% (764/1440)   \rReceiving objects:  54% (778/1440)   \rReceiving objects:  55% (792/1440)   \rReceiving objects:  56% (807/1440)   \rReceiving objects:  57% (821/1440)   \rReceiving objects:  58% (836/1440)   \rReceiving objects:  59% (850/1440)   \rReceiving objects:  60% (864/1440)   \rReceiving objects:  61% (879/1440)   \rReceiving objects:  62% (893/1440)   \rReceiving objects:  63% (908/1440)   \rReceiving objects:  64% (922/1440)   \rReceiving objects:  65% (936/1440)   \rReceiving objects:  66% (951/1440)   \rReceiving objects:  67% (965/1440)   \rReceiving objects:  68% (980/1440)   \rReceiving objects:  69% (994/1440)   \rReceiving objects:  70% (1008/1440)   \rReceiving objects:  71% (1023/1440)   \rReceiving objects:  72% (1037/1440)   \rReceiving objects:  73% (1052/1440)   \rReceiving objects:  74% (1066/1440)   \rReceiving objects:  75% (1080/1440)   \rReceiving objects:  76% (1095/1440)   \rReceiving objects:  77% (1109/1440)   \rReceiving objects:  78% (1124/1440)   \rReceiving objects:  79% (1138/1440)   \rReceiving objects:  80% (1152/1440)   \rReceiving objects:  81% (1167/1440)   \rReceiving objects:  82% (1181/1440)   \rReceiving objects:  83% (1196/1440)   \rReceiving objects:  84% (1210/1440)   \rReceiving objects:  85% (1224/1440)   \rReceiving objects:  86% (1239/1440)   \rReceiving objects:  87% (1253/1440)   \rReceiving objects:  88% (1268/1440)   \rReceiving objects:  89% (1282/1440)   \rReceiving objects:  90% (1296/1440)   \rReceiving objects:  91% (1311/1440)   \rremote: Total 1440 (delta 9), reused 32 (delta 7), pack-reused 1395\u001b[K\n","Receiving objects:  92% (1325/1440)   \rReceiving objects:  93% (1340/1440)   \rReceiving objects:  94% (1354/1440)   \rReceiving objects:  95% (1368/1440)   \rReceiving objects:  96% (1383/1440)   \rReceiving objects:  97% (1397/1440)   \rReceiving objects:  98% (1412/1440)   \rReceiving objects:  99% (1426/1440)   \rReceiving objects: 100% (1440/1440)   \rReceiving objects: 100% (1440/1440), 1.55 MiB | 18.48 MiB/s, done.\n","Resolving deltas:   0% (0/769)   \rResolving deltas:   1% (9/769)   \rResolving deltas:   2% (17/769)   \rResolving deltas:   3% (27/769)   \rResolving deltas:   4% (33/769)   \rResolving deltas:   5% (39/769)   \rResolving deltas:   6% (47/769)   \rResolving deltas:   7% (55/769)   \rResolving deltas:   8% (65/769)   \rResolving deltas:   9% (70/769)   \rResolving deltas:  10% (79/769)   \rResolving deltas:  11% (85/769)   \rResolving deltas:  12% (98/769)   \rResolving deltas:  13% (102/769)   \rResolving deltas:  14% (110/769)   \rResolving deltas:  15% (118/769)   \rResolving deltas:  16% (124/769)   \rResolving deltas:  18% (139/769)   \rResolving deltas:  19% (150/769)   \rResolving deltas:  20% (154/769)   \rResolving deltas:  21% (162/769)   \rResolving deltas:  22% (171/769)   \rResolving deltas:  23% (182/769)   \rResolving deltas:  24% (185/769)   \rResolving deltas:  25% (193/769)   \rResolving deltas:  26% (200/769)   \rResolving deltas:  27% (208/769)   \rResolving deltas:  28% (216/769)   \rResolving deltas:  30% (232/769)   \rResolving deltas:  31% (239/769)   \rResolving deltas:  33% (259/769)   \rResolving deltas:  34% (262/769)   \rResolving deltas:  35% (271/769)   \rResolving deltas:  36% (277/769)   \rResolving deltas:  37% (287/769)   \rResolving deltas:  38% (293/769)   \rResolving deltas:  39% (301/769)   \rResolving deltas:  40% (308/769)   \rResolving deltas:  41% (316/769)   \rResolving deltas:  42% (323/769)   \rResolving deltas:  43% (332/769)   \rResolving deltas:  44% (341/769)   \rResolving deltas:  45% (347/769)   \rResolving deltas:  46% (354/769)   \rResolving deltas:  47% (362/769)   \rResolving deltas:  48% (370/769)   \rResolving deltas:  49% (377/769)   \rResolving deltas:  50% (385/769)   \rResolving deltas:  51% (395/769)   \rResolving deltas:  52% (405/769)   \rResolving deltas:  53% (408/769)   \rResolving deltas:  54% (417/769)   \rResolving deltas:  58% (450/769)   \rResolving deltas:  59% (457/769)   \rResolving deltas:  60% (465/769)   \rResolving deltas:  61% (470/769)   \rResolving deltas:  62% (480/769)   \rResolving deltas:  63% (487/769)   \rResolving deltas:  64% (495/769)   \rResolving deltas:  65% (502/769)   \rResolving deltas:  66% (515/769)   \rResolving deltas:  68% (530/769)   \rResolving deltas:  71% (553/769)   \rResolving deltas:  72% (554/769)   \rResolving deltas:  73% (567/769)   \rResolving deltas:  74% (571/769)   \rResolving deltas:  75% (581/769)   \rResolving deltas:  78% (602/769)   \rResolving deltas:  82% (633/769)   \rResolving deltas:  83% (642/769)   \rResolving deltas:  86% (665/769)   \rResolving deltas:  88% (678/769)   \rResolving deltas:  91% (703/769)   \rResolving deltas:  92% (709/769)   \rResolving deltas:  93% (718/769)   \rResolving deltas:  94% (728/769)   \rResolving deltas:  95% (734/769)   \rResolving deltas:  96% (740/769)   \rResolving deltas:  97% (751/769)   \rResolving deltas:  98% (754/769)   \rResolving deltas:  99% (762/769)   \rResolving deltas: 100% (769/769)   \rResolving deltas: 100% (769/769), done.\n"],"name":"stdout"}]},{"metadata":{"id":"Hq4UosxRHMkq","colab_type":"code","colab":{}},"cell_type":"code","source":["!cp -a ./rllab-git/* ."],"execution_count":0,"outputs":[]},{"metadata":{"id":"DUHGgASMNg2N","colab_type":"text"},"cell_type":"markdown","source":["# REINFORCE on LunarLanderContinous-v2 (OpenAI Gym version)\n","**Important!**\n","Before running the following cell, make sure rllab is set up properly in your current runtime by executing codes in RLLAB setup scripts."]},{"metadata":{"id":"XA0g5MBZHQ3o","colab_type":"text"},"cell_type":"markdown","source":["**1. Implement REINFORCE Algorithm**"]},{"metadata":{"id":"EJcqSePquDny","colab_type":"text"},"cell_type":"markdown","source":["- Import necessary packages\n","(Execute **once again** if you encounter an error)"]},{"metadata":{"id":"C49pvNj5HooC","colab_type":"code","colab":{}},"cell_type":"code","source":["from rllab.envs.gym_env import GymEnv\n","from rllab.policies.gaussian_mlp_policy import GaussianMLPPolicy\n","from rllab.policies.categorical_mlp_policy import CategoricalMLPPolicy\n","from rllab.envs.normalized_env import normalize\n","import numpy as np\n","import theano\n","import theano.tensor as TT\n","from lasagne.updates import adam\n","from rllab.misc.instrument import run_experiment_lite\n","import rllab.misc.logger as logger\n","\n","########## REINFORCE_GYM_Lunvar_v2_With_RUN_EXP_LITE ##########\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ThinxFhruY1K","colab_type":"text"},"cell_type":"markdown","source":["- Implement REINFORCE algorithm"]},{"metadata":{"id":"wpx46tkruXqt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"62e1ea9c-da15-4e91-ff5b-faf580c7509c","executionInfo":{"status":"ok","timestamp":1547624529604,"user_tz":-540,"elapsed":4121,"user":{"displayName":"Haanvid Lee","photoUrl":"","userId":"02061776945051468424"}}},"cell_type":"code","source":["def REINFORCE(*_):\n","    # normalize() makes sure that the actions for the environment lies\n","    # within the range [-1, 1] (only works for environments with continuous actions)\n","\n","    env = normalize(GymEnv(env_name = \"LunarLanderContinuous-v2\", force_reset=True, record_video=True))\n","    # env = normalize(GymEnv(env_name=\"CartPole-v0\", force_reset=True))\n","\n","    # Initialize a neural network policy with a single hidden layer of 8 hidden units\n","\n","    policy = GaussianMLPPolicy(env.spec, hidden_sizes=(64,64))\n","    # policy = CategoricalMLPPolicy(env.spec, hidden_sizes=(64, 64))\n","\n","    # We will collect 3 trajectories per iteration\n","    N = 3\n","    # Each trajectory will have at most 400 time steps\n","    T = 400\n","    # Number of iterations\n","    n_itr = 1000\n","    # Set the discount factor for the problem\n","    discount = 0.99\n","    # Learning rate for the gradient update\n","    learning_rate = 0.001\n","\n","    # Construct the computation graph\n","\n","    # Create a Theano variable for storing the observations\n","    # We could have simply written `observations_var = TT.matrix('observations')` instead for this example. However,\n","    # doing it in a slightly more abstract way allows us to delegate to the environment for handling the correct data\n","    # type for the variable. For instance, for an environment with discrete observations, we might want to use integer\n","    # types if the observations are represented as one-hot vectors.\n","    observations_var = env.observation_space.new_tensor_variable(\n","        'observations',\n","        # It should have 1 extra dimension since we want to represent a list of observations\n","        extra_dims=1\n","    )\n","    actions_var = env.action_space.new_tensor_variable(\n","        'actions',\n","        extra_dims=1\n","    )\n","    returns_var = TT.vector('returns')\n","\n","    # policy.dist_info_sym returns a dictionary, whose values are symbolic expressions for quantities related to the\n","    # distribution of the actions. For a Gaussian policy, it contains the mean and the logarithm of the standard deviation.\n","    dist_info_vars = policy.dist_info_sym(observations_var)\n","\n","    # policy.distribution returns a distribution object under rllab.distributions. It contains many utilities for computing\n","    # distribution-related quantities, given the computed dist_info_vars. Below we use dist.log_likelihood_sym to compute\n","    # the symbolic log-likelihood. For this example, the corresponding distribution is an instance of the class\n","    # rllab.distributions.DiagonalGaussian\n","    dist = policy.distribution\n","\n","    # Exp. #1, Prob.2: Define a proper loss function for REINFORCE\n","    ###########################################################################\n","    # Note that we negate the objective, since most optimizers assume a minimization problem\n","    \n","\n","    # Get the list of trainable parameters.\n","    params = policy.get_params(trainable=True)\n","    grads = theano.grad(surr, params)\n","\n","    f_train = theano.function(\n","        inputs=[observations_var, actions_var, returns_var],\n","        outputs=None,\n","        updates=adam(grads, params, learning_rate=learning_rate),\n","        allow_input_downcast=True\n","    )\n","\n","    for epoch in range(n_itr):\n","        ##################################################################\n","        logger.push_prefix('Epoch #%d | ' % (epoch))\n","        logger.log(\"Training started\")\n","        ##################################################################\n","        paths = []\n","\n","        for _ in range(N):\n","            observations = []\n","            actions = []\n","            rewards = []\n","\n","            observation = env.reset()\n","\n","            for _ in range(T):\n","                # policy.get_action() returns a pair of values. The second one returns a dictionary, whose values contains\n","                # sufficient statistics for the action distribution. It should at least contain entries that would be\n","                # returned by calling policy.dist_info(), which is the non-symbolic analog of policy.dist_info_sym().\n","                # Storing these statistics is useful, e.g., when forming importance sampling ratios. In our case it is\n","                # not needed.\n","                action, _ = policy.get_action(observation)\n","                # Recall that the last entry of the tuple stores diagnostic information about the environment. In our\n","                # case it is not needed.\n","                next_observation, reward, terminal, _ = env.step(action)\n","                observations.append(observation)\n","                actions.append(action)\n","                rewards.append(reward)\n","                observation = next_observation\n","                if terminal:\n","                    # Finish rollout if terminal state reached\n","                    break\n","\n","\n","            # Exp. #1, Prob.1: Calculate returns.\n","            ###########################################################################\n","            # We need to compute the empirical return for each time step along the\n","            # trajectory (return to go)\n","            returns = []\n","\n","\n","\n","\n","\n","            # The returns are stored backwards in time, so we need to revert it\n","\n","\n","            paths.append(dict(\n","                observations=np.array(observations),\n","                actions=np.array(actions),\n","                rewards=np.array(rewards),\n","                returns=np.array(returns)\n","            ))\n","\n","        #### Dim. of observation: [Sum of length of all traj, Dim(observation)] ####\n","        observations = np.concatenate([p[\"observations\"] for p in paths])\n","\n","        #### Dim. of actions    : [Sum of length of all traj, Dim(action)]      ####\n","        actions = np.concatenate([p[\"actions\"] for p in paths])\n","\n","        #### Dim. of returns    : [Sum of length of all traj, ]                 ####\n","        returns = np.concatenate([p[\"returns\"] for p in paths])\n","\n","        f_train(observations, actions, returns)\n","        print('Average Return:', np.mean([sum(p[\"rewards\"]) for p in paths]))\n","        ############################################################################\n","        logger.log(\"Training finished\")\n","        logger.save_itr_params(epoch, params)\n","        logger.dump_tabular(with_prefix=False)\n","        logger.pop_prefix()\n","\n","\n","        logger.record_tabular('Epoch', epoch)\n","        logger.record_tabular('Steps', epoch*N*T)\n","        logger.record_tabular('AverageReturn', np.mean(returns))\n","        logger.record_tabular('StdReturn', np.std(returns))\n","        logger.record_tabular('MaxReturn', np.max(returns))\n","        logger.record_tabular('MinReturn', np.min(returns))\n","\n","        #############################################################################\n","        \n","##### Creating & Running a task #####\n","\n","mypath = './log/vpg_reinforce/'\n","\n","run_experiment_lite(\n","    REINFORCE,\n","    # Number of parallel workers for sampling\n","    log_dir=mypath,\n","    n_parallel=1,\n","    # Only keep the snapshot parameters for the last iteration\n","    snapshot_mode=\"last\",\n","    # Specifies the seed for the experiment. If this is not provided, a random seed\n","    # will be used\n","    seed=1,\n","    # plot=True,\n",")"],"execution_count":12,"outputs":[{"output_type":"stream","text":["python /content/scripts/run_experiment_lite.py  --n_parallel '1'  --snapshot_mode 'last'  --seed '1'  --exp_name 'experiment_2019_01_16_07_39_30_0003'  --log_dir './log/vpg_reinforce/'  --use_cloudpickle 'True'  --args_data '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'  --variant_data 'gAN9cQBYCAAAAGV4cF9uYW1lcQFYIwAAAGV4cGVyaW1lbnRfMjAxOV8wMV8xNl8wN18zOV8zMF8wMDAzcQJzLg=='\n"],"name":"stdout"}]},{"metadata":{"id":"xF6kAwo57hb6","colab_type":"text"},"cell_type":"markdown","source":["**2. Execute Your Algorithm**"]},{"metadata":{"id":"UjHQm_6t7wK1","colab_type":"text"},"cell_type":"markdown","source":["- Activate a virtual display"]},{"metadata":{"id":"oIUuJMGv7u8O","colab_type":"code","colab":{}},"cell_type":"code","source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()\n","import os\n","os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x6yM_2LavPBR","colab_type":"text"},"cell_type":"markdown","source":["- Create & Run a RL task for Acrobot"]},{"metadata":{"id":"74oGATzdIO9b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"b49686f7-a053-4620-9b8a-592ef87ac0f8","executionInfo":{"status":"ok","timestamp":1547624408752,"user_tz":-540,"elapsed":19168,"user":{"displayName":"Haanvid Lee","photoUrl":"","userId":"02061776945051468424"}}},"cell_type":"code","source":["\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["python /content/scripts/run_experiment_lite.py  --n_parallel '1'  --snapshot_mode 'last'  --seed '1'  --exp_name 'experiment_2019_01_16_07_39_30_0001'  --log_dir './log/vpg_reinforce/'  --use_cloudpickle 'True'  --args_data 'gASVgAoAAAAAAACMF2Nsb3VkcGlja2xlLmNsb3VkcGlja2xllIwOX2ZpbGxfZnVuY3Rpb26Uk5QoaACMD19tYWtlX3NrZWxfZnVuY5STlGgAjA1fYnVpbHRpbl90eXBllJOUjAhDb2RlVHlwZZSFlFKUKEsASwBLG0sJS0dChgIAAHQAdAFkAWQCZAONAoMBfQF0AnwBagNkJWQFjQJ9AmQGfQNkB30EZAh9BWQJfQZkCn0HfAFqBGoFZAtkDGQNjQJ9CHwBagZqBWQOZAxkDY0CfQl0B2oIZA+DAX0KfAJqCXwIgwF9C3wCagp9DHwCagtkAmQQjQF9DXQMag10DnwNgwJ9DnQMag98CHwJfApnA2QAdBB8DnwNfAdkEY0DZAJkEo0EfQ+QAXjYdBF8BYMBRACQAV3KfRB0EmoTZBN8EBYAgwEBAHQSahRkFIMBAQBnAH0ReLR0EXwDgwFEAF2ofQBnAH0SZwB9E2cAfRR8AWoVgwB9FXhcdBF8BIMBRABdUH0AfAJqFnwVgwFcAn0WfQB8AWoXfBaDAVwEfRd9GH0ZfQB8EmoYfBWDAQEAfBNqGHwWgwEBAHwUahh8GIMBAQB8F30VfBmQAXIAUACQAXEAVwBnAH0afBFqGHQZdBpqG3wSgwF0GmobfBODAXQaaht8FIMBdBpqG3wagwFkFY0EgwEBAHHeVwB0GmocZBZkF4QAfBFEAIMBgwF9EnQaahxkGGQXhAB8EUQAgwGDAX0TdBpqHGQZZBeEAHwRRACDAYMBfRp8D3wSfBN8GoMDAQB0HWQadBpqHmQbZBeEAHwRRACDAYMBgwIBAHQSahRkHIMBAQB0EmoffBB8DYMCAQB0EmogZB1kHo0BAQB0EmohgwABAHQSaiJkH3wQgwIBAHQSaiJkIHwQfAMUAHwEFACDAgEAdBJqImQhdBpqHnwagwGDAgEAdBJqImQidBpqI3wagwGDAgEAdBJqImQjdBpqJHwagwGDAgEAdBJqImQkdBpqJXwagwGDAgEAcbJXAGQAUwCUKE6MGEx1bmFyTGFuZGVyQ29udGludW91cy12MpSIjAhlbnZfbmFtZZSMC2ZvcmNlX3Jlc2V0lIaUS0CMDGhpZGRlbl9zaXplc5SFlEsDTZABTegDRz/vrhR64UeuRz9QYk3S8an8jAxvYnNlcnZhdGlvbnOUSwGMCmV4dHJhX2RpbXOUhZSMB2FjdGlvbnOUjAdyZXR1cm5zlIwJdHJhaW5hYmxllIWUjA1sZWFybmluZ19yYXRllIWUKIwGaW5wdXRzlIwHb3V0cHV0c5SMB3VwZGF0ZXOUjBRhbGxvd19pbnB1dF9kb3duY2FzdJR0lIwMRXBvY2ggIyVkIHwglIwQVHJhaW5pbmcgc3RhcnRlZJQoaBFoFIwHcmV3YXJkc5RoFXSUaAkoSwFLAEsCSwRLU0MUZwB8AF0MfQF8AWQAGQCRAnEEUwCUaBGFlCmMAi4wlIwBcJSGlIwePGlweXRob24taW5wdXQtOC0xMWM1YTU5OThlOTg+lIwKPGxpc3Rjb21wPpRLekMCBgCUKSl0lFKUjB1SRUlORk9SQ0UuPGxvY2Fscz4uPGxpc3Rjb21wPpRoCShLAUsASwJLBEtTQxRnAHwAXQx9AXwBZAAZAJECcQRTAJRoFIWUKWglaCaGlGgoaClLfUMCBgCUKSl0lFKUaAkoSwFLAEsCSwRLU0MUZwB8AF0MfQF8AWQAGQCRAnEEUwCUaBWFlCloJWgmhpRoKGgpS4BDAgYAlCkpdJRSlIwPQXZlcmFnZSBSZXR1cm46lGgJKEsBSwBLAksFS1NDGGcAfABdEH0BdAB8AWQAGQCDAZECcQRTAJRoIYWUjANzdW2UhZRoJWgmhpRoKGgpS4NDAgYAlCkpdJRSlIwRVHJhaW5pbmcgZmluaXNoZWSUiYwLd2l0aF9wcmVmaXiUhZSMBUVwb2NolIwFU3RlcHOUjA1BdmVyYWdlUmV0dXJulIwJU3RkUmV0dXJulIwJTWF4UmV0dXJulIwJTWluUmV0dXJulEtAS0CGlHSUKIwJbm9ybWFsaXpllIwGR3ltRW52lIwRR2F1c3NpYW5NTFBQb2xpY3mUjARzcGVjlIwRb2JzZXJ2YXRpb25fc3BhY2WUjBNuZXdfdGVuc29yX3ZhcmlhYmxllIwMYWN0aW9uX3NwYWNllIwCVFSUjAZ2ZWN0b3KUjA1kaXN0X2luZm9fc3ltlIwMZGlzdHJpYnV0aW9ulIwKZ2V0X3BhcmFtc5SMBnRoZWFub5SMBGdyYWSUjARzdXJylIwIZnVuY3Rpb26UjARhZGFtlIwFcmFuZ2WUjAZsb2dnZXKUjAtwdXNoX3ByZWZpeJSMA2xvZ5SMBXJlc2V0lIwKZ2V0X2FjdGlvbpSMBHN0ZXCUjAZhcHBlbmSUjARkaWN0lIwCbnCUjAVhcnJheZSMC2NvbmNhdGVuYXRllIwFcHJpbnSUjARtZWFulIwPc2F2ZV9pdHJfcGFyYW1zlIwMZHVtcF90YWJ1bGFylIwKcG9wX3ByZWZpeJSMDnJlY29yZF90YWJ1bGFylIwDc3RklIwDbWF4lIwDbWlulHSUKIwBX5SMA2VudpSMBnBvbGljeZSMAU6UjAFUlIwFbl9pdHKUjAhkaXNjb3VudJRoGIwQb2JzZXJ2YXRpb25zX3ZhcpSMC2FjdGlvbnNfdmFylIwLcmV0dXJuc192YXKUjA5kaXN0X2luZm9fdmFyc5SMBGRpc3SUjAZwYXJhbXOUjAVncmFkc5SMB2ZfdHJhaW6UjAVlcG9jaJSMBXBhdGhzlGgRaBRoIYwLb2JzZXJ2YXRpb26UjAZhY3Rpb26UjBBuZXh0X29ic2VydmF0aW9ulIwGcmV3YXJklIwIdGVybWluYWyUaBV0lGgojAlSRUlORk9SQ0WUSwFDfAAEEAUOBAQCBAIEAgQCBAkGAQICCAIGAQIBCAIKBAoGBggMAQwCBAEIAQIBDAEIAxICDgEKAgQCDgEEAQQBBAIIAg4GDgMSAQoBCgEKAQQBBgIIBwQJBgEIAQgBCAEUBBQDFAMUAgwBGgIKAQwBDAEIAwwBFAESARIBEgGUKSl0lFKUSv////+MCF9fbWFpbl9flIeUUpR9lCiMB2dsb2JhbHOUfZQoaFpoAIwJc3ViaW1wb3J0lJOUjAZ0aGVhbm+UhZRSlGhPjBJybGxhYi5lbnZzLmd5bV9lbnaUaE+TlGhOjBlybGxhYi5lbnZzLm5vcm1hbGl6ZWRfZW52lIwNTm9ybWFsaXplZEVudpSTlGhoaJeMBW51bXB5lIWUUpRoYGiXjBFybGxhYi5taXNjLmxvZ2dlcpSFlFKUaFVol4wNdGhlYW5vLnRlbnNvcpSFlFKUaFCMInJsbGFiLnBvbGljaWVzLmdhdXNzaWFuX21scF9wb2xpY3mUaFCTlGhejA9sYXNhZ25lLnVwZGF0ZXOUaF6TlHWMCGRlZmF1bHRzlE5oZ32UjA5jbG9zdXJlX3ZhbHVlc5ROjAZtb2R1bGWUaJCMBG5hbWWUaIyMA2RvY5ROjAhxdWFsbmFtZZRojHV0Ui4='  --variant_data 'gAN9cQBYCAAAAGV4cF9uYW1lcQFYIwAAAGV4cGVyaW1lbnRfMjAxOV8wMV8xNl8wN18zOV8zMF8wMDAxcQJzLg=='\n"],"name":"stdout"}]},{"metadata":{"id":"sdDOU5hwKlW4","colab_type":"text"},"cell_type":"markdown","source":["**3. Average Reward Plotting**"]},{"metadata":{"id":"ip1bXHMtvtWy","colab_type":"text"},"cell_type":"markdown","source":["- You can evaluate how your agent is being trained with reward it gets in every iteration. \n","- Whenever you execute the code 'run_experiment_lite', it will generate a experiment directory.\n","- (/content/log/vpg_reinforce/)\n","- Please update the value of '**mypath**' and specify your new experiment directory name. "]},{"metadata":{"id":"RlIgTMPOKqHX","colab_type":"code","colab":{}},"cell_type":"code","source":["import os.path as osp\n","import numpy as np\n","import csv\n","import matplotlib.pyplot as plt\n","import json\n","import joblib\n","from glob import glob\n","import os\n","\n","# mypath = './log/vpg_reinforce/'\n","\n","plots = []\n","legends = []\n","returns = []\n","with open(osp.join(mypath, 'progress.csv'), 'rt') as csvfile:\n","    reader = csv.DictReader(csvfile)\n","    for row in reader:\n","        if row['AverageReturn']:\n","            returns.append(float(row['AverageReturn']))\n","returns = np.array(returns)\n","plots.append(plt.plot(returns)[0])\n","legends.append('AverageReturn')\n","plt.legend(plots, legends)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rWDQUoPePLDx","colab_type":"text"},"cell_type":"markdown","source":["**4. Play Videos of your Agent Behavior**"]},{"metadata":{"id":"AhdxUQuozcRv","colab_type":"text"},"cell_type":"markdown","source":["- You can watch how your agent's behavior improves.\n","- If you haven't update 'mypath' in the code above, you need to update it here.\n","- (/content/log/vpg_reinforce/')"]},{"metadata":{"id":"TPjtJ3IjPY5A","colab_type":"code","colab":{}},"cell_type":"code","source":["from IPython import display as pythondisplay\n","# from pyvirtualdisplay import Display\n","\n","# from matplotlib.pyplot import imshow\n","import matplotlib.pyplot as plt\n","from matplotlib import animation\n","from JSAnimation import IPython_display\n","from IPython.display import HTML\n","\n","import imageio\n","\n","from os import listdir\n","from os.path import isfile, join\n","\n","def plot_movie_js(image_array, filename):\n","    dpi = 10.0\n","    xpixels, ypixels = image_array[0].shape[0], image_array[0].shape[1]\n","    fig = plt.figure(figsize=(ypixels/(dpi), xpixels/(dpi)), dpi=dpi)\n","    fig.suptitle(filename, fontsize=160)\n","    # fig.set_xlabel(filename, fontsize=160)\n","    # fig.xlabel(filename, fontsize=160)\n","    im = plt.figimage(image_array[0])\n","\n","    def animate(i):\n","        im.set_array(image_array[i])\n","        return (im,)\n","    \n","    anim = animation.FuncAnimation(fig, animate, frames=len(image_array))\n","    pythondisplay.display(IPython_display.display_animation(anim))\n","\n","# mypath = './log/vpg_reinforce/'\n","mypath += 'gym_log/'\n","mp4files = [f for f in listdir(mypath) if f.endswith(\".mp4\")]\n","mp4files.sort()\n","\n","\n","for filename in mp4files:\n","    vid = imageio.get_reader(join(mypath, filename),  'ffmpeg')\n","    # print(len(vid))\n","    # print(vid.get_data(0).shape)\n","\n","    screenlist = []\n","    for i in range(len(vid)):\n","        image = vid.get_data(i)\n","        screenlist.append(image)\n","        # fig = plt.figure()\n","        # fig.suptitle('image #{}'.format(i), fontsize=20)\n","        # plt.imshow(image)\n","\n","    plot_movie_js(screenlist, filename)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VAsgT4hsid4X","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}
