{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day1_rllab_DQN++.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "wP3KREMIG196",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RLLAB setup scripts for google colab\n",
        "Install packages with compatible versions"
      ]
    },
    {
      "metadata": {
        "id": "2J5ROkw6Gcxq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get -qq install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tvacEH-tGtI1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q path.py\n",
        "!pip install -q pyprind\n",
        "!pip install -q cached_property\n",
        "!pip install -q gym==0.7.4\n",
        "!pip install -q theano==0.8.2\n",
        "!pip install -q git+https://github.com/neocxi/Lasagne.git@484866cf8b38d878e92d521be445968531646bb8#egg=Lasagne\n",
        "  \n",
        "!pip install -q PyOpenGL piglet pyglet pyvirtualdisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TWKA_0xQHGAS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install box2d-py mako==1.0.7 Pygame JSAnimation imageio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iDxIzxg9HJhp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kekim/rllab.git rllab-git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hq4UosxRHMkq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp -a ./rllab-git/* ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DUHGgASMNg2N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# DQN++ on CartPole (OpenAI Gym version)\n",
        "**Important!**\n",
        "Before running the following cell, make sure rllab is set up properly in your current runtime by executing codes in RLLAB setup scripts."
      ]
    },
    {
      "metadata": {
        "id": "XA0g5MBZHQ3o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**1. Implement DQN++ Algorithm**"
      ]
    },
    {
      "metadata": {
        "id": "EJcqSePquDny",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Import necessary packages\n",
        "(Execute **once again** if you encounter an error)"
      ]
    },
    {
      "metadata": {
        "id": "C49pvNj5HooC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########## DQN_GYM_CARTPOLE_With_RUN_EXP_LITE ##########\n",
        "from rllab.envs.gym_env import GymEnv\n",
        "#from dqn.policies.categorical_mlp_q_policy import CategoricalMlpQPolicy\n",
        "from dqn.exploration_strategies.eps_greedy_strategy import EpsilonGreedyStrategy\n",
        "\n",
        "# from dqn.algos.dqn import DQN\n",
        "\n",
        "from rllab.envs.normalized_env import normalize\n",
        "from rllab.misc.instrument import run_experiment_lite\n",
        "from rllab.q_functions.continuous_mlp_q_function import ContinuousMLPQFunction\n",
        "\n",
        "import lasagne.nonlinearities as NL\n",
        "'''\n",
        "import gym.envs\n",
        "from rllab.envs.gym_env import GymEnv\n",
        "from envs.proxy_gym_env import ProxyGymEnv\n",
        "from misc.retro_wrappers import wrap_deepmind_retro\n",
        "from policies.categorical_mlp_q_policy import CategoricalMlpQPolicy\n",
        "from exploration_strategies.eps_greedy_strategy import EpsilonGreedyStrategy\n",
        "\n",
        "from algos.dqn import DQN\n",
        "\n",
        "from rllab.envs.normalized_env import normalize\n",
        "from rllab.misc.instrument import run_experiment_lite\n",
        "from rllab.q_functions.continuous_mlp_q_function import ContinuousMLPQFunction\n",
        "\n",
        "import lasagne.nonlinearities as NL\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ogAGnWnU6TJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Implement Dueling Q-network\n",
        "\n",
        "![대체 텍스트](https://hadovanhasselt.files.wordpress.com/2016/06/dueling-network.png?w=1000)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "V_maHsxZU6Co",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from rllab.core.lasagne_powered import LasagnePowered\n",
        "import lasagne.layers as L\n",
        "from rllab.core.network import MLP\n",
        "from rllab.distributions.categorical import Categorical\n",
        "from rllab.policies.base import Policy\n",
        "from rllab.misc import tensor_utils\n",
        "from rllab.spaces.discrete import Discrete\n",
        "\n",
        "from rllab.core.serializable import Serializable\n",
        "from rllab.misc import ext\n",
        "from rllab.misc import logger\n",
        "from rllab.misc.overrides import overrides\n",
        "import numpy as np\n",
        "import lasagne.nonlinearities as NL\n",
        "import theano.tensor as TT\n",
        "import theano\n",
        "\n",
        "class CategoricalMlpQPolicy(Policy, LasagnePowered, Serializable):\n",
        "    def __init__(\n",
        "            self,\n",
        "            name,\n",
        "            env_spec,\n",
        "            hidden_sizes=(32, 32),\n",
        "            hidden_nonlinearity=NL.tanh,\n",
        "            num_seq_inputs=1,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param env_spec: A spec for the mdp.\n",
        "        :param hidden_sizes: list of sizes for the fully connected hidden layers\n",
        "        :param hidden_nonlinearity: nonlinearity used for each hidden layer\n",
        "        :param prob_network: manually specified network for this policy, other network params\n",
        "        are ignored\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        Serializable.quick_init(self, locals())\n",
        "\n",
        "        assert isinstance(env_spec.action_space, Discrete)\n",
        "\n",
        "        self._env_spec = env_spec\n",
        "        \n",
        "        # print( env_spec.observation_space.shape )\n",
        "        #import pdb;pdb.set_trace()\n",
        "        '''\n",
        "        q_network = MLP(\n",
        "            input_shape=(env_spec.observation_space.flat_dim * num_seq_inputs,),\n",
        "            output_dim=env_spec.action_space.n,\n",
        "            hidden_sizes=hidden_sizes,\n",
        "            hidden_nonlinearity=hidden_nonlinearity,\n",
        "            output_nonlinearity=NL.linear,\n",
        "            name=name\n",
        "        )\n",
        "        '''\n",
        "        ########################### Dueling network ###############################\n",
        "\n",
        "        value_stream = MLP(\n",
        "            input_shape=(env_spec.observation_space.flat_dim * num_seq_inputs,),\n",
        "            output_dim=1,\n",
        "            hidden_sizes=hidden_sizes,\n",
        "            hidden_nonlinearity=hidden_nonlinearity,\n",
        "            output_nonlinearity=NL.linear,\n",
        "            name=name\n",
        "        )\n",
        "\n",
        "\n",
        "        advntg_fn_stream = MLP(\n",
        "            input_shape=(env_spec.observation_space.flat_dim * num_seq_inputs,),\n",
        "            output_dim=env_spec.action_space.n,\n",
        "            hidden_sizes=hidden_sizes,\n",
        "            hidden_nonlinearity=hidden_nonlinearity,\n",
        "            output_nonlinearity=NL.linear,\n",
        "            name=name\n",
        "        )\n",
        "        #########################################################################\n",
        "        \n",
        "        #self._l_q = q_network.output_layer\n",
        "        self._l_value = value_stream.output_layer\n",
        "        self._l_advntg = advntg_fn_stream.output_layer\n",
        "\n",
        "        self._l_obs = value_stream.input_layer\n",
        "        self._l_obs2 = advntg_fn_stream.input_layer\n",
        "        '''\n",
        "        self._l_obs = q_network.input_layer\n",
        "\n",
        "        self._f_q = ext.compile_function(\n",
        "            [q_network.input_layer.input_var],\n",
        "            L.get_output(q_network.output_layer)\n",
        "        )\n",
        "        '''\n",
        "        self._f_value = ext.compile_function(\n",
        "            [value_stream.input_layer.input_var],\n",
        "            L.get_output(value_stream.output_layer)\n",
        "        )\n",
        "        self._f_advntg = ext.compile_function(\n",
        "            [advntg_fn_stream.input_layer.input_var],\n",
        "            L.get_output(advntg_fn_stream.output_layer)\n",
        "        )\n",
        "\n",
        "        #import pdb; pdb.set_trace()\n",
        "        self._dist = Categorical(env_spec.action_space.n)\n",
        "\n",
        "        super(CategoricalMlpQPolicy, self).__init__(env_spec)\n",
        "        '''\n",
        "        LasagnePowered.__init__(self, [q_network.output_layer])\n",
        "        '''\n",
        "        LasagnePowered.__init__(self, [value_stream.output_layer, advntg_fn_stream.output_layer])\n",
        "        \n",
        "\n",
        "    @property\n",
        "    def vectorized(self):\n",
        "        return True\n",
        "\n",
        "    # The return value is a pair. The first item is a matrix (N, A), where each\n",
        "    # entry corresponds to the action value taken. The second item is a vector\n",
        "    # of length N, where each entry is the density value for that action, under\n",
        "    # the current policy\n",
        "    @overrides\n",
        "    def get_action(self, observation):\n",
        "        flat_obs = self.observation_space.flatten(observation)\n",
        "        '''\n",
        "        q = self._f_q([flat_obs])[0]\n",
        "        '''\n",
        "        value = self._f_value([flat_obs])[0]        \n",
        "        advntg = self._f_advntg([flat_obs])[0]\n",
        "\n",
        "        #import pdb; pdb.set_trace()\n",
        "\n",
        "        value = np.repeat(value, self._env_spec.action_space.n, axis=-1)\n",
        "        identifiable_advntg = advntg - np.mean(advntg, axis=-1)  \n",
        "        dueling_q = value + identifiable_advntg\n",
        "        '''\n",
        "        action = np.argmax(q)\n",
        "        return action, dict(q=q)\n",
        "        '''\n",
        "        action = np.argmax(dueling_q)\n",
        "        return action, dict(q=dueling_q)\n",
        "\n",
        "    def get_actions(self, observations):\n",
        "        \n",
        "        flat_obs = self.observation_space.flatten_n(observations)\n",
        "        '''\n",
        "        q = self._f_q(flat_obs)\n",
        "        '''\n",
        "        value = self._f_value(flat_obs)\n",
        "        advntg = self._f_advntg(flat_obs)        \n",
        "\n",
        "        value = np.repeat(value, self._env_spec.action_space.n, axis=1)\n",
        "        identifiable_advntg = advntg - np.repeat(np.reshape(np.mean(advntg,axis=1),(-1,1)),axis=1,repeats=self._env_spec.action_space.n)\n",
        "        dueling_q = value + identifiable_advntg\n",
        "\n",
        "        '''\n",
        "        actions = np.argmax(q, axis=1)\n",
        "        return actions, dict(q=q)\n",
        "        '''\n",
        "        actions = np.argmax(dueling_q, axis=1)\n",
        "        return actions, dict(q=dueling_q)\n",
        "    \n",
        "        \n",
        "    def get_qval_sym(self, obs_var, a_var): # qval.shape=(20,) , obs.shape=(20,4) , actions.shape=(20,2)\n",
        "        '''\n",
        "        q_vals_sym = L.get_output(self._l_q,                    # maybe (20,2)\n",
        "                                  {self._l_obs: obs_var})\n",
        "        '''\n",
        "        \n",
        "        value_vals_sym = L.get_output(self._l_value,            # maybe (20,1)\n",
        "                                    {self._l_obs: obs_var})\n",
        "        advntg_vals_sym = L.get_output(self._l_advntg,          # maybe (20,2)\n",
        "                                    {self._l_obs2: obs_var})\n",
        "\n",
        "        value_vals_sym = TT.extra_ops.repeat(value_vals_sym, self._env_spec.action_space.n, axis=-1)\n",
        "        #identifiable_advntg_vals_sym = advntg_vals_sym - TT.max(advntg_vals_sym, axis=-1, keepdims=True) \n",
        "        identifiable_advntg_vals_sym = advntg_vals_sym - TT.mean(advntg_vals_sym, axis=-1, keepdims=True)\n",
        "        dueling_q_vals_sym = value_vals_sym + identifiable_advntg_vals_sym\n",
        "\n",
        "        #dueling_q_vals_sym = L.ElemwiseSumLayer([value_vals_sym, identifiable_advntg_vals_sym])\n",
        "        #import pdb; pdb.set_trace()\n",
        "        '''\n",
        "        return (q_vals_sym * a_var).sum(axis=1)\n",
        "        '''\n",
        "        return (dueling_q_vals_sym * a_var).sum(axis=1)\n",
        "    \n",
        "    def get_qval_sym_test(self, obs_var, a_var):\n",
        "        '''\n",
        "        q_vals_sym = L.get_output(self._l_q, \n",
        "                                  {self._l_obs: obs_var})\n",
        "        '''            \n",
        "\n",
        "        value_vals_sym = L.get_output(self._l_value,            \n",
        "                                    {self._l_obs: obs_var})\n",
        "        advntg_vals_sym = L.get_output(self._l_advntg,         \n",
        "                                    {self._l_obs2: obs_var})\n",
        "\n",
        "        value_vals_sym = TT.extra_ops.repeat(value_vals_sym, self._env_spec.action_space.n, axis=-1)\n",
        "        #identifiable_advntg_vals_sym = advntg_vals_sym - TT.max(advntg_vals_sym, axis=-1, keepdims=True)\n",
        "        identifiable_advntg_vals_sym = advntg_vals_sym - TT.mean(advntg_vals_sym, axis=-1, keepdims=True)\n",
        "        dueling_q_vals_sym = value_vals_sym + identifiable_advntg_vals_sym\n",
        "        #dueling_q_vals_sym = L.ElemwiseSumLayer([value_vals_sym, identifiable_advntg_vals_sym])\n",
        "        '''\n",
        "        return [q_vals_sym, a_var, q_vals_sym * a_var, (q_vals_sym * a_var).sum(axis=1)]\n",
        "        '''\n",
        "        return [dueling_q_vals_sym, a_var, dueling_q_vals_sym * a_var, (dueling_q_vals_sym * a_var).sum(axis=1)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ceoTshj9U35c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "CAnQANVpuL2c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Implement DQN++ algorithm"
      ]
    },
    {
      "metadata": {
        "id": "ZT3OayOoO6F9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from rllab.algos.base import RLAlgorithm\n",
        "from rllab.misc.overrides import overrides\n",
        "from rllab.misc import special\n",
        "from rllab.misc import ext\n",
        "from rllab.sampler import parallel_sampler\n",
        "from rllab.plotter import plotter\n",
        "from functools import partial\n",
        "import rllab.misc.logger as logger\n",
        "import theano.tensor as TT\n",
        "import pickle as pickle\n",
        "import numpy as np\n",
        "import pyprind\n",
        "import lasagne\n",
        "from collections import deque\n",
        "from rllab.algos.ddpg import parse_update_method, SimpleReplayPool\n",
        "\n",
        "def huber_loss(diffs, clip):\n",
        "    return TT.mean(TT.switch(abs(diffs) < clip, 0.5 * TT.square(diffs), clip * (abs(diffs) - 0.5 * clip)))\n",
        "    \n",
        "'''\n",
        "class SimpleReplayPool(object):\n",
        "    def __init__(\n",
        "            self, max_pool_size, observation_dim, action_dim):\n",
        "        self._observation_dim = observation_dim\n",
        "        self._action_dim = action_dim\n",
        "        self._max_pool_size = max_pool_size\n",
        "        self._observations = np.zeros(\n",
        "            (max_pool_size, observation_dim),\n",
        "        )\n",
        "        self._actions = np.zeros(\n",
        "            (max_pool_size, action_dim),\n",
        "        )\n",
        "        self._rewards = np.zeros(max_pool_size)\n",
        "        self._terminals = np.zeros(max_pool_size, dtype='uint8')\n",
        "        self._priorities = np.zeros(max_pool_size)\n",
        "        self._bottom = 0\n",
        "        self._top = 0\n",
        "        self._size = 0\n",
        "\n",
        "    def add_sample(self, observation, action, reward, terminal):\n",
        "        self._observations[self._top] = observation\n",
        "        self._actions[self._top] = action\n",
        "        self._rewards[self._top] = reward\n",
        "        self._terminals[self._top] = terminal\n",
        "        self._priorities[self._top] = np.max(self._priorities)\n",
        "        self._top = (self._top + 1) % self._max_pool_size\n",
        "        if self._size >= self._max_pool_size:\n",
        "            self._bottom = (self._bottom + 1) % self._max_pool_size\n",
        "        else:\n",
        "            self._size += 1\n",
        "\n",
        "    def update_sample(self, indices, priorities):\n",
        "        self._priorities[indices] = priorities\n",
        "\n",
        "    def priority_batch(self, batch_size):\n",
        "        assert self._size > batch_size\n",
        "        indices = np.zeros(batch_size, dtype='uint64')\n",
        "        transition_indices = np.zeros(batch_size, dtype='uint64')\n",
        "        count = 0\n",
        "        while count < batch_size:\n",
        "            index = np.random.randint(self._bottom, self._bottom + self._size) % self._max_pool_size\n",
        "            # make sure that the transition is valid: if we are at the end of the pool,\n",
        "            # we need to discard this sample\n",
        "            if index == self._size - 1 and self._size <= self._max_pool_size:\n",
        "                continue\n",
        "            # if self._terminals[index]:\n",
        "            #     continue\n",
        "            transition_index = (index + 1) % self._max_pool_size\n",
        "            indices[count] = index\n",
        "            transition_indices[count] = transition_index\n",
        "            count += 1\n",
        "        return dict(\n",
        "            observations=self._observations[indices],\n",
        "            actions=self._actions[indices],\n",
        "            rewards=self._rewards[indices],\n",
        "            terminals=self._terminals[indices],\n",
        "            indices=indices,\n",
        "            next_observations=self._observations[transition_indices]\n",
        "        )\n",
        "\n",
        "    def random_batch(self, batch_size):\n",
        "        assert self._size > batch_size\n",
        "        indices = np.zeros(batch_size, dtype='uint64')\n",
        "        transition_indices = np.zeros(batch_size, dtype='uint64')\n",
        "        count = 0\n",
        "        while count < batch_size:\n",
        "            index = np.random.randint(self._bottom, self._bottom + self._size) % self._max_pool_size\n",
        "            # make sure that the transition is valid: if we are at the end of the pool, we need to discard\n",
        "            # this sample\n",
        "            if index == self._size - 1 and self._size <= self._max_pool_size:\n",
        "                continue\n",
        "            # if self._terminals[index]:\n",
        "            #     continue\n",
        "            transition_index = (index + 1) % self._max_pool_size\n",
        "            indices[count] = index\n",
        "            transition_indices[count] = transition_index\n",
        "            count += 1\n",
        "        return dict(\n",
        "            observations=self._observations[indices],\n",
        "            actions=self._actions[indices],\n",
        "            rewards=self._rewards[indices],\n",
        "            terminals=self._terminals[indices],\n",
        "            next_observations=self._observations[transition_indices]\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def size(self):\n",
        "        return self._size\n",
        "'''\n",
        "class DDQN(RLAlgorithm):\n",
        "    \"\"\"\n",
        "    Deep Q Network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            env,\n",
        "            policy,\n",
        "            es,\n",
        "            batch_size=32,\n",
        "            n_steps=8000000,\n",
        "            min_pool_size=1000000,\n",
        "            replay_pool_size=1000000,\n",
        "            discount=0.99,\n",
        "            policy_update_method='adam',\n",
        "            policy_learning_rate=1e-3,\n",
        "            target_model_update=10000,\n",
        "            n_updates_per_sample=1,\n",
        "            train_epoch_interval=10000,\n",
        "            max_path_length=np.inf,\n",
        "            n_eval_samples=5,\n",
        "            delta_clip=np.inf,\n",
        "            include_horizon_terminal_transitions=False,\n",
        "            plot=False,\n",
        "            pause_for_plot=False):\n",
        "        \"\"\"\n",
        "        :param env: Environment\n",
        "        :param policy: Policy\n",
        "        :param qf: Q function\n",
        "        :param es: Exploration strategy\n",
        "        :param batch_size: Number of samples for each minibatch.\n",
        "        :param n_epochs: Number of epochs. Policy will be evaluated after each epoch.\n",
        "        :param epoch_length: How many timesteps for each epoch.\n",
        "        :param min_pool_size: Minimum size of the pool to start training.\n",
        "        :param replay_pool_size: Size of the experience replay pool.\n",
        "        :param discount: Discount factor for the cumulative return.\n",
        "        :param max_path_length: Discount factor for the cumulative return.\n",
        "        :param qf_weight_decay: Weight decay factor for parameters of the Q function.\n",
        "        :param qf_update_method: Online optimization method for training Q function.\n",
        "        :param qf_learning_rate: Learning rate for training Q function.\n",
        "        :param policy_weight_decay: Weight decay factor for parameters of the policy.\n",
        "        :param policy_update_method: Online optimization method for training the policy.\n",
        "        :param policy_learning_rate: Learning rate for training the policy.\n",
        "        :param n_eval_samples: Number of samples (timesteps) for evaluating the policy.\n",
        "        :param soft_target_tau: Interpolation parameter for doing the soft target update.\n",
        "        :param n_updates_per_sample: Number of Q function and policy updates per new sample obtained\n",
        "        :param scale_reward: The scaling factor applied to the rewards when training\n",
        "        :param include_horizon_terminal_transitions: whether to include transitions with terminal=True because the\n",
        "        horizon was reached. This might make the Q value back up less stable for certain tasks.\n",
        "        :param plot: Whether to visualize the policy performance after each train_epoch_interval.\n",
        "        :param pause_for_plot: Whether to pause before continuing when plotting.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.policy = policy\n",
        "        self.es = es\n",
        "        self.batch_size = batch_size\n",
        "        self.min_pool_size = min_pool_size\n",
        "        self.replay_pool_size = replay_pool_size\n",
        "        self.discount = discount\n",
        "        self.n_steps = n_steps\n",
        "        \n",
        "        self.policy_learning_rate = policy_learning_rate\n",
        "        self.policy_update_method = parse_update_method(\n",
        "                policy_update_method,\n",
        "                learning_rate=policy_learning_rate,\n",
        "        )\n",
        "        \n",
        "        assert target_model_update >= 0\n",
        "        if target_model_update >= 1:\n",
        "            self.target_model_update = int(target_model_update) # hard update every xx steps\n",
        "        else:\n",
        "            self.target_model_update = float(target_model_update) # soft update\n",
        "            \n",
        "        self.n_updates_per_sample = n_updates_per_sample\n",
        "        self.train_epoch_interval = train_epoch_interval\n",
        "        self.max_path_length = max_path_length\n",
        "        self.n_eval_samples = n_eval_samples\n",
        "        self.delta_clip = delta_clip\n",
        "        \n",
        "        self.include_horizon_terminal_transitions = include_horizon_terminal_transitions\n",
        "        self.plot = plot\n",
        "        self.pause_for_plot = pause_for_plot\n",
        "\n",
        "        self.qf_loss_averages = []\n",
        "        self.q_averages = []\n",
        "        self.y_averages = []\n",
        "        self.paths = []\n",
        "        self.es_path_returns = []\n",
        "        self.paths_samples_cnt = 0\n",
        "\n",
        "        self.opt_info = None\n",
        "\n",
        "    def start_worker(self):\n",
        "        parallel_sampler.populate_task(self.env, self.policy)\n",
        "        if self.plot:\n",
        "            plotter.init_plot(self.env, self.policy)\n",
        "\n",
        "    @overrides\n",
        "    def train(self):\n",
        "        # This seems like a rather sequential method\n",
        "        pool = SimpleReplayPool(\n",
        "            max_pool_size=int(self.replay_pool_size),\n",
        "            observation_dim=self.env.observation_space.flat_dim,\n",
        "            action_dim=self.env.action_space.flat_dim,\n",
        "        )\n",
        "            \n",
        "        self.start_worker()\n",
        "\n",
        "        self.init_opt()\n",
        "        itr = 0\n",
        "        path_length = 0\n",
        "        path_return = 0\n",
        "        terminal = False\n",
        "        \n",
        "        observation = self.env.reset()\n",
        "\n",
        "        sample_policy = pickle.loads(pickle.dumps(self.policy))\n",
        "\n",
        "        train_epoch = 0\n",
        "        \n",
        "        while train_epoch * self.train_epoch_interval < self.n_steps: \n",
        "            logger.push_prefix('step #%d | ' % (train_epoch * self.train_epoch_interval))\n",
        "            logger.log(\"Training started\")\n",
        "            for train_epoch_step in pyprind.prog_bar(range(self.train_epoch_interval)):\n",
        "                # Execute policy\n",
        "                if terminal or path_length > self.max_path_length:\n",
        "                    # Note that if the last step step ends an episode, the very\n",
        "                    # last state and observation will be ignored and not added\n",
        "                    # to the replay pool\n",
        "                    # print('terminal! ' + str(itr))\n",
        "                    observation = self.env.reset()\n",
        "                    self.es.reset()\n",
        "                    sample_policy.reset()\n",
        "                    self.es_path_returns.append(path_return)\n",
        "                    path_length = 0\n",
        "                    path_return = 0\n",
        "                action = self.es.get_action(itr, observation, policy=sample_policy)  # qf=qf)\n",
        "\n",
        "                next_observation, reward, terminal, _ = self.env.step(action)\n",
        "                path_length += 1\n",
        "                path_return += reward\n",
        "\n",
        "                \n",
        "                pool.add_sample(self.env.observation_space.flatten(observation), \n",
        "                                self.env.action_space.flatten(action), \n",
        "                                reward, \n",
        "                                terminal) ## clipping?\n",
        "\n",
        "                observation = next_observation\n",
        "\n",
        "                if pool.size >= self.min_pool_size:\n",
        "                    for update_itr in range(self.n_updates_per_sample):\n",
        "                        # Train policy\n",
        "                        batch = pool.random_batch(self.batch_size)\n",
        "                        self.do_training(itr, batch)\n",
        "                    sample_policy.set_param_values(self.policy.get_param_values())\n",
        "                \n",
        "                itr += 1\n",
        "\n",
        "            logger.log(\"Training finished\")\n",
        "            if pool.size >= self.min_pool_size:\n",
        "                self.evaluate(train_epoch * self.train_epoch_interval, pool)\n",
        "                if self.n_eval_samples > 0: # we performed rollout!\n",
        "                    observation = self.env.reset()\n",
        "                params = self.get_epoch_snapshot(train_epoch * self.train_epoch_interval)\n",
        "                logger.save_itr_params(train_epoch * self.train_epoch_interval, params)\n",
        "            logger.dump_tabular(with_prefix=False)\n",
        "            logger.pop_prefix()\n",
        "            train_epoch += 1\n",
        "            if self.plot:\n",
        "                self.update_plot()\n",
        "                if self.pause_for_plot:\n",
        "                    input(\"Plotting evaluation run: Press Enter to \"\n",
        "                              \"continue...\")\n",
        "        self.env.terminate()\n",
        "        self.policy.terminate()\n",
        "\n",
        "    def init_opt(self):\n",
        "        # First, create \"target\" policy and Q functions\n",
        "        target_policy = pickle.loads(pickle.dumps(self.policy))\n",
        "\n",
        "        # y need to be computed first\n",
        "        obs = self.env.observation_space.new_tensor_variable(\n",
        "            'obs',\n",
        "            extra_dims=1,\n",
        "        )\n",
        "\n",
        "        # The yi values are computed separately as above and then passed to\n",
        "        # the training functions below\n",
        "        action = self.env.action_space.new_tensor_variable(\n",
        "            'action',\n",
        "            extra_dims=1,\n",
        "        )\n",
        "        yvar = TT.vector('ys')\n",
        "\n",
        "        qval = self.policy.get_qval_sym(obs, action).flatten()\n",
        "\n",
        "        # qf_loss = huber_loss(yvar - qval, self.delta_clip)\n",
        "        qf_loss = TT.mean(TT.square(yvar - qval))\n",
        "        qf_reg_loss = qf_loss # + qf_weight_decay_term\n",
        "       \n",
        "        qf_updates = self.policy_update_method(\n",
        "            qf_reg_loss, \n",
        "            self.policy.get_params(trainable=True))\n",
        "\n",
        "        f_train_qf = ext.compile_function(\n",
        "            inputs=[yvar, obs, action],\n",
        "            outputs=[qf_loss, qval],\n",
        "            updates=qf_updates\n",
        "        )\n",
        "        \n",
        "        # qval_test = self.policy.get_qval_sym_test(obs, action)\n",
        "        \n",
        "#         f_train_qf_test = ext.compile_function(\n",
        "#             inputs=[yvar, obs, action],\n",
        "#             outputs=qval_test\n",
        "#         )\n",
        "\n",
        "        self.opt_info = dict(\n",
        "            f_train_qf=f_train_qf,\n",
        "            # f_train_qf_test=f_train_qf_test,\n",
        "            target_policy=target_policy,\n",
        "        )\n",
        "\n",
        "    def do_training(self, itr, batch):\n",
        "        \n",
        "        # obs, actions, next_obs should be all flattened\n",
        "        obs, actions, rewards, next_obs, terminals = ext.extract(\n",
        "            batch,\n",
        "            \"observations\", \"actions\", \"rewards\", \"next_observations\",\n",
        "            \"terminals\"\n",
        "        )\n",
        "        \n",
        "        target_policy = self.opt_info[\"target_policy\"]\n",
        "        _, target_actions_info = target_policy.get_actions(next_obs)\n",
        "        next_actions, _ = self.policy.get_actions(next_obs)\n",
        "        next_qvals = target_actions_info['q'][range(self.batch_size),next_actions]\n",
        "        \n",
        "        if np.any(np.isnan(next_qvals)):\n",
        "            print(itr)\n",
        "            print(target_policy.get_param_values())\n",
        "            print(next_qvals)\n",
        "            assert False\n",
        "        \n",
        "        ys = rewards + (1. - terminals) * self.discount * next_qvals\n",
        "        \n",
        "        # print(max(rewards), min(rewards), max(terminals), min(terminals))\n",
        "        \n",
        "        f_train_qf = self.opt_info[\"f_train_qf\"]  \n",
        "        qf_loss, qval = f_train_qf(ys, obs, actions)\n",
        "        \n",
        "        if self.target_model_update >= 1 and itr % self.target_model_update == 0:\n",
        "            target_policy.set_param_values(self.policy.get_param_values())\n",
        "        elif self.target_model_update < 1:\n",
        "            target_policy.set_param_values(\n",
        "                target_policy.get_param_values() * (1.0 - self.target_model_update) + \n",
        "                self.policy.get_param_values() * self.target_model_update) \n",
        "\n",
        "        self.qf_loss_averages.append(qf_loss)\n",
        "        self.q_averages.append(qval)\n",
        "        self.y_averages.append(ys)\n",
        "\n",
        "    def evaluate(self, epoch, pool):\n",
        "        \n",
        "        logger.record_tabular('Epoch', epoch)\n",
        "        \n",
        "        if self.n_eval_samples > 0:\n",
        "            logger.log(\"Collecting samples for evaluation\")\n",
        "            paths = parallel_sampler.sample_paths(\n",
        "                policy_params=self.policy.get_param_values(),\n",
        "                max_samples=self.n_eval_samples,\n",
        "                max_path_length=self.max_path_length,\n",
        "            )\n",
        "\n",
        "            average_discounted_return = np.mean(\n",
        "                [special.discount_return(path[\"rewards\"], self.discount) for path in paths]\n",
        "            )\n",
        "\n",
        "            returns = [sum(path[\"rewards\"]) for path in paths]\n",
        "            \n",
        "            average_action = np.mean(np.square(np.concatenate(\n",
        "                                    [path[\"actions\"] for path in paths])))\n",
        "            \n",
        "            logger.record_tabular('AverageReturn', np.mean(returns))\n",
        "            logger.record_tabular('StdReturn', np.std(returns))\n",
        "            logger.record_tabular('MaxReturn', np.max(returns))\n",
        "            logger.record_tabular('MinReturn', np.min(returns))\n",
        "            \n",
        "            logger.record_tabular('AverageDiscountedReturn', average_discounted_return)\n",
        "            \n",
        "            logger.record_tabular('AverageAction', average_action)\n",
        "            \n",
        "            self.env.log_diagnostics(paths)\n",
        "            self.policy.log_diagnostics(paths)\n",
        "\n",
        "        all_qs = np.concatenate(self.q_averages)\n",
        "        all_ys = np.concatenate(self.y_averages)\n",
        "\n",
        "        average_q_loss = np.mean(self.qf_loss_averages)\n",
        "        \n",
        "\n",
        "        policy_reg_param_norm = np.linalg.norm(\n",
        "            self.policy.get_param_values(regularizable=True)\n",
        "        )\n",
        "#         qfun_reg_param_norm = np.linalg.norm(\n",
        "#             self.qf.get_param_values(regularizable=True)\n",
        "#         )\n",
        "          \n",
        "        if len(self.es_path_returns) > 0:\n",
        "            logger.record_tabular('AverageEsReturn',\n",
        "                                  np.mean(self.es_path_returns))\n",
        "            logger.record_tabular('StdEsReturn',\n",
        "                                  np.std(self.es_path_returns))\n",
        "            logger.record_tabular('MaxEsReturn',\n",
        "                                  np.max(self.es_path_returns))\n",
        "            logger.record_tabular('MinEsReturn',\n",
        "                                  np.min(self.es_path_returns))\n",
        "            logger.record_tabular('NbEs', len(self.es_path_returns))\n",
        "        \n",
        "        logger.record_tabular('AverageQLoss', average_q_loss)\n",
        "        logger.record_tabular('AverageQ', np.mean(all_qs))\n",
        "        logger.record_tabular('AverageAbsQ', np.mean(np.abs(all_qs)))\n",
        "        logger.record_tabular('AverageY', np.mean(all_ys))\n",
        "        logger.record_tabular('AverageAbsY', np.mean(np.abs(all_ys)))\n",
        "        logger.record_tabular('AverageAbsQYDiff',\n",
        "                              np.mean(np.abs(all_qs - all_ys)))\n",
        "\n",
        "        logger.record_tabular('PolicyRegParamNorm',\n",
        "                              policy_reg_param_norm)\n",
        "\n",
        "        self.qf_loss_averages = []\n",
        "\n",
        "        self.q_averages = []\n",
        "        self.y_averages = []\n",
        "        self.es_path_returns = []\n",
        "\n",
        "    def update_plot(self):\n",
        "        if self.plot:\n",
        "            plotter.update_plot(self.policy, self.max_path_length)\n",
        "\n",
        "    def get_epoch_snapshot(self, epoch):\n",
        "        return dict(\n",
        "            env=self.env,\n",
        "            epoch=epoch,\n",
        "            policy=self.policy,\n",
        "            target_policy=self.opt_info[\"target_policy\"],\n",
        "            es=self.es,\n",
        "        )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xF6kAwo57hb6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**2. Execute Your Algorithm**"
      ]
    },
    {
      "metadata": {
        "id": "UjHQm_6t7wK1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Activate a virtual display"
      ]
    },
    {
      "metadata": {
        "id": "oIUuJMGv7u8O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "import os\n",
        "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x6yM_2LavPBR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Create & Run a RL task for Cartpole"
      ]
    },
    {
      "metadata": {
        "id": "74oGATzdIO9b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Creating & Running a task #####\n",
        "\n",
        "\n",
        "def run_task(*_):\n",
        "    env = GymEnv('Acrobot-v1', record_video=True, record_log=True)\n",
        "    \n",
        "    policy = CategoricalMlpQPolicy(\n",
        "        name='dqn_policy',\n",
        "        env_spec=env.spec,\n",
        "        # The neural network policy should have two hidden layers, each with 32 hidden units.\n",
        "        hidden_sizes=[64],\n",
        "        hidden_nonlinearity=NL.rectify\n",
        "    )\n",
        "    \n",
        "    n_steps = 80000\n",
        "    es = EpsilonGreedyStrategy(env_spec=env.spec, max_eps=0.5, min_eps=0.05, decay_period=n_steps//4)\n",
        "    \n",
        "    algo = DDQN(\n",
        "        env=env,\n",
        "        policy=policy,\n",
        "        es=es,\n",
        "        n_steps=n_steps,\n",
        "        min_pool_size=100,\n",
        "        replay_pool_size=200,\n",
        "        train_epoch_interval=1000,\n",
        "        max_path_length=500,\n",
        "        policy_update_method='sgd',\n",
        "        policy_learning_rate=0.0005,\n",
        "        target_model_update=0.5,\n",
        "        n_eval_samples=0,\n",
        "        batch_size=20,\n",
        "    #     batch_size=32,\n",
        "    #     max_path_length=100,\n",
        "    #     epoch_length=1000,\n",
        "    #     min_pool_size=10000,\n",
        "    #     n_epochs=1000,\n",
        "    #     discount=0.99,\n",
        "    #     scale_reward=0.01,\n",
        "    #     qf_learning_rate=1e-3,\n",
        "    #     policy_learning_rate=1e-4,\n",
        "        # Uncomment both lines (this and the plot parameter below) to enable plotting\n",
        "        # plot=True,\n",
        "    )\n",
        "    algo.train()\n",
        "\n",
        "#################################################################################\n",
        "# Set up path with corresponding environment name and algorithm name to save logs\n",
        "# Ex) mypath = '/content/acrobot_dqn'\n",
        "\n",
        "mypath = './acrobot_dqn++'\n",
        "#################################################################################\n",
        "\n",
        "run_experiment_lite(\n",
        "    run_task,\n",
        "    log_dir=mypath,\n",
        "    # Number of parallel workers for sampling\n",
        "    n_parallel=1,\n",
        "    # Only keep the snapshot parameters for the last iteration\n",
        "    snapshot_mode=\"last\",\n",
        "    # Specifies the seed for the experiment. If this is not provided, a random seed\n",
        "    # will be used\n",
        "    seed=1,\n",
        "    # plot=True,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sdDOU5hwKlW4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**3. Average Reward Plotting**"
      ]
    },
    {
      "metadata": {
        "id": "ip1bXHMtvtWy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- You can evaluate how your agent is being trained with reward it gets in every iteration. \n",
        "- Whenever you execute the code 'run_experiment_lite', it will generate a experiment directory.\n",
        "- (/content/cartpole_dqn/)\n",
        "- Please update the value of '**mypath**' and specify your new experiment directory name. "
      ]
    },
    {
      "metadata": {
        "id": "RlIgTMPOKqHX",
        "colab_type": "code",
        "outputId": "fb3b5067-e6c1-4df7-acc3-3d2f4d78b4cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "Dimport os.path as osp\n",
        "import numpy as np\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import joblib\n",
        "from glob import glob\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "plots = []\n",
        "legends = []\n",
        "returns = []\n",
        "with open(osp.join(mypath, 'progress.csv'), 'rt') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "        if row['AverageEsReturn']:\n",
        "            returns.append(float(row['AverageEsReturn']))\n",
        "returns = np.array(returns)\n",
        "plots.append(plt.plot(returns)[0])\n",
        "legends.append('AverageEsReturn')\n",
        "plt.legend(plots, legends)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFKCAYAAADScRzUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9v+JMhbdZramSwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fc4e8fd9588>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "rWDQUoPePLDx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**4. Play Videos of your Agent Behavior**"
      ]
    },
    {
      "metadata": {
        "id": "AhdxUQuozcRv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- You can watch how your agent's behavior improves.\n",
        "- If you haven't update 'mypath' in the code above, you need to update it here.\n",
        "- (/content/cartpole_dqn/)"
      ]
    },
    {
      "metadata": {
        "id": "TPjtJ3IjPY5A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython import display as pythondisplay\n",
        "# from pyvirtualdisplay import Display\n",
        "\n",
        "# from matplotlib.pyplot import imshow\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from JSAnimation import IPython_display\n",
        "from IPython.display import HTML\n",
        "\n",
        "import imageio\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "def plot_movie_js(image_array, filename):\n",
        "    dpi = 10.0\n",
        "    xpixels, ypixels = image_array[0].shape[0], image_array[0].shape[1]\n",
        "    fig = plt.figure(figsize=(ypixels/(dpi), xpixels/(dpi)), dpi=dpi)\n",
        "    fig.suptitle(filename, fontsize=160)\n",
        "    # fig.set_xlabel(filename, fontsize=160)\n",
        "    # fig.xlabel(filename, fontsize=160)\n",
        "    im = plt.figimage(image_array[0])\n",
        "\n",
        "    def animate(i):\n",
        "        im.set_array(image_array[i])\n",
        "        return (im,)\n",
        "    \n",
        "    anim = animation.FuncAnimation(fig, animate, frames=len(image_array))\n",
        "    pythondisplay.display(IPython_display.display_animation(anim))\n",
        "\n",
        "#mypath = '/content/cartpole_dqn/'\n",
        "mypath += 'gym_log/'\n",
        "mp4files = [f for f in listdir(mypath) if f.endswith(\".mp4\")]\n",
        "mp4files.sort()\n",
        "\n",
        "\n",
        "for filename in mp4files:\n",
        "    vid = imageio.get_reader(join(mypath, filename),  'ffmpeg')\n",
        "    # print(len(vid))\n",
        "    # print(vid.get_data(0).shape)\n",
        "\n",
        "    screenlist = []\n",
        "    for i in range(len(vid)):\n",
        "        image = vid.get_data(i)\n",
        "        screenlist.append(image)\n",
        "        # fig = plt.figure()\n",
        "        # fig.suptitle('image #{}'.format(i), fontsize=20)\n",
        "        # plt.imshow(image)\n",
        "\n",
        "    plot_movie_js(screenlist, filename)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}