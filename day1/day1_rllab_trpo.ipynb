{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"day1_rllab_trpo.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"H6rtQMrJq4y5","colab_type":"text"},"cell_type":"markdown","source":["# Simple example running TRPO on CartPole (OpenAI Gym version)\n","\n","**Important!! ** Before running the following cell, make sure rllab is set up properly in your **current** runtime by executing codes in **day1_rllab_setup.ipynb** \n","\n","Also, when run for the first time, the code will exit without training, just creating the personal profile. If this happens, just run the code again."]},{"metadata":{"id":"ldsoWZsEtvsq","colab_type":"code","colab":{}},"cell_type":"code","source":["from rllab.algos.trpo import TRPO\n","from rllab.baselines.linear_feature_baseline import LinearFeatureBaseline\n","from rllab.envs.gym_env import GymEnv\n","from rllab.envs.normalized_env import normalize\n","from rllab.misc.instrument import run_experiment_lite\n","from rllab.policies.categorical_mlp_policy import CategoricalMLPPolicy\n","\n","\n","# Please note that different environments with different action spaces may\n","# require different policies. For example with a Discrete action space, a\n","# CategoricalMLPPolicy works, but for a Box action space may need to use\n","# a GaussianMLPPolicy (see the trpo_gym_pendulum.py example)\n","env = normalize(GymEnv(\"CartPole-v0\", record_video=False))\n","\n","policy = CategoricalMLPPolicy(\n","    env_spec=env.spec,\n","    # The neural network policy should have two hidden layers, each with 32 hidden units.\n","    hidden_sizes=(32, 32)\n",")\n","\n","baseline = LinearFeatureBaseline(env_spec=env.spec)\n","\n","algo = TRPO(\n","    env=env,\n","    policy=policy,\n","    baseline=baseline,\n","    batch_size=4000,\n","    max_path_length=env.horizon,\n","    n_itr=50,\n","    discount=0.99,\n","    step_size=0.01,\n","    # Uncomment both lines (this and the plot parameter below) to enable plotting\n","    # plot=True,\n",")\n","\n","algo.train()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nIeBTaZ3uI_L","colab_type":"text"},"cell_type":"markdown","source":["A better way to conduct experiments is to use **run_experiment_lite** as follows: (can't render videos in colab)"]},{"metadata":{"id":"jVdzZRiciemp","colab_type":"code","colab":{}},"cell_type":"code","source":["from rllab.algos.trpo import TRPO\n","from rllab.baselines.linear_feature_baseline import LinearFeatureBaseline\n","from rllab.envs.gym_env import GymEnv\n","from rllab.envs.normalized_env import normalize\n","from rllab.misc.instrument import run_experiment_lite\n","from rllab.policies.categorical_mlp_policy import CategoricalMLPPolicy\n","\n","\n","def run_task(*_):\n","    # Please note that different environments with different action spaces may\n","    # require different policies. For example with a Discrete action space, a\n","    # CategoricalMLPPolicy works, but for a Box action space may need to use\n","    # a GaussianMLPPolicy (see the trpo_gym_pendulum.py example)\n","    env = normalize(GymEnv(\"CartPole-v0\", record_video=False))\n","\n","    policy = CategoricalMLPPolicy(\n","        env_spec=env.spec,\n","        # The neural network policy should have two hidden layers, each with 32 hidden units.\n","        hidden_sizes=(32, 32)\n","    )\n","\n","    baseline = LinearFeatureBaseline(env_spec=env.spec)\n","\n","    algo = TRPO(\n","        env=env,\n","        policy=policy,\n","        baseline=baseline,\n","        batch_size=4000,\n","        max_path_length=env.horizon,\n","        n_itr=50,\n","        discount=0.99,\n","        step_size=0.01,\n","        # Uncomment both lines (this and the plot parameter below) to enable plotting\n","        # plot=True,\n","    )\n","    algo.train()\n","\n","\n","run_experiment_lite(\n","    run_task,\n","    # python_command='!/usr/local/bin/python',\n","    # Number of parallel workers for sampling\n","    n_parallel=1,\n","    # Only keep the snapshot parameters for the last iteration\n","    snapshot_mode=\"last\",\n","    # Specifies the seed for the experiment. If this is not provided, a random seed\n","    # will be used\n","    seed=1,\n","    # plot=True,\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5B6zWu1C9g0Z","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}