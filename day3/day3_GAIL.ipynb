{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day3_GAIL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4Vtb_VJITO3T"
      },
      "source": [
        "# GAIL (Generative Adversarial Imitation Learning) 실습\n",
        "\n",
        "이번 실습자료는 GAIL 저자 (Jonathan Ho)의  [구현코드](/https://github.com/openai/imitation.git)를 바탕으로 제작되었습니다. 직접적인 구현 자체 (`TrainsitionClassifier`, `ImitationOptimizer`) 위주로 실습자료를 만들었고, 구현에 필요한 유틸리티 메서드들은 저자코드 저장소에서 그대로 받아와서 사용하도록 제작했습니다.\n",
        "\n",
        "## 환경설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oD1K_HR1HW9H"
      },
      "outputs": [],
      "source": [
        "!rm -rf /usr/local/lib/python2.7/dist-packages/gym; rm -rf /usr/local/lib/python2.7/dist-packages/pyglet\n",
        "!pip install virtualenv\n",
        "!virtualenv gailenv\n",
        "!apt-get install -y xvfb python-opengl swig ffmpeg zlib1g-dev python3-tk\n",
        "!source /content/gailenv/bin/activate; pip install numpy==1.10.4 theano==0.8.2 gym==0.1.0 mujoco_py==0.4.0 JSAnimation PyOpenGL piglet pyglet==1.3.2 xlib pyvirtualdisplay box2d-py mako==1.0.7 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qenMm72mEd8g"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/gailenv/lib/python2.7/site-packages')\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "import os\n",
        "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pxv1IalyQuPH"
      },
      "outputs": [],
      "source": [
        "!git init .\n",
        "!git remote add origin https://github.com/tzs930/imitation.git\n",
        "!git pull origin master"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sEEyhSrqTB3I"
      },
      "source": [
        "## Hyperparameter 설정\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LT80O2WW55ip"
      },
      "source": [
        "### 데이터 입력 관련\n",
        "\n",
        "- `data` : 전문가 행동데이터(Expert trajectory)들이 저장된 `.h5` 파일의 경로를 지정합니다.\n",
        "- `limit_trajs` : 최대 몇개의 Expert episode를 가져올지 지정합니다. 작아질수록 학습에 들어가는 데이터의 양이 적어집니다.\n",
        "- `data_subsamp_freq` : 한 Episode 내에서 Subsampling을 얼마나 할지 지정합니다. 1일 경우 Subsampling을 하지 않고 모든 데이터를 사용합니다.\n",
        "\n",
        "### Gym 환경 관련\n",
        "- `env_name` : 어떤 Gym Environment에서 실험할 것인지 지정합니다. 반드시 `gym`에 등록(register)된 환경이어야 합니다. \n",
        "> e.g. `CartPole-v0`, `Acrobot-v0`, `MountainCar-v0`, ...\n",
        "- `max_traj_len` :  Environment의 최대 에피소드 길이 값을 지정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oydeE44w3FDH"
      },
      "outputs": [],
      "source": [
        "# Expert dataset\n",
        "data='expert_trajs/trajs_cartpole.h5'\n",
        "limit_trajs=25\n",
        "data_subsamp_freq=1\n",
        "\n",
        "# MDP options\n",
        "env_name='CartPole-v0'\n",
        "max_traj_len=1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CCttOHW-tXHh"
      },
      "source": [
        "### 학습 진행 관련 \n",
        "- `min_total_sa` : 1번의 Policy Optimization에서 최대 몇개의 (s, a) 쌍을 샘플할 것인지 지정합니다.\n",
        "- `max_iter` :  최대 몇 번의 Iteration을 학습할 것인지 지정합니다. \n",
        "\n",
        "한 번의 Iteration은 outer-loop에서 *Reward 학습*, inner-loop에서 *Policy 학습* 의 2단계로 이루어지며,  Inner loop에서 샘플된 (s,a)을 이용하여 TRPO를 학습하므로 사실상  `min_total_sa * max_iter` 만큼의 학습이 진행되게 됩니다.\n",
        "\n",
        "\n",
        "### 저장 및 출력 관련 \n",
        "\n",
        "- `print_freq` : 학습 도중 결과를 얼마나 자주 출력할지 지정합니다.\n",
        "- `save_freq` :  학습 도중 결과 및 모델을 얼마나 자주 저장할지 지정합니다.\n",
        "- `plot_freq` : 학습 도중 그래프를 얼마나 자주 출력할지 지정합니다.\n",
        "- `logfilename` : 저장될 로그 및 학습결과의 경로를 지정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "L3w6OVgFtVoX"
      },
      "outputs": [],
      "source": [
        "# Learning hyperparameters\n",
        "min_total_sa=50000\n",
        "max_iter=10\n",
        "\n",
        "# Saving hyperparameters\n",
        "print_freq=1\n",
        "save_freq=10\n",
        "plot_freq=10\n",
        "logfilename='./' + env_name + '.h5'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sVIvujnv5uQq"
      },
      "source": [
        "### Reward 학습 관련 \n",
        "- `reward_steps` : 한 번의 Reward optimization마다 몇 step의 update를 할 것인지 지정합니다.\n",
        "- `reward_lr` :  Reward의 Learning Rate를 지정합니다.\n",
        "- `reward_ent_reg_weight` :  Reward Loss에 적용되는 Entropy Regularization 정도를 지정합니다.\n",
        "- `favor_zero_expert_reward` : Discriminator가 Expert trajectory를 1 또는 0으로 판단할 지 지정합니다. 가능한 한 빨리 도달해야하는 태스크들(ex. `MountainCar-v0`) 의 경우 이 옵션(`favor_zero_expert_reward=1`)이 필요합니다.\n",
        "\n",
        "\n",
        "### TRPO (Trust-Region Policy Optimization) 관련 \n",
        "\n",
        "GAIL은 inner-loop에서 RL Optimization을 이용하여 Policy를 학습시키는데, TRPO [(Schulman. J. et al., 2015)](https://arxiv.org/abs/1502.05477)를 사용하여 학습합니다. 이번 실습에서는 TRPO가 중점이 아니므로 이 부분은 생략합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0-eRYwWt5uy5"
      },
      "outputs": [],
      "source": [
        "# Reward hyperparameters\n",
        "reward_steps=1\n",
        "reward_lr=.01\n",
        "reward_ent_reg_weight=.001\n",
        "favor_zero_expert_reward=0\n",
        "\n",
        "# TRPO hyperparameters\n",
        "discount=.995\n",
        "lam=.97\n",
        "policy_max_kl=.01\n",
        "policy_cg_damping=.1\n",
        "vf_max_kl=.01\n",
        "vf_cg_damping=.1\n",
        "policy_ent_reg=0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QbVe_S6WU_Gy"
      },
      "source": [
        "## Generative Adversarial Learning을 통해 Reward를 학습 : `TrainsitionClassifier`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TWDGpYQV69YB"
      },
      "source": [
        "GAIL에서는 Discriminator를 local reward 혹은 cost function으로 삼아서 학습을 하게 됩니다. 따라서, $w$는 Discriminator의 weight, $D_w : \\mathcal S \\times \\mathcal A  \\to (0,1)$ 는 Discriminator network라고 하면  reward function는 다음과 같습니다. \n",
        "\n",
        "\\begin{equation}\\\n",
        "r(s,a) = \\log D_w (s,a)\n",
        "\\end{equation}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xnMTEwCH9j4L"
      },
      "outputs": [],
      "source": [
        "# os.environ['PYTHONPATH'] = '/content'\n",
        "from policyopt import nn, rl, util, RaggedArray, ContinuousSpace, FiniteSpace, optim, thutil\n",
        "import numpy as np\n",
        "from contextlib import contextmanager\n",
        "import theano; from theano import tensor\n",
        "\n",
        "class TransitionClassifier(nn.Model):\n",
        "    '''Reward/adversary for generative-adversarial training'''\n",
        "\n",
        "    def __init__(self, obsfeat_space, action_space, hidden_spec, adam_lr, adam_steps, ent_reg_weight,\n",
        "                 enable_inputnorm, include_time, time_scale, favor_zero_expert_reward, varscope_name):\n",
        "        self.obsfeat_space, self.action_space = obsfeat_space, action_space\n",
        "        self.hidden_spec = hidden_spec        \n",
        "        self.adam_steps = adam_steps\n",
        "        self.ent_reg_weight = ent_reg_weight; assert ent_reg_weight >= 0\n",
        "        self.include_time = include_time\n",
        "        self.time_scale = time_scale\n",
        "        self.favor_zero_expert_reward = favor_zero_expert_reward\n",
        "\n",
        "        with nn.variable_scope(varscope_name) as self.__varscope:\n",
        "            # Map (s,a) pairs to classifier scores (log probabilities of classes)\n",
        "            obsfeat_B_Df = tensor.matrix(name='obsfeat_B_Df')\n",
        "            a_B_Da = tensor.matrix(name='a_B_Da', dtype=theano.config.floatX if self.action_space.storage_type == float else 'int64')\n",
        "            t_B = tensor.vector(name='t_B')\n",
        "\n",
        "            scaled_t_B = self.time_scale * t_B\n",
        "\n",
        "            if isinstance(self.action_space, ContinuousSpace):\n",
        "                # For a continuous action space, map observation-action pairs to a real number (reward)\n",
        "                trans_B_Doa = tensor.concatenate([obsfeat_B_Df, a_B_Da], axis=1)\n",
        "                trans_dim = self.obsfeat_space.dim + self.action_space.dim\n",
        "                # Normalize\n",
        "                with nn.variable_scope('inputnorm'):\n",
        "                    self.inputnorm = (nn.Standardizer if enable_inputnorm else nn.NoOpStandardizer)(self.obsfeat_space.dim + self.action_space.dim)\n",
        "                normedtrans_B_Doa = self.inputnorm.standardize_expr(trans_B_Doa)\n",
        "                if self.include_time:\n",
        "                    net_input = tensor.concatenate([normedtrans_B_Doa, scaled_t_B[:,None]], axis=1)\n",
        "                    net_input_dim = trans_dim + 1\n",
        "                else:\n",
        "                    net_input = normedtrans_B_Doa\n",
        "                    net_input_dim = trans_dim\n",
        "                # Compute scores\n",
        "                with nn.variable_scope('hidden'):\n",
        "                    net = nn.FeedforwardNet(net_input, (net_input_dim,), self.hidden_spec)\n",
        "                with nn.variable_scope('out'):\n",
        "                    out_layer = nn.AffineLayer(net.output, net.output_shape, (1,), initializer=np.zeros((net.output_shape[0], 1)))\n",
        "                scores_B = out_layer.output[:,0]\n",
        "\n",
        "            else:\n",
        "                # For a finite action space, map observation observations to a vector of rewards\n",
        "\n",
        "                # Normalize observations\n",
        "                with nn.variable_scope('inputnorm'):\n",
        "                    self.inputnorm = (nn.Standardizer if enable_inputnorm else nn.NoOpStandardizer)(self.obsfeat_space.dim)\n",
        "                normedobs_B_Df = self.inputnorm.standardize_expr(obsfeat_B_Df)\n",
        "                if self.include_time:\n",
        "                    net_input = tensor.concatenate([normedobs_B_Df, scaled_t_B[:,None]], axis=1)\n",
        "                    net_input_dim = self.obsfeat_space.dim + 1\n",
        "                else:\n",
        "                    net_input = normedobs_B_Df\n",
        "                    net_input_dim = self.obsfeat_space.dim\n",
        "                # Compute scores\n",
        "                with nn.variable_scope('hidden'):\n",
        "                    net = nn.FeedforwardNet(net_input, (net_input_dim,), self.hidden_spec)\n",
        "                with nn.variable_scope('out'):\n",
        "                    out_layer = nn.AffineLayer(\n",
        "                        net.output, net.output_shape, (self.action_space.size,),\n",
        "                        initializer=np.zeros((net.output_shape[0], self.action_space.size)))\n",
        "                scores_B = out_layer.output[tensor.arange(normedobs_B_Df.shape[0]), a_B_Da[:,0]]\n",
        "\n",
        "\n",
        "        if self.include_time:\n",
        "            self._compute_scores = thutil.function([obsfeat_B_Df, a_B_Da, t_B], scores_B) # scores define the conditional distribution p(label | (state,action))\n",
        "            \n",
        "        else:\n",
        "            compute_scores_without_time = thutil.function([obsfeat_B_Df, a_B_Da], scores_B)\n",
        "            self._compute_scores = lambda _obsfeat_B_Df, _a_B_Da, _t_B: compute_scores_without_time(_obsfeat_B_Df, _a_B_Da)\n",
        "\n",
        "        if self.favor_zero_expert_reward:\n",
        "            # 0 for expert-like states, goes to -inf for non-expert-like states\n",
        "            # compatible with envs with traj cutoffs for good (expert-like) behavior\n",
        "            # e.g. mountain car, which gets cut off when the car reaches the destination\n",
        "            rewards_B = thutil.logsigmoid(scores_B)\n",
        "            \n",
        "        else:\n",
        "            # 0 for non-expert-like states, goes to +inf for expert-like states\n",
        "            # compatible with envs with traj cutoffs for bad (non-expert-like) behavior\n",
        "            # e.g. walking simulations that get cut off when the robot falls over\n",
        "            rewards_B = -tensor.log(1.-tensor.nnet.sigmoid(scores_B))\n",
        "            \n",
        "        if self.include_time:\n",
        "            self._compute_reward = thutil.function([obsfeat_B_Df, a_B_Da, t_B], rewards_B)\n",
        "            \n",
        "        else:\n",
        "            compute_reward_without_time = thutil.function([obsfeat_B_Df, a_B_Da], rewards_B)\n",
        "            self._compute_reward = lambda _obsfeat_B_Df, _a_B_Da, _t_B: compute_reward_without_time(_obsfeat_B_Df, _a_B_Da)\n",
        "\n",
        "        param_vars = self.get_trainable_variables()\n",
        "\n",
        "        # Logistic regression loss, regularized by negative entropy\n",
        "        labels_B = tensor.vector(name='labels_B')\n",
        "        weights_B = tensor.vector(name='weights_B')\n",
        "        losses_B = thutil.sigmoid_cross_entropy_with_logits(scores_B, labels_B)\n",
        "        ent_B = thutil.logit_bernoulli_entropy(scores_B)\n",
        "        loss = ((losses_B - self.ent_reg_weight*ent_B)*weights_B).sum(axis=0)\n",
        "        lossgrad_P = thutil.flatgrad(loss, param_vars)\n",
        "\n",
        "        if self.include_time:\n",
        "            self._adamstep = thutil.function(\n",
        "                [obsfeat_B_Df, a_B_Da, t_B, labels_B, weights_B], loss,\n",
        "                updates=thutil.adam(loss, param_vars, lr=adam_lr))\n",
        "        else:\n",
        "            adamstep_without_time = thutil.function(\n",
        "                [obsfeat_B_Df, a_B_Da, labels_B, weights_B], loss,\n",
        "                updates=thutil.adam(loss, param_vars, lr=adam_lr))\n",
        "            self._adamstep = lambda _obsfeat_B_Df, _a_B_Da, _t_B, _labels_B, _weights_B: adamstep_without_time(_obsfeat_B_Df, _a_B_Da, _labels_B, _weights_B)\n",
        "\n",
        "    @property\n",
        "    def varscope(self): return self.__varscope\n",
        "\n",
        "    def compute_reward(self, obsfeat_B_Df, a_B_Da, t_B):\n",
        "        return self._compute_reward(obsfeat_B_Df, a_B_Da, t_B)\n",
        "\n",
        "    def fit(self, obsfeat_B_Df, a_B_Da, t_B, exobs_Bex_Do, exa_Bex_Da, ext_Bex):\n",
        "        # Transitions from the current policy go first, then transitions from the expert\n",
        "        obsfeat_Ball_Df = np.concatenate([obsfeat_B_Df, exobs_Bex_Do])\n",
        "        a_Ball_Da = np.concatenate([a_B_Da, exa_Bex_Da])\n",
        "        t_Ball = np.concatenate([t_B, ext_Bex])\n",
        "\n",
        "        # Update normalization\n",
        "        self.update_inputnorm(obsfeat_Ball_Df, a_Ball_Da)\n",
        "\n",
        "        B = obsfeat_B_Df.shape[0] # number of examples from the current policy\n",
        "        Ball = obsfeat_Ball_Df.shape[0] # Ball - b = num examples from expert\n",
        "\n",
        "        # Label expert as 1, current policy as 0\n",
        "        labels_Ball = np.zeros(Ball)\n",
        "        labels_Ball[B:] = 1.\n",
        "\n",
        "        # Evenly weight the loss terms for the expert and the current policy\n",
        "        weights_Ball = np.zeros(Ball)\n",
        "        weights_Ball[:B] = 1./B\n",
        "        weights_Ball[B:] = 1./(Ball - B); assert len(weights_Ball[B:]) == Ball-B\n",
        "\n",
        "        # Optimize\n",
        "        for _ in range(self.adam_steps):\n",
        "            loss, kl, num_bt_steps = self._adamstep(obsfeat_Ball_Df, a_Ball_Da, t_Ball, labels_Ball, weights_Ball), None, 0\n",
        "\n",
        "        # Evaluate\n",
        "        scores_Ball = self._compute_scores(obsfeat_Ball_Df, a_Ball_Da, t_Ball); assert scores_Ball.shape == (Ball,)\n",
        "        accuracy = .5 * (weights_Ball * ((scores_Ball < 0) == (labels_Ball == 0))).sum()\n",
        "        accuracy_for_currpolicy = (scores_Ball[:B] <= 0).mean()\n",
        "        accuracy_for_expert = (scores_Ball[B:] > 0).mean()\n",
        "        assert np.allclose(accuracy, .5*(accuracy_for_currpolicy + accuracy_for_expert))\n",
        "\n",
        "        return [\n",
        "            ('rloss', loss, float), # reward function fitting loss\n",
        "            ('racc', accuracy, float), # reward function accuracy\n",
        "            ('raccpi', accuracy_for_currpolicy, float), # reward function accuracy\n",
        "            ('raccex', accuracy_for_expert, float), # reward function accuracy\n",
        "            ('rkl', kl, float),\n",
        "            ('rbt', num_bt_steps, int),\n",
        "            # ('rpnorm', util.maxnorm(self.get_params()), float),\n",
        "            # ('snorm', util.maxnorm(scores_Ball), float),\n",
        "        ]\n",
        "\n",
        "    def update_inputnorm(self, obs_B_Do, a_B_Da):\n",
        "        if isinstance(self.action_space, ContinuousSpace):\n",
        "            self.inputnorm.update(np.concatenate([obs_B_Do, a_B_Da], axis=1))\n",
        "        else:\n",
        "            self.inputnorm.update(obs_B_Do)\n",
        "\n",
        "    def plot(self, ax, idx1, idx2, range1, range2, n=100):\n",
        "        assert len(range1) == len(range2) == 2 and idx1 != idx2\n",
        "        x, y = np.mgrid[range1[0]:range1[1]:(n+0j), range2[0]:range2[1]:(n+0j)]\n",
        "\n",
        "        if isinstance(self.action_space, ContinuousSpace):\n",
        "            points_B_Doa = np.zeros((n*n, self.obsfeat_space.storage_size + self.action_space.storage_size))\n",
        "            points_B_Doa[:,idx1] = x.ravel()\n",
        "            points_B_Doa[:,idx2] = y.ravel()\n",
        "            obsfeat_B_Df, a_B_Da = points_B_Doa[:,:self.obsfeat_space.storage_size], points_B_Doa[:,self.obsfeat_space.storage_size:]\n",
        "            assert a_B_Da.shape[1] == self.action_space.storage_size\n",
        "            t_B = np.zeros(a_B_Da.shape[0]) # XXX make customizable\n",
        "            z = self.compute_reward(obsfeat_B_Df, a_B_Da, t_B).reshape(x.shape)\n",
        "        else:\n",
        "            obsfeat_B_Df = np.zeros((n*n, self.obsfeat_space.storage_size))\n",
        "            obsfeat_B_Df[:,idx1] = x.ravel()\n",
        "            obsfeat_B_Df[:,idx2] = y.ravel()\n",
        "            a_B_Da = np.zeros((obsfeat_B_Df.shape[0], 1), dtype=np.int32) # XXX make customizable\n",
        "            t_B = np.zeros(a_B_Da.shape[0]) # XXX make customizable\n",
        "            z = self.compute_reward(obsfeat_B_Df, a_B_Da, t_B).reshape(x.shape)\n",
        "\n",
        "        ax.pcolormesh(x, y, z, cmap='viridis')\n",
        "        # ax.contour(x, y, z, levels=np.log(np.linspace(2., 3., 10)))\n",
        "        ax.contourf(x, y, z, levels=[np.log(2.), np.log(2.)+.5], alpha=.5) # high-reward region is highlighted\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Yi7djwfqVWXF"
      },
      "source": [
        "## 학습한 Reward로 Policy Optimization :  `ImitationOptimizer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5tMesV3FBai3"
      },
      "source": [
        "여기서는 Discriminator의 Parameter인 $w$를 다음과 같이 학습하는 부분을 포함하고 있습니다.\n",
        "\n",
        "\\begin{equation}\\\n",
        "w_{i+1} \\leftarrow w_i - \\eta \\space \\mathbb{E}_{\\tau_i} [ \\nabla_w log(D_w (s,a))] + \\mathbb E_{\\tau_E} [\\nabla_w \\log(1-D_w(s,a))] \\tag{17}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3Cl38j-S-fge"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "class ImitationOptimizer(object):\n",
        "    def __init__(self, mdp, discount, lam, policy, sim_cfg, step_func, reward_func, value_func,\n",
        "                 policy_obsfeat_fn, reward_obsfeat_fn, policy_ent_reg, ex_obs, ex_a, ex_t):\n",
        "        self.mdp, self.discount, self.lam, self.policy = mdp, discount, lam, policy\n",
        "        self.sim_cfg = sim_cfg\n",
        "        self.step_func = step_func\n",
        "        self.reward_func = reward_func\n",
        "        self.value_func = value_func\n",
        "        # assert value_func is not None, 'not tested'\n",
        "        self.policy_obsfeat_fn = policy_obsfeat_fn\n",
        "        self.reward_obsfeat_fn = reward_obsfeat_fn\n",
        "        self.policy_ent_reg = policy_ent_reg\n",
        "        util.header('Policy entropy regularization: {}'.format(self.policy_ent_reg))\n",
        "\n",
        "        assert ex_obs.ndim == ex_a.ndim == 2 and ex_t.ndim == 1 and ex_obs.shape[0] == ex_a.shape[0] == ex_t.shape[0]\n",
        "        self.ex_pobsfeat, self.ex_robsfeat, self.ex_a, self.ex_t = policy_obsfeat_fn(ex_obs), reward_obsfeat_fn(ex_obs), ex_a, ex_t\n",
        "\n",
        "        self.total_num_trajs = 0\n",
        "        self.total_num_sa = 0\n",
        "        self.total_time = 0.\n",
        "        self.curr_iter = 0\n",
        "        self.last_sampbatch = None # for outside access for debugging\n",
        "\n",
        "    def step(self):\n",
        "        with util.Timer() as t_all:\n",
        "\n",
        "            # Sample trajectories using current policy\n",
        "            # print 'Sampling'\n",
        "            with util.Timer() as t_sample:\n",
        "                sampbatch = self.mdp.sim_mp(\n",
        "                    policy_fn=lambda obsfeat_B_Df: self.policy.sample_actions(obsfeat_B_Df),\n",
        "                    obsfeat_fn=self.policy_obsfeat_fn,\n",
        "                    cfg=self.sim_cfg)\n",
        "                samp_pobsfeat = sampbatch.obsfeat\n",
        "                self.last_sampbatch = sampbatch\n",
        "\n",
        "            # Compute baseline / advantages\n",
        "            # print 'Computing advantages'\n",
        "            with util.Timer() as t_adv:\n",
        "                # Compute observation features for reward input\n",
        "                samp_robsfeat_stacked = self.reward_obsfeat_fn(sampbatch.obs.stacked)\n",
        "                # Reward is computed wrt current reward function\n",
        "                # TODO: normalize rewards\n",
        "                rcurr_stacked = self.reward_func.compute_reward(samp_robsfeat_stacked, sampbatch.a.stacked, sampbatch.time.stacked)\n",
        "                assert rcurr_stacked.shape == (samp_robsfeat_stacked.shape[0],)\n",
        "\n",
        "                # If we're regularizing the policy, add negative log probabilities to the rewards\n",
        "                # Intuitively, the policy gets a bonus for being less certain of its actions\n",
        "                orig_rcurr_stacked = rcurr_stacked.copy()\n",
        "                if self.policy_ent_reg is not None and self.policy_ent_reg != 0:\n",
        "                    assert self.policy_ent_reg > 0\n",
        "                    # XXX probably faster to compute this from sampbatch.adist instead\n",
        "                    actionlogprobs_B = self.policy.compute_action_logprobs(samp_pobsfeat.stacked, sampbatch.a.stacked)\n",
        "                    policyentbonus_B = -self.policy_ent_reg * actionlogprobs_B\n",
        "                    rcurr_stacked += policyentbonus_B\n",
        "                else:\n",
        "                    policyentbonus_B = np.zeros_like(rcurr_stacked)\n",
        "\n",
        "                rcurr = RaggedArray(rcurr_stacked, lengths=sampbatch.r.lengths)\n",
        "\n",
        "                # Compute advantages using these rewards\n",
        "                advantages, qvals, vfunc_r2, simplev_r2 = rl.compute_advantage(\n",
        "                    rcurr, samp_pobsfeat, sampbatch.time, self.value_func, self.discount, self.lam)\n",
        "\n",
        "            # Take a step\n",
        "            # print 'Fitting policy'\n",
        "            with util.Timer() as t_step:\n",
        "                params0_P = self.policy.get_params()\n",
        "                step_print = self.step_func(\n",
        "                    self.policy, params0_P,\n",
        "                    samp_pobsfeat.stacked, sampbatch.a.stacked, sampbatch.adist.stacked,\n",
        "                    advantages.stacked)\n",
        "                self.policy.update_obsnorm(samp_pobsfeat.stacked)\n",
        "\n",
        "            # Fit reward function\n",
        "            # print 'Fitting reward'\n",
        "            with util.Timer() as t_r_fit:\n",
        "                if True:#self.curr_iter % 20 == 0:\n",
        "                    # Subsample expert transitions to the same sample count for the policy\n",
        "                    inds = np.random.choice(self.ex_robsfeat.shape[0], size=samp_pobsfeat.stacked.shape[0])\n",
        "                    exbatch_robsfeat = self.ex_robsfeat[inds,:]\n",
        "                    exbatch_pobsfeat = self.ex_pobsfeat[inds,:] # only used for logging\n",
        "                    exbatch_a = self.ex_a[inds,:]\n",
        "                    exbatch_t = self.ex_t[inds]\n",
        "                    rfit_print = self.reward_func.fit(samp_robsfeat_stacked, sampbatch.a.stacked, sampbatch.time.stacked, exbatch_robsfeat, exbatch_a, exbatch_t)\n",
        "                else:\n",
        "                    rfit_print = []\n",
        "\n",
        "            # Fit value function for next iteration\n",
        "            # print 'Fitting value function'\n",
        "            with util.Timer() as t_vf_fit:\n",
        "                if self.value_func is not None:\n",
        "                    # Recompute q vals # XXX: this is only necessary if fitting reward after policy\n",
        "                    # qnew = qvals\n",
        "\n",
        "                    # TODO: this should be a byproduct of reward fitting\n",
        "                    rnew = RaggedArray(\n",
        "                        self.reward_func.compute_reward(samp_robsfeat_stacked, sampbatch.a.stacked, sampbatch.time.stacked),\n",
        "                        lengths=sampbatch.r.lengths)\n",
        "                    qnew, _ = rl.compute_qvals(rnew, self.discount)\n",
        "                    vfit_print = self.value_func.fit(samp_pobsfeat.stacked, sampbatch.time.stacked, qnew.stacked)\n",
        "                else:\n",
        "                    vfit_print = []\n",
        "\n",
        "        # Log\n",
        "        self.total_num_trajs += len(sampbatch)\n",
        "        self.total_num_sa += sum(len(traj) for traj in sampbatch)\n",
        "        self.total_time += t_all.dt\n",
        "        fields = [\n",
        "            ('iter', self.curr_iter, int),\n",
        "            ('trueret', sampbatch.r.padded(fill=0.).sum(axis=1).mean(), float), # average return for this batch of trajectories\n",
        "            ('iret', rcurr.padded(fill=0.).sum(axis=1).mean(), float), # average return on imitation reward\n",
        "            ('avglen', int(np.mean([len(traj) for traj in sampbatch])), int), # average traj length\n",
        "            ('ntrajs', self.total_num_trajs, int), # total number of trajs sampled over the course of training\n",
        "            ('nsa', self.total_num_sa, int), # total number of state-action pairs sampled over the course of training\n",
        "            ('ent', self.policy._compute_actiondist_entropy(sampbatch.adist.stacked).mean(), float), # entropy of action distributions\n",
        "            ('vf_r2', vfunc_r2, float),\n",
        "            ('tdvf_r2', simplev_r2, float),\n",
        "            ('dx', util.maxnorm(params0_P - self.policy.get_params()), float), # max parameter difference from last iteration\n",
        "        ] + step_print + vfit_print + rfit_print + [\n",
        "            ('avgr', rcurr_stacked.mean(), float), # average regularized reward encountered\n",
        "            ('avgunregr', orig_rcurr_stacked.mean(), float), # average unregularized reward\n",
        "            ('avgpreg', policyentbonus_B.mean(), float), # average policy regularization\n",
        "            # ('bcloss', -self.policy.compute_action_logprobs(exbatch_pobsfeat, exbatch_a).mean(), float), # negative log likelihood of expert actions\n",
        "            # ('bcloss', np.square(self.policy.compute_actiondist_mean(exbatch_pobsfeat) - exbatch_a).sum(axis=1).mean(axis=0), float),\n",
        "            ('tsamp', t_sample.dt, float), # time for sampling\n",
        "            ('tadv', t_adv.dt + t_vf_fit.dt, float), # time for advantage computation\n",
        "            ('tstep', t_step.dt, float), # time for step computation\n",
        "            ('ttotal', self.total_time, float), # total time\n",
        "        ]\n",
        "        self.curr_iter += 1\n",
        "        return fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e99aQH2uTGue"
      },
      "source": [
        "# 메인함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "c-DpwoJKlzrS"
      },
      "outputs": [],
      "source": [
        "import argparse, h5py, json\n",
        "import numpy as np\n",
        "from environments import rlgymenv\n",
        "import policyopt\n",
        "from policyopt import imitation, nn, rl, util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "kKEoEBCG_7sO"
      },
      "outputs": [],
      "source": [
        "# main\n",
        "mdp = rlgymenv.RLGymMDP(env_name)\n",
        "util.header('MDP observation space, action space sizes: %d, %d\\n' % (mdp.obs_space.dim, mdp.action_space.storage_size))\n",
        "\n",
        "# Policy architecture\n",
        "OBSNORM_MODES = ('none', 'expertdata', 'online')\n",
        "TINY_ARCHITECTURE = '[{\"type\": \"fc\", \"n\": 64}, {\"type\": \"nonlin\", \"func\": \"tanh\"}, {\"type\": \"fc\", \"n\": 64}, {\"type\": \"nonlin\", \"func\": \"tanh\"}]'\n",
        "SIMPLE_ARCHITECTURE = '[{\"type\": \"fc\", \"n\": 100}, {\"type\": \"nonlin\", \"func\": \"tanh\"}, {\"type\": \"fc\", \"n\": 100}, {\"type\": \"nonlin\", \"func\": \"tanh\"}]'\n",
        "\n",
        "obsnorm_mode='expertdata'\n",
        "sim_batch_size=1\n",
        "policy_hidden_spec=SIMPLE_ARCHITECTURE\n",
        "\n",
        "# Initialize the policy\n",
        "enable_obsnorm = obsnorm_mode != 'none'\n",
        "if isinstance(mdp.action_space, policyopt.ContinuousSpace):\n",
        "    policy_cfg = rl.GaussianPolicyConfig(\n",
        "        hidden_spec=policy_hidden_spec,\n",
        "        min_stdev=0.,\n",
        "        init_logstdev=0.,\n",
        "        enable_obsnorm=enable_obsnorm)\n",
        "    policy = rl.GaussianPolicy(policy_cfg, mdp.obs_space, mdp.action_space, 'GaussianPolicy')\n",
        "else:\n",
        "    policy_cfg = rl.GibbsPolicyConfig(\n",
        "        hidden_spec=policy_hidden_spec,\n",
        "        enable_obsnorm=enable_obsnorm)\n",
        "    policy = rl.GibbsPolicy(policy_cfg, mdp.obs_space, mdp.action_space, 'GibbsPolicy')\n",
        "\n",
        "util.header('Policy architecture')\n",
        "for v in policy.get_trainable_variables():\n",
        "    util.header('- %s (%d parameters)' % (v.name, v.get_value().size))\n",
        "util.header('Total: %d parameters' % (policy.get_num_params(),))\n",
        "\n",
        "# Load expert data\n",
        "def load_dataset(filename, limit_trajs, data_subsamp_freq):\n",
        "    # Load expert data\n",
        "    with h5py.File(filename, 'r') as f:\n",
        "        # Read data as written by vis_mj.py\n",
        "        full_dset_size = f['obs_B_T_Do'].shape[0] # full dataset size\n",
        "        dset_size = min(full_dset_size, limit_trajs) if limit_trajs is not None else full_dset_size\n",
        "\n",
        "        exobs_B_T_Do = f['obs_B_T_Do'][:dset_size,...][...]\n",
        "        exa_B_T_Da = f['a_B_T_Da'][:dset_size,...][...]\n",
        "        exr_B_T = f['r_B_T'][:dset_size,...][...]\n",
        "        exlen_B = f['len_B'][:dset_size,...][...]\n",
        "\n",
        "    print 'Expert dataset size: {} transitions ({} trajectories)'.format(exlen_B.sum(), len(exlen_B))\n",
        "    print 'Expert average return:', exr_B_T.sum(axis=1).mean()\n",
        " \n",
        "    # Stack everything together\n",
        "    start_times_B = np.random.RandomState(0).randint(0, data_subsamp_freq, size=exlen_B.shape[0])\n",
        "    print 'start times'\n",
        "    print start_times_B\n",
        "    exobs_Bstacked_Do = np.concatenate(\n",
        "        [exobs_B_T_Do[i,start_times_B[i]:l:data_subsamp_freq,:] for i, l in enumerate(exlen_B)],\n",
        "        axis=0)\n",
        "    exa_Bstacked_Da = np.concatenate(\n",
        "        [exa_B_T_Da[i,start_times_B[i]:l:data_subsamp_freq,:] for i, l in enumerate(exlen_B)],\n",
        "        axis=0)\n",
        "    ext_Bstacked = np.concatenate(\n",
        "        [np.arange(start_times_B[i], l, step=data_subsamp_freq) for i, l in enumerate(exlen_B)]).astype(float)\n",
        "\n",
        "    assert exobs_Bstacked_Do.shape[0] == exa_Bstacked_Da.shape[0] == ext_Bstacked.shape[0]# == np.ceil(exlen_B.astype(float)/data_subsamp_freq).astype(int).sum() > 0\n",
        "\n",
        "    print 'Subsampled data every {} timestep(s)'.format(data_subsamp_freq)\n",
        "    print 'Final dataset size: {} transitions (average {} per traj)'.format(exobs_Bstacked_Do.shape[0], float(exobs_Bstacked_Do.shape[0])/dset_size)\n",
        "\n",
        "    return exobs_Bstacked_Do, exa_Bstacked_Da, ext_Bstacked\n",
        "  \n",
        "exobs_Bstacked_Do, exa_Bstacked_Da, ext_Bstacked = load_dataset(\n",
        "    data, limit_trajs, data_subsamp_freq)\n",
        "\n",
        "assert exobs_Bstacked_Do.shape[1] == mdp.obs_space.storage_size\n",
        "assert exa_Bstacked_Da.shape[1] == mdp.action_space.storage_size\n",
        "assert ext_Bstacked.ndim == 1\n",
        "\n",
        "# Start optimization\n",
        "max_traj_len = max_traj_len if max_traj_len is not None else mdp.env_spec.timestep_limit\n",
        "print 'Max traj len:', max_traj_len\n",
        "\n",
        "reward = TransitionClassifier(\n",
        "    hidden_spec=policy_hidden_spec,\n",
        "    obsfeat_space=mdp.obs_space,\n",
        "    action_space=mdp.action_space,    \n",
        "    adam_lr=reward_lr,\n",
        "    adam_steps=reward_steps,\n",
        "    ent_reg_weight=reward_ent_reg_weight,\n",
        "    enable_inputnorm=True,\n",
        "    include_time=False,\n",
        "    time_scale=1./mdp.env_spec.timestep_limit,\n",
        "    favor_zero_expert_reward=bool(favor_zero_expert_reward),\n",
        "    varscope_name='TransitionClassifier')\n",
        "\n",
        "vf = rl.ValueFunc(\n",
        "    hidden_spec=policy_hidden_spec,\n",
        "    obsfeat_space=mdp.obs_space,\n",
        "    enable_obsnorm=obsnorm_mode != 'none',\n",
        "    enable_vnorm=True,\n",
        "    max_kl=vf_max_kl,\n",
        "    damping=vf_cg_damping,\n",
        "    time_scale=1./mdp.env_spec.timestep_limit,\n",
        "    varscope_name='ValueFunc')\n",
        "\n",
        "opt = ImitationOptimizer(\n",
        "    mdp=mdp,\n",
        "    discount=discount,\n",
        "    lam=lam,\n",
        "    policy=policy,\n",
        "    sim_cfg=policyopt.SimConfig(\n",
        "        min_num_trajs=-1, min_total_sa=min_total_sa,\n",
        "        batch_size=sim_batch_size, max_traj_len=max_traj_len),\n",
        "    step_func=rl.TRPO(max_kl=policy_max_kl, damping=policy_cg_damping),\n",
        "    reward_func=reward,\n",
        "    value_func=vf,\n",
        "    policy_obsfeat_fn=lambda obs: obs,\n",
        "    reward_obsfeat_fn=lambda obs: obs,\n",
        "    policy_ent_reg=policy_ent_reg,\n",
        "    ex_obs=exobs_Bstacked_Do,\n",
        "    ex_a=exa_Bstacked_Da,\n",
        "    ex_t=ext_Bstacked)\n",
        "\n",
        "# Set observation normalization\n",
        "if obsnorm_mode == 'expertdata':\n",
        "    policy.update_obsnorm(exobs_Bstacked_Do)\n",
        "    if reward is not None: reward.update_inputnorm(opt.reward_obsfeat_fn(exobs_Bstacked_Do), exa_Bstacked_Da)\n",
        "    if vf is not None: vf.update_obsnorm(opt.policy_obsfeat_fn(exobs_Bstacked_Do))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WDo3gQzbk-bI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "log = nn.TrainingLog( logfilename, [] )\n",
        "for i in xrange(max_iter):\n",
        "    iter_info = opt.step()\n",
        "    log.write(iter_info, print_header=i % (20*print_freq) == 0, display=i % print_freq == 0)\n",
        "    if save_freq != 0 and i % save_freq == 0 and log is not None:\n",
        "        log.write_snapshot(policy, i)\n",
        "\n",
        "    if plot_freq != 0 and i % plot_freq == 0:\n",
        "        exdata_N_Doa = np.concatenate([exobs_Bstacked_Do, exa_Bstacked_Da], axis=1)\n",
        "        pdata_M_Doa = np.concatenate([opt.last_sampbatch.obs.stacked, opt.last_sampbatch.a.stacked], axis=1)\n",
        "\n",
        "        # Plot reward\n",
        "        _, ax = plt.subplots()\n",
        "        idx1, idx2 = 0,1\n",
        "        range1 = (min(exdata_N_Doa[:,idx1].min(), pdata_M_Doa[:,idx1].min()), max(exdata_N_Doa[:,idx1].max(), pdata_M_Doa[:,idx1].max()))\n",
        "        range2 = (min(exdata_N_Doa[:,idx2].min(), pdata_M_Doa[:,idx2].min()), max(exdata_N_Doa[:,idx2].max(), pdata_M_Doa[:,idx2].max()))\n",
        "        reward.plot(ax, idx1, idx2, range1, range2, n=100)\n",
        "       \n",
        "        # Plot policy samples\n",
        "        ax.scatter(pdata_M_Doa[:,idx1], pdata_M_Doa[:,idx2], color='red',  alpha=0.2, s=0.5, label='apprentice')\n",
        "        # Plot expert data\n",
        "        ax.scatter(exdata_N_Doa[:,idx1], exdata_N_Doa[:,idx2], color='blue', alpha=0.2, s=0.5, label='expert')\n",
        "\n",
        "        ax.legend()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VrU3v8nzAXgM"
      },
      "source": [
        "## Evaluation : 학습된 Policy 실행시켜보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HNXwECdEEmtE"
      },
      "outputs": [],
      "source": [
        "def import_policy(policy_file, env_name, max_traj_len=None):\n",
        "    policy_file, policy_key = util.split_h5_name(policy_file)\n",
        "    print 'Loading policy parameters from %s in %s' % (policy_key, policy_file)\n",
        "    with h5py.File(policy_file, 'r') as f:        \n",
        "        dset = f[policy_key]\n",
        "        import pprint\n",
        "        pprint.pprint(dict(dset.attrs))\n",
        "\n",
        "    # Initialize the MDP    |\n",
        "    print 'Loading environment', env_name\n",
        "    mdp = rlgymenv.RLGymMDP(env_name)\n",
        "    util.header('MDP observation space, action space sizes: %d, %d\\n' % (mdp.obs_space.dim, mdp.action_space.storage_size))\n",
        "\n",
        "    if max_traj_len is None:\n",
        "        max_traj_len = mdp.env_spec.timestep_limit\n",
        "    util.header('Max traj len is {}'.format(max_traj_len))\n",
        "\n",
        "    # Initialize the policy and load its parameters\n",
        "    enable_obsnorm = True\n",
        "    if isinstance(mdp.action_space, policyopt.ContinuousSpace):\n",
        "        policy_cfg = rl.GaussianPolicyConfig(\n",
        "            hidden_spec=SIMPLE_ARCHITECTURE,\n",
        "            min_stdev=0.,\n",
        "            init_logstdev=0.,\n",
        "            enable_obsnorm=enable_obsnorm)\n",
        "        policy = rl.GaussianPolicy(policy_cfg, mdp.obs_space, mdp.action_space, 'GaussianPolicy')\n",
        "        \n",
        "    else:\n",
        "        policy_cfg = rl.GibbsPolicyConfig(\n",
        "            hidden_spec=SIMPLE_ARCHITECTURE,\n",
        "            enable_obsnorm=enable_obsnorm)\n",
        "        policy = rl.GibbsPolicy(policy_cfg, mdp.obs_space, mdp.action_space, 'GibbsPolicy')\n",
        "        \n",
        "    policy.load_h5(policy_file, policy_key)\n",
        "    \n",
        "    return policy, mdp\n",
        "\n",
        "# If you want to load from weight file use `import_policy` function:\n",
        "# key_iter=0\n",
        "# policy, mdp = import_policy(\"CartPole-v0.h5/snapshots/iter%07d\" % key_iter, 'CartPole-v0')\n",
        "# Otherwise, we'll use results of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "prFicyZN-0WT"
      },
      "outputs": [],
      "source": [
        "returns = []\n",
        "lengths = []\n",
        "images = []\n",
        "\n",
        "sim = mdp.new_sim()\n",
        "env = sim.env\n",
        "n = 10\n",
        "\n",
        "for i_traj in xrange(n):\n",
        "#     print i_traj, n\n",
        "    obs = env.reset()\n",
        "    totalr = 0.\n",
        "    l = 0\n",
        "    done = False\n",
        "    imgarr = []\n",
        "    while not done:\n",
        "        a = policy.sample_actions(obs[None], True)[0][0,0]\n",
        "        obs, r, done, _ = env.step(a)\n",
        "        img = env.render(mode='rgb_array')\n",
        "        imgarr.append(img)\n",
        "        totalr += r\n",
        "        l += 1                \n",
        "          \n",
        "    returns.append(totalr)\n",
        "    lengths.append(l)\n",
        "    images.append(imgarr)\n",
        "    print(\"-- Episode : %2d/%d | Return : %4.3f | Length : %4d \"%(i_traj+1, n, totalr, l))\n",
        "    \n",
        "print(\"**** Return : Avg=%.3f , Std=%.3f\"%(np.array(returns).mean(), np.array(returns).std()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "U2_a4m7MF0Jb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "from matplotlib import animation\n",
        "from JSAnimation import IPython_display\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def plot_movie_mp4(image_array):\n",
        "    dpi = 72.0\n",
        "    xpixels, ypixels = image_array[0].shape[0], image_array[0].shape[1]\n",
        "    fig = plt.figure(figsize=(ypixels/dpi, xpixels/dpi), dpi=dpi)\n",
        "    im = plt.figimage(image_array[0])\n",
        "\n",
        "    def animate(i):\n",
        "        im.set_array(image_array[i])\n",
        "        return (im,)\n",
        "\n",
        "    anim = animation.FuncAnimation(fig, animate, frames=len(image_array))\n",
        "    display(HTML(anim.to_html5_video()))\n",
        "\n",
        "def plot_movie_js(image_array):\n",
        "    dpi = 10.0\n",
        "    xpixels, ypixels = image_array[0].shape[0], image_array[0].shape[1]\n",
        "    fig = plt.figure(figsize=(ypixels/(dpi), xpixels/(dpi)), dpi=dpi)\n",
        "    im = plt.figimage(image_array[0])\n",
        "\n",
        "    def animate(i):\n",
        "        im.set_array(image_array[i])\n",
        "        return (im,)\n",
        "    \n",
        "    anim = animation.FuncAnimation(fig, animate, frames=len(image_array))\n",
        "    display(IPython_display.display_animation(anim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yAXjUlROcOKF"
      },
      "outputs": [],
      "source": [
        "% matplotlib inline\n",
        "plot_movie_js(images[6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4utw5lMidpbG"
      },
      "outputs": [],
      "source": [
        "% matplotlib inline\n",
        "plot_movie_mp4(images[6])"
      ]
    }
  ]
}