{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day3_GAIL_HW.ipynb","version":"0.3.2","provenance":[{"file_id":"1MdmwlWo9Tb8C5lNBke-d00YpvXrRM3Vf","timestamp":1547642852793},{"file_id":"1l6w6LMG8zQ2qydc-gXNjr1DBHY5_9ZRv","timestamp":1547642478113}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4Vtb_VJITO3T"},"source":["# GAIL (Generative Adversarial Imitation Learning) 실습\n","\n","이번 실습자료는 GAIL 저자 (Jonathan Ho)의  [구현코드](/https://github.com/openai/imitation.git)를 바탕으로 제작되었습니다. \n","\n","직접적인 구현 자체 (`TrainsitionClassifier`, `ImitationOptimizer`) 위주로 실습자료를 만들었고, 구현에 필요한 유틸리티 메서드들은 저자코드 저장소에서 그대로 받아와서 사용하도록 제작했습니다."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LK4uPUhTx9dn"},"source":["## 0. 환경설정"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"uJZ_DqjS2Z2d"},"outputs":[],"source":["!rm -rf /usr/local/lib/python2.7/dist-packages/gym; rm -rf /usr/local/lib/python2.7/dist-packages/pyglet\n","!pip install virtualenv\n","!virtualenv gailenv\n","!apt-get install -y xvfb python-opengl swig ffmpeg zlib1g-dev python3-tk\n","!source /content/gailenv/bin/activate; pip install numpy==1.10.4 theano==0.8.2 gym==0.1.0 mujoco_py==0.4.0 JSAnimation PyOpenGL piglet pyglet==1.3.2 xlib pyvirtualdisplay box2d-py mako==1.0.7 "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.append('/content/gailenv/lib/python2.7/site-packages')\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()\n","import os\n","os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"pxv1IalyQuPH"},"outputs":[],"source":["!git init .\n","!git remote add origin https://github.com/tzs930/imitation.git\n","!git pull origin master"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sEEyhSrqTB3I"},"source":["## 1. Hyperparameter 설정\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LT80O2WW55ip"},"source":["### 데이터 입력 관련\n","\n","- `data` : 전문가 행동데이터(Expert trajectory)들이 저장된 `.h5` 파일의 경로를 지정합니다.\n","- `limit_trajs` : 최대 몇개의 Expert episode를 가져올지 지정합니다. 작아질수록 학습에 들어가는 데이터의 양이 적어집니다.\n","- `data_subsamp_freq` : 한 Episode 내에서 Subsampling을 얼마나 할지 지정합니다. 1일 경우 Subsampling을 하지 않고 모든 데이터를 사용합니다.\n","\n","### Gym 환경 관련\n","- `env_name` : 어떤 Gym Environment에서 실험할 것인지 지정합니다. 반드시 `gym`에 등록(register)된 환경이어야 합니다. \n","> e.g. `CartPole-v0`, `Acrobot-v0`, `MountainCar-v0`, ...\n","- `max_traj_len` :  Environment의 최대 에피소드 길이 값을 지정합니다."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"oydeE44w3FDH"},"outputs":[],"source":["# Expert dataset\n","data='expert_trajs/trajs_cartpole.h5'\n","limit_trajs=25\n","data_subsamp_freq=10\n","\n","# MDP options\n","env_name='CartPole-v0'\n","max_traj_len=1000"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CCttOHW-tXHh"},"source":["### 학습 진행 관련 \n","- `min_total_sa` : 1번의 Policy Optimization에서 최대 몇개의 (s, a) 쌍을 샘플할 것인지 지정합니다.\n","- `max_iter` :  최대 몇 번의 Iteration을 학습할 것인지 지정합니다. \n","\n","한 번의 Iteration은 outer-loop에서 *Reward 학습*, inner-loop에서 *Policy 학습* 의 2단계로 이루어지며,  Inner loop에서 샘플된 (s,a)을 이용하여 TRPO를 학습하므로 사실상  `min_total_sa * max_iter` 만큼의 학습이 진행되게 됩니다.\n","\n","\n","### 저장 및 출력 관련 \n","\n","- `print_freq` : 학습 도중 결과를 얼마나 자주 출력할지 지정합니다.\n","- `save_freq` :  학습 도중 결과 및 모델을 얼마나 자주 저장할지 지정합니다.\n","- `plot_freq` : 학습 도중 그래프를 얼마나 자주 출력할지 지정합니다.\n","- `logfilename` : 저장될 로그 및 학습결과의 경로를 지정합니다."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"L3w6OVgFtVoX"},"outputs":[],"source":["# Learning hyperparameters\n","min_total_sa=50000\n","max_iter=100\n","\n","# Saving hyperparameters\n","print_freq=1\n","save_freq=10\n","plot_freq=10\n","logfilename='./' + env_name + '.h5'"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sVIvujnv5uQq"},"source":["### Reward 학습 관련 \n","- `reward_steps` : 한 번의 Reward optimization마다 몇 step의 update를 할 것인지 지정합니다.\n","- `reward_lr` :  Reward의 Learning Rate를 지정합니다.\n","- `reward_ent_reg_weight` :  Reward Loss에 적용되는 Entropy Regularization 정도를 지정합니다.\n","- `favor_zero_expert_reward` : Discriminator가 Expert trajectory를 1 또는 0으로 판단할 지 지정합니다. 가능한 한 빨리 도달해야하는 태스크들(ex. `MountainCar-v0`) 의 경우 이 옵션(`favor_zero_expert_reward=1`)이 필요합니다.\n","\n","\n","### TRPO (Trust-Region Policy Optimization) 관련 \n","\n","GAIL은 inner-loop에서 RL Optimization을 이용하여 Policy를 학습시키는데, TRPO [(Schulman. J. et al., 2015)](https://arxiv.org/abs/1502.05477)를 사용하여 학습합니다. \n","\n","이번 실습에서는 TRPO가 중점이 아니므로 이 부분은 생략합니다.\n","\n"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"0-eRYwWt5uy5"},"outputs":[],"source":["# Reward hyperparameters\n","reward_steps=1\n","reward_lr=.01\n","reward_ent_reg_weight=.001\n","favor_zero_expert_reward=0\n","\n","# TRPO hyperparameters\n","discount=.995\n","lam=.97\n","policy_max_kl=.01\n","policy_cg_damping=.1\n","vf_max_kl=.01\n","vf_cg_damping=.1\n","policy_ent_reg=0."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QbVe_S6WU_Gy"},"source":["## 2. Generative Adversarial Learning을 통해 Reward를 학습 : `TrainsitionClassifier`"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TWDGpYQV69YB"},"source":["GAIL에서는 Discriminator를 local reward 혹은 cost function으로 삼아서 학습을 하게 됩니다. 논문에 따르면, Discriminator의 weight를 $w$, Discriminator network를 $D_w : \\mathcal S \\times \\mathcal A  \\to [0,1]$ 으로 정의하면, GAIL의 (local imaginary) reward function는 다음과 같습니다. \n","\n","&nbsp;\n","\n","\\begin{equation}\n","r(s,a) = \\log D_w (s,a) \n","\\end{equation}\n","&nbsp;\n","\n","다만 위 Reward function의 경우, 항상 0보다 작거나 같은 reward를 갖게 되므로, 다음과 같은 트릭을 사용하여 non-negative reward를 얻을 수 있습니다.\n","\n","&nbsp;\n","\n","### Positive (Non-Negative) Reward Function\n","\n","\\begin{equation}\n","r(s,a) = - \\log(1- D_w (s,a) + \\epsilon)\n","\\end{equation}\n","&nbsp;\n","\n","이 Reward function의 경우 $D_w$가 0이 되면 reward가 0, 1에 가까워질수록 점점 커지는 형태의 reward를 갖게 됩니다. 또한 $\\infty$로 발산할 경우를 대비하여 $\\epsilon$=1e-8 정도의 값을 $\\log$ 안에 추가해주기도 합니다.\n","\n","&nbsp;\n","\n","### Negative (Non-Positive) Reward Function\n","\n","앞서 말한 Reward function의 경우 non-negative reward를 학습하게 되는데, 태스크에 따라서 negative reward를 필요로 하게 되는 경우가 있습니다. 그 경우, 다음과 같이 reward function을 디자인할 수 있습니다.\n","\n","\\begin{equation}\n","r(s,a) = \\log( D_w (s,a) + \\epsilon )\n","\\end{equation}\n","&nbsp;\n","\n","이 Reward function의 경우 $D_w$가 0이 되면 reward가 $-\\infty$, 1에 가까워질수록 0에 가까워주는 reward를 갖게 됩니다. 마찬가지로 $\\infty$로 발산할 경우를 대비하여 $\\epsilon$=1e-8 정도의 값을 $\\log$ 안에 추가할 수 있습니다.\n","\n","&nbsp;\n","\n","### Learning a Discriminator \n","\n","논문에 따르면, Reward 학습을 위해 다음과 같은 Gradient를 이용해 Discriminator를 학습할 수 있습니다.\n","\n","&nbsp;\n","\n","$$\\\n","\\mathbb{E}_{\\tau_i} [ \\nabla_w log(D_w (s,a))] + \\mathbb E_{\\tau_E} [\\nabla_w \\log(1-D_w(s,a))] \\tag{17}\n","$$\n","\n","&nbsp;\n","\n","이 코드에서는 위 Gradient를 이용해 학습하는 것과 같은 효과를 주기 위해, Expert trajectory를 0, Sampled trajectory를 1으로 labeling 한 후, Cross-Entropy Loss로 Regression을 하는 방법으로 Discriminator를 학습하고 있습니다.\n","\n","&nbsp;\n","\n"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"xnMTEwCH9j4L"},"outputs":[],"source":["# from pyvirtualdisplay import Display\n","# display = Display(visible=0, size=(1400, 900))\n","# display.start()\n","# import os\n","# os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\n","\n","from policyopt import nn, rl, util, RaggedArray, ContinuousSpace, FiniteSpace, optim, thutil\n","import numpy as np\n","from contextlib import contextmanager\n","import theano; from theano import tensor\n","\n","class TransitionClassifier(nn.Model):\n","    '''Reward/adversary for generative-adversarial training'''\n","\n","    def __init__(self, obsfeat_space, action_space, hidden_spec, adam_lr, adam_steps, ent_reg_weight,\n","                 enable_inputnorm, time_scale, favor_zero_expert_reward, varscope_name):\n","        self.obsfeat_space, self.action_space = obsfeat_space, action_space\n","        self.hidden_spec = hidden_spec        \n","        self.adam_steps = adam_steps\n","        self.ent_reg_weight = ent_reg_weight; assert ent_reg_weight >= 0        \n","        self.time_scale = time_scale\n","        self.favor_zero_expert_reward = favor_zero_expert_reward\n","\n","        with nn.variable_scope(varscope_name) as self.__varscope:\n","            # Map (s,a) pairs to classifier scores (log probabilities of classes)\n","            obsfeat_B_Df = tensor.matrix(name='obsfeat_B_Df')\n","            a_B_Da = tensor.matrix(name='a_B_Da', dtype=theano.config.floatX if self.action_space.storage_type == float else 'int64')\n","            t_B = tensor.vector(name='t_B')\n","\n","            scaled_t_B = self.time_scale * t_B\n","\n","            if isinstance(self.action_space, ContinuousSpace):\n","                # For a continuous action space, map observation-action pairs to a real number (reward)\n","                trans_B_Doa = tensor.concatenate([obsfeat_B_Df, a_B_Da], axis=1)\n","                trans_dim = self.obsfeat_space.dim + self.action_space.dim\n","                \n","                # Normalize\n","                with nn.variable_scope('inputnorm'):\n","                    self.inputnorm = (nn.Standardizer if enable_inputnorm else nn.NoOpStandardizer)(self.obsfeat_space.dim + self.action_space.dim)\n","                normedtrans_B_Doa = self.inputnorm.standardize_expr(trans_B_Doa)\n","                \n","                net_input = normedtrans_B_Doa\n","                net_input_dim = trans_dim\n","                \n","                # Compute scores\n","                with nn.variable_scope('hidden'):\n","                    net = nn.FeedforwardNet(net_input, (net_input_dim,), self.hidden_spec)\n","                with nn.variable_scope('out'):\n","                    out_layer = nn.AffineLayer(net.output, net.output_shape, (1,), initializer=np.zeros((net.output_shape[0], 1)))\n","                scores_B = out_layer.output[:,0]\n","\n","            else:\n","                # For a finite action space, map observation observations to a vector of rewards\n","\n","                # Normalize observations\n","                with nn.variable_scope('inputnorm'):\n","                    self.inputnorm = (nn.Standardizer if enable_inputnorm else nn.NoOpStandardizer)(self.obsfeat_space.dim)\n","                normedobs_B_Df = self.inputnorm.standardize_expr(obsfeat_B_Df)\n","                \n","                net_input = normedobs_B_Df\n","                net_input_dim = self.obsfeat_space.dim\n","                \n","                # Compute scores\n","                with nn.variable_scope('hidden'):\n","                    net = nn.FeedforwardNet(net_input, (net_input_dim,), self.hidden_spec)\n","                    \n","                with nn.variable_scope('out'):\n","                    out_layer = nn.AffineLayer(\n","                        net.output, net.output_shape, (self.action_space.size,),\n","                        initializer=np.zeros((net.output_shape[0], self.action_space.size)))\n","                scores_B = out_layer.output[tensor.arange(normedobs_B_Df.shape[0]), a_B_Da[:,0]]\n","        \n","        compute_scores_without_time = thutil.function([obsfeat_B_Df, a_B_Da], scores_B)\n","        self._compute_scores = lambda _obsfeat_B_Df, _a_B_Da, _t_B: compute_scores_without_time(_obsfeat_B_Df, _a_B_Da)\n","\n","        # For preventing Numerical Error\n","        eps = 1e-8\n","        \n","        if self.favor_zero_expert_reward:\n","            # 0 for expert-like states, goes to -inf for non-expert-like states\n","            # compatible with envs with traj cutoffs for good (expert-like) behavior\n","            # e.g. mountain car, which gets cut off when the car reaches the destination\n","            \n","            ################# Implement Here #################\n","            \n","            raise NotImplementedError\n","            \n","            ##################################################\n","            \n","        else:\n","            # 0 for non-expert-like states, goes to +inf for expert-like states\n","            # compatible with envs with traj cutoffs for bad (non-expert-like) behavior\n","            # e.g. walking simulations that get cut off when the robot falls over\n","            rewards_B = -tensor.log(1. - tensor.nnet.sigmoid(scores_B) + eps)\n","            \n","        \n","        compute_reward_func = thutil.function([obsfeat_B_Df, a_B_Da], rewards_B)\n","        self._compute_reward = lambda _obsfeat_B_Df, _a_B_Da, _t_B: compute_reward_func(_obsfeat_B_Df, _a_B_Da)\n","\n","        param_vars = self.get_trainable_variables()\n","\n","        # Logistic regression loss, regularized by negative entropy\n","        labels_B = tensor.vector(name='labels_B')\n","        weights_B = tensor.vector(name='weights_B')\n","        losses_B = thutil.sigmoid_cross_entropy_with_logits(scores_B, labels_B)\n","        ent_B = thutil.logit_bernoulli_entropy(scores_B)\n","        loss = ((losses_B - self.ent_reg_weight*ent_B) * weights_B).sum(axis=0)\n","        lossgrad_P = thutil.flatgrad(loss, param_vars)\n","\n","        adamstep_func = thutil.function(\n","            [obsfeat_B_Df, a_B_Da, labels_B, weights_B], loss,\n","            updates=thutil.adam(loss, param_vars, lr=adam_lr))\n","        \n","        self._adamstep = lambda _obsfeat_B_Df, _a_B_Da, _t_B, _labels_B, _weights_B: adamstep_func(_obsfeat_B_Df, _a_B_Da, _labels_B, _weights_B)\n","\n","    @property\n","    def varscope(self): return self.__varscope\n","\n","    def compute_reward(self, obsfeat_B_Df, a_B_Da, t_B):\n","        return self._compute_reward(obsfeat_B_Df, a_B_Da, t_B)\n","\n","    def fit(self, obsfeat_B_Df, a_B_Da, t_B, exobs_Bex_Do, exa_Bex_Da, ext_Bex):\n","        # Transitions from the current policy go first, then transitions from the expert\n","        obsfeat_Ball_Df = np.concatenate([obsfeat_B_Df, exobs_Bex_Do])\n","        a_Ball_Da = np.concatenate([a_B_Da, exa_Bex_Da])\n","        t_Ball = np.concatenate([t_B, ext_Bex])\n","\n","        # Update normalization\n","        self.update_inputnorm(obsfeat_Ball_Df, a_Ball_Da)\n","\n","        B = obsfeat_B_Df.shape[0] # number of examples from the current policy\n","        Ball = obsfeat_Ball_Df.shape[0] # Ball - b = num examples from expert\n","\n","        # Label expert as 1, current policy as 0\n","        labels_Ball = np.zeros(Ball)\n","        labels_Ball[B:] = 1.\n","\n","        # Evenly weight the loss terms for the expert and the current policy\n","        weights_Ball = np.zeros(Ball)\n","        weights_Ball[:B] = 1./B\n","        weights_Ball[B:] = 1./(Ball - B); assert len(weights_Ball[B:]) == Ball-B\n","\n","        # Optimize\n","        for _ in range(self.adam_steps):\n","            loss, kl, num_bt_steps = self._adamstep(obsfeat_Ball_Df, a_Ball_Da, t_Ball, labels_Ball, weights_Ball), None, 0\n","\n","        # Evaluate\n","        scores_Ball = self._compute_scores(obsfeat_Ball_Df, a_Ball_Da, t_Ball); assert scores_Ball.shape == (Ball,)\n","        accuracy = .5 * (weights_Ball * ((scores_Ball < 0) == (labels_Ball == 0))).sum()\n","        accuracy_for_currpolicy = (scores_Ball[:B] <= 0).mean()\n","        accuracy_for_expert = (scores_Ball[B:] > 0).mean()\n","        assert np.allclose(accuracy, .5*(accuracy_for_currpolicy + accuracy_for_expert))\n","\n","        return [\n","            ('rloss', loss, float), # reward function fitting loss\n","            ('racc', accuracy, float), # reward function accuracy\n","            ('raccpi', accuracy_for_currpolicy, float), # reward function accuracy\n","            ('raccex', accuracy_for_expert, float), # reward function accuracy\n","            ('rkl', kl, float),\n","            ('rbt', num_bt_steps, int),\n","            # ('rpnorm', util.maxnorm(self.get_params()), float),\n","            # ('snorm', util.maxnorm(scores_Ball), float),\n","        ]\n","\n","    def update_inputnorm(self, obs_B_Do, a_B_Da):\n","        if isinstance(self.action_space, ContinuousSpace):\n","            self.inputnorm.update(np.concatenate([obs_B_Do, a_B_Da], axis=1))\n","        else:\n","            self.inputnorm.update(obs_B_Do)\n","\n","    def plot(self, ax, idx1, idx2, range1, range2, n=100):\n","        assert len(range1) == len(range2) == 2 and idx1 != idx2\n","        x, y = np.mgrid[range1[0]:range1[1]:(n+0j), range2[0]:range2[1]:(n+0j)]\n","\n","        if isinstance(self.action_space, ContinuousSpace):\n","            points_B_Doa = np.zeros((n*n, self.obsfeat_space.storage_size + self.action_space.storage_size))\n","            points_B_Doa[:,idx1] = x.ravel()\n","            points_B_Doa[:,idx2] = y.ravel()\n","            obsfeat_B_Df, a_B_Da = points_B_Doa[:,:self.obsfeat_space.storage_size], points_B_Doa[:,self.obsfeat_space.storage_size:]\n","            assert a_B_Da.shape[1] == self.action_space.storage_size\n","            t_B = np.zeros(a_B_Da.shape[0]) # XXX make customizable\n","            z = self.compute_reward(obsfeat_B_Df, a_B_Da, t_B).reshape(x.shape)\n","            \n","        else:\n","            obsfeat_B_Df = np.zeros((n*n, self.obsfeat_space.storage_size))\n","            obsfeat_B_Df[:,idx1] = x.ravel()\n","            obsfeat_B_Df[:,idx2] = y.ravel()\n","            a_B_Da = np.zeros((obsfeat_B_Df.shape[0], 1), dtype=np.int32) # XXX make customizable\n","            t_B = np.zeros(a_B_Da.shape[0]) # XXX make customizable\n","            z = self.compute_reward(obsfeat_B_Df, a_B_Da, t_B).reshape(x.shape)\n","\n","        ax.pcolormesh(x, y, z, cmap='viridis')\n","        # ax.contour(x, y, z, levels=np.log(np.linspace(2., 3., 10)))\n","        ax.contourf(x, y, z, levels=[np.log(2.), np.log(2.)+.5], alpha=.5) # high-reward region is highlighted\n","        "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Yi7djwfqVWXF"},"source":["## 3. 학습한 Reward로 Policy Optimization :  `ImitationOptimizer`"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5tMesV3FBai3"},"source":["GAIL의 학습은 총 5단계로 이루어집니다.\n","\n","1. 현재 Policy를 이용해 Trajectory를 Sampling 합니다.\n","2. 현재 Reward를 이용해 Baseline 혹은 Advantage function을 계산합니다.\n","3. 현재 Reward와 Sample된 Trajectory를 이용해 TRPO로 Policy를 학습합니다.\n","4. 주어진 Expert trajectory와 Sampled trajectory를 이용해 Reward를 학습합니다.\n","5. 학습된 Reward를 이용해 Value function (Q-function)을 학습합니다."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"3Cl38j-S-fge"},"outputs":[],"source":["import pickle\n","class ImitationOptimizer(object):\n","    def __init__(self, mdp, discount, lam, policy, sim_cfg, step_func, reward_func, value_func,\n","                 policy_obsfeat_fn, reward_obsfeat_fn, policy_ent_reg, ex_obs, ex_a, ex_t):\n","        self.mdp, self.discount, self.lam, self.policy = mdp, discount, lam, policy\n","        self.sim_cfg = sim_cfg\n","        self.step_func = step_func\n","        self.reward_func = reward_func\n","        self.value_func = value_func\n","        # assert value_func is not None, 'not tested'\n","        self.policy_obsfeat_fn = policy_obsfeat_fn\n","        self.reward_obsfeat_fn = reward_obsfeat_fn\n","        self.policy_ent_reg = policy_ent_reg\n","        util.header('Policy entropy regularization: {}'.format(self.policy_ent_reg))\n","\n","        assert ex_obs.ndim == ex_a.ndim == 2 and ex_t.ndim == 1 and ex_obs.shape[0] == ex_a.shape[0] == ex_t.shape[0]\n","        self.ex_pobsfeat, self.ex_robsfeat, self.ex_a, self.ex_t = policy_obsfeat_fn(ex_obs), reward_obsfeat_fn(ex_obs), ex_a, ex_t\n","\n","        self.total_num_trajs = 0\n","        self.total_num_sa = 0\n","        self.total_time = 0.\n","        self.curr_iter = 0\n","        self.last_sampbatch = None # for outside access for debugging\n","\n","    def step(self):\n","        with util.Timer() as t_all:\n","\n","            # 1. Sample trajectories using current policy\n","            # print 'Sampling'\n","            with util.Timer() as t_sample:\n","                sampbatch = self.mdp.sim_mp(\n","                    policy_fn=lambda obsfeat_B_Df: self.policy.sample_actions(obsfeat_B_Df),\n","                    obsfeat_fn=self.policy_obsfeat_fn,\n","                    cfg=self.sim_cfg)\n","                samp_pobsfeat = sampbatch.obsfeat\n","                self.last_sampbatch = sampbatch\n","\n","            # 2. Compute baseline / advantages\n","            # print 'Computing advantages'\n","            with util.Timer() as t_adv:\n","                # Compute observation features for reward input\n","                samp_robsfeat_stacked = self.reward_obsfeat_fn(sampbatch.obs.stacked)\n","                # Reward is computed wrt current reward function\n","                # TODO: normalize rewards\n","                rcurr_stacked = self.reward_func.compute_reward(samp_robsfeat_stacked, sampbatch.a.stacked, sampbatch.time.stacked)\n","                assert rcurr_stacked.shape == (samp_robsfeat_stacked.shape[0],)\n","\n","                # If we're regularizing the policy, add negative log probabilities to the rewards\n","                # Intuitively, the policy gets a bonus for being less certain of its actions\n","                orig_rcurr_stacked = rcurr_stacked.copy()\n","                if self.policy_ent_reg is not None and self.policy_ent_reg != 0:\n","                    assert self.policy_ent_reg > 0\n","                    # XXX probably faster to compute this from sampbatch.adist instead\n","                    actionlogprobs_B = self.policy.compute_action_logprobs(samp_pobsfeat.stacked, sampbatch.a.stacked)\n","                    policyentbonus_B = -self.policy_ent_reg * actionlogprobs_B\n","                    rcurr_stacked += policyentbonus_B\n","                else:\n","                    policyentbonus_B = np.zeros_like(rcurr_stacked)\n","\n","                rcurr = RaggedArray(rcurr_stacked, lengths=sampbatch.r.lengths)\n","\n","                # Compute advantages using these rewards\n","                advantages, qvals, vfunc_r2, simplev_r2 = rl.compute_advantage(\n","                    rcurr, samp_pobsfeat, sampbatch.time, self.value_func, self.discount, self.lam)\n","\n","            # 3. Learn a policy\n","            # print 'Fitting policy'\n","            with util.Timer() as t_step:\n","                params0_P = self.policy.get_params()\n","                step_print = self.step_func(\n","                    self.policy, params0_P,\n","                    samp_pobsfeat.stacked, sampbatch.a.stacked, sampbatch.adist.stacked,\n","                    advantages.stacked)\n","                self.policy.update_obsnorm(samp_pobsfeat.stacked)\n","\n","            # 4. Fit reward function\n","            # print 'Fitting reward'\n","            with util.Timer() as t_r_fit:\n","                if True:#self.curr_iter % 20 == 0:\n","                    # Subsample expert transitions to the same sample count for the policy\n","                    inds = np.random.choice(self.ex_robsfeat.shape[0], size=samp_pobsfeat.stacked.shape[0])\n","                    exbatch_robsfeat = self.ex_robsfeat[inds,:]\n","                    exbatch_pobsfeat = self.ex_pobsfeat[inds,:] # only used for logging\n","                    exbatch_a = self.ex_a[inds,:]\n","                    exbatch_t = self.ex_t[inds]\n","                    rfit_print = self.reward_func.fit(samp_robsfeat_stacked, sampbatch.a.stacked, sampbatch.time.stacked, exbatch_robsfeat, exbatch_a, exbatch_t)\n","                else:\n","                    rfit_print = []\n","\n","            # 5. Fit value function for next iteration\n","            # print 'Fitting value function'\n","            with util.Timer() as t_vf_fit:\n","                if self.value_func is not None:\n","                    # Recompute q vals # XXX: this is only necessary if fitting reward after policy\n","                    # qnew = qvals\n","\n","                    # TODO: this should be a byproduct of reward fitting\n","                    rnew = RaggedArray(\n","                        self.reward_func.compute_reward(samp_robsfeat_stacked, sampbatch.a.stacked, sampbatch.time.stacked),\n","                        lengths=sampbatch.r.lengths)\n","                    qnew, _ = rl.compute_qvals(rnew, self.discount)\n","                    vfit_print = self.value_func.fit(samp_pobsfeat.stacked, sampbatch.time.stacked, qnew.stacked)\n","                else:\n","                    vfit_print = []\n","\n","        # Log\n","        self.total_num_trajs += len(sampbatch)\n","        self.total_num_sa += sum(len(traj) for traj in sampbatch)\n","        self.total_time += t_all.dt\n","        fields = [\n","            ('iter', self.curr_iter, int),\n","            ('trueret', sampbatch.r.padded(fill=0.).sum(axis=1).mean(), float), # average return for this batch of trajectories\n","            ('iret', rcurr.padded(fill=0.).sum(axis=1).mean(), float), # average return on imitation reward\n","            ('avglen', int(np.mean([len(traj) for traj in sampbatch])), int), # average traj length\n","            ('ntrajs', self.total_num_trajs, int), # total number of trajs sampled over the course of training\n","            ('nsa', self.total_num_sa, int), # total number of state-action pairs sampled over the course of training\n","            ('ent', self.policy._compute_actiondist_entropy(sampbatch.adist.stacked).mean(), float), # entropy of action distributions\n","            ('vf_r2', vfunc_r2, float),\n","            ('tdvf_r2', simplev_r2, float),\n","            ('dx', util.maxnorm(params0_P - self.policy.get_params()), float), # max parameter difference from last iteration\n","        ] + step_print + vfit_print + rfit_print + [\n","            ('avgr', rcurr_stacked.mean(), float), # average regularized reward encountered\n","            ('avgunregr', orig_rcurr_stacked.mean(), float), # average unregularized reward\n","            ('avgpreg', policyentbonus_B.mean(), float), # average policy regularization\n","            # ('bcloss', -self.policy.compute_action_logprobs(exbatch_pobsfeat, exbatch_a).mean(), float), # negative log likelihood of expert actions\n","            # ('bcloss', np.square(self.policy.compute_actiondist_mean(exbatch_pobsfeat) - exbatch_a).sum(axis=1).mean(axis=0), float),\n","            ('tsamp', t_sample.dt, float), # time for sampling\n","            ('tadv', t_adv.dt + t_vf_fit.dt, float), # time for advantage computation\n","            ('tstep', t_step.dt, float), # time for step computation\n","            ('ttotal', self.total_time, float), # total time\n","        ]\n","        self.curr_iter += 1\n","        return fields"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"e99aQH2uTGue"},"source":["## 4. 메인함수 : GAIL Training 해보기"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"c-DpwoJKlzrS"},"outputs":[],"source":["import argparse, h5py, json\n","import numpy as np\n","from environments import rlgymenv\n","import policyopt\n","from policyopt import imitation, nn, rl, util\n","\n","# Policy architecture\n","OBSNORM_MODES = ('none', 'expertdata', 'online')\n","TINY_ARCHITECTURE = '[{\"type\": \"fc\", \"n\": 64}, {\"type\": \"nonlin\", \"func\": \"tanh\"}, {\"type\": \"fc\", \"n\": 64}, {\"type\": \"nonlin\", \"func\": \"tanh\"}]'\n","SIMPLE_ARCHITECTURE = '[{\"type\": \"fc\", \"n\": 100}, {\"type\": \"nonlin\", \"func\": \"tanh\"}, {\"type\": \"fc\", \"n\": 100}, {\"type\": \"nonlin\", \"func\": \"tanh\"}]'\n","\n","obsnorm_mode='expertdata'\n","sim_batch_size=1\n","policy_hidden_spec=SIMPLE_ARCHITECTURE"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"H3-yZr9mxn4U"},"source":["### #1 : Environment & Policy 셋업\n","- Gym Environmnet 세팅\n","- Policy Architecture 구성\n","- Dataset Load"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":706},"colab_type":"code","executionInfo":{"elapsed":79496,"status":"ok","timestamp":1547714133459,"user":{"displayName":"Hyeongjoo Hwang","photoUrl":"","userId":"00454735331078182073"},"user_tz":-540},"id":"kKEoEBCG_7sO","outputId":"ecd14266-c156-40c6-9c21-f78c4819b199"},"outputs":[{"name":"stdout","output_type":"stream","text":["Gym version: 0.1.0\n","\u001b[95mMDP observation space, action space sizes: 4, 1\n","\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python2.7/dist-packages/gym/envs/registration.py:13: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n","  result = entry_point.load(False)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[95mLoading feedforward net specification\u001b[0m\n","[\n","  {\n","    \"type\": \"fc\",\n","    \"n\": 100\n","  },\n","  {\n","    \"type\": \"nonlin\",\n","    \"func\": \"tanh\"\n","  },\n","  {\n","    \"type\": \"fc\",\n","    \"n\": 100\n","  },\n","  {\n","    \"type\": \"nonlin\",\n","    \"func\": \"tanh\"\n","  }\n","]\n","\u001b[95mAffine(in=4, out=100)\u001b[0m\n","\u001b[95mNonlinearity(func=tanh)\u001b[0m\n","\u001b[95mAffine(in=100, out=100)\u001b[0m\n","\u001b[95mNonlinearity(func=tanh)\u001b[0m\n","\u001b[95mAffine(in=100, out=2)\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[95mPolicy architecture\u001b[0m\n","\u001b[95m- /GibbsPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W (400 parameters)\u001b[0m\n","\u001b[95m- /GibbsPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b (100 parameters)\u001b[0m\n","\u001b[95m- /GibbsPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W (10000 parameters)\u001b[0m\n","\u001b[95m- /GibbsPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b (100 parameters)\u001b[0m\n","\u001b[95m- /GibbsPolicy/out/AffineLayer/W (200 parameters)\u001b[0m\n","\u001b[95m- /GibbsPolicy/out/AffineLayer/b (2 parameters)\u001b[0m\n","\u001b[95mTotal: 10802 parameters\u001b[0m\n"]}],"source":["# main\n","mdp = rlgymenv.RLGymMDP(env_name)\n","util.header('MDP observation space, action space sizes: %d, %d\\n' % (mdp.obs_space.dim, mdp.action_space.storage_size))\n","\n","# Policy architecture\n","OBSNORM_MODES = ('none', 'expertdata', 'online')\n","TINY_ARCHITECTURE = '[{\"type\": \"fc\", \"n\": 64}, {\"type\": \"nonlin\", \"func\": \"tanh\"}, {\"type\": \"fc\", \"n\": 64}, {\"type\": \"nonlin\", \"func\": \"tanh\"}]'\n","SIMPLE_ARCHITECTURE = '[{\"type\": \"fc\", \"n\": 100}, {\"type\": \"nonlin\", \"func\": \"tanh\"}, {\"type\": \"fc\", \"n\": 100}, {\"type\": \"nonlin\", \"func\": \"tanh\"}]'\n","\n","obsnorm_mode='expertdata'\n","sim_batch_size=1\n","policy_hidden_spec=SIMPLE_ARCHITECTURE\n","\n","# Initialize the policy\n","enable_obsnorm = obsnorm_mode != 'none'\n","if isinstance(mdp.action_space, policyopt.ContinuousSpace):\n","    policy_cfg = rl.GaussianPolicyConfig(\n","        hidden_spec=policy_hidden_spec,\n","        min_stdev=0.,\n","        init_logstdev=0.,\n","        enable_obsnorm=enable_obsnorm)\n","    policy = rl.GaussianPolicy(policy_cfg, mdp.obs_space, mdp.action_space, 'GaussianPolicy')\n","else:\n","    policy_cfg = rl.GibbsPolicyConfig(\n","        hidden_spec=policy_hidden_spec,\n","        enable_obsnorm=enable_obsnorm)\n","    policy = rl.GibbsPolicy(policy_cfg, mdp.obs_space, mdp.action_space, 'GibbsPolicy')\n","\n","util.header('Policy architecture')\n","for v in policy.get_trainable_variables():\n","    util.header('- %s (%d parameters)' % (v.name, v.get_value().size))\n","util.header('Total: %d parameters' % (policy.get_num_params(),))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OE-jqBJ_ykTn"},"source":["### #2 : Expert Trajectory 불러오기"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"colab_type":"code","executionInfo":{"elapsed":604,"status":"ok","timestamp":1547714141282,"user":{"displayName":"Hyeongjoo Hwang","photoUrl":"","userId":"00454735331078182073"},"user_tz":-540},"id":"zz9urG9XyjlM","outputId":"d7de6279-d411-4ab6-e20d-26242432e8ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Expert dataset size: 3400 transitions (17 trajectories)\n","Expert average return: 200.0\n","start times\n","[4 0 3 3 3 1 3 2 4 0 0 4 2 1 0 1 1]\n","Subsampled data every 5 timestep(s)\n","Final dataset size: 680 transitions (average 40.0 per traj)\n","Max traj len: 1000\n"]}],"source":["# Load expert data\n","def load_dataset(filename, limit_trajs, data_subsamp_freq):\n","    # Load expert data\n","    with h5py.File(filename, 'r') as f:\n","        # Read data as written by vis_mj.py\n","        full_dset_size = f['obs_B_T_Do'].shape[0] # full dataset size\n","        dset_size = min(full_dset_size, limit_trajs) if limit_trajs is not None else full_dset_size\n","\n","        exobs_B_T_Do = f['obs_B_T_Do'][:dset_size,...][...]\n","        exa_B_T_Da = f['a_B_T_Da'][:dset_size,...][...]\n","        exr_B_T = f['r_B_T'][:dset_size,...][...]\n","        exlen_B = f['len_B'][:dset_size,...][...]\n","\n","    print 'Expert dataset size: {} transitions ({} trajectories)'.format(exlen_B.sum(), len(exlen_B))\n","    print 'Expert average return:', exr_B_T.sum(axis=1).mean()\n"," \n","    # Stack everything together\n","    start_times_B = np.random.RandomState(0).randint(0, data_subsamp_freq, size=exlen_B.shape[0])\n","    print 'start times'\n","    print start_times_B\n","    exobs_Bstacked_Do = np.concatenate(\n","        [exobs_B_T_Do[i,start_times_B[i]:l:data_subsamp_freq,:] for i, l in enumerate(exlen_B)],\n","        axis=0)\n","    exa_Bstacked_Da = np.concatenate(\n","        [exa_B_T_Da[i,start_times_B[i]:l:data_subsamp_freq,:] for i, l in enumerate(exlen_B)],\n","        axis=0)\n","    ext_Bstacked = np.concatenate(\n","        [np.arange(start_times_B[i], l, step=data_subsamp_freq) for i, l in enumerate(exlen_B)]).astype(float)\n","\n","    assert exobs_Bstacked_Do.shape[0] == exa_Bstacked_Da.shape[0] == ext_Bstacked.shape[0]# == np.ceil(exlen_B.astype(float)/data_subsamp_freq).astype(int).sum() > 0\n","\n","    print 'Subsampled data every {} timestep(s)'.format(data_subsamp_freq)\n","    print 'Final dataset size: {} transitions (average {} per traj)'.format(exobs_Bstacked_Do.shape[0], float(exobs_Bstacked_Do.shape[0])/dset_size)\n","\n","    return exobs_Bstacked_Do, exa_Bstacked_Da, ext_Bstacked\n","  \n","exobs_Bstacked_Do, exa_Bstacked_Da, ext_Bstacked = load_dataset(\n","    data, limit_trajs, data_subsamp_freq)\n","\n","assert exobs_Bstacked_Do.shape[1] == mdp.obs_space.storage_size\n","assert exa_Bstacked_Da.shape[1] == mdp.action_space.storage_size\n","assert ext_Bstacked.ndim == 1\n","\n","# Start optimization\n","max_traj_len = max_traj_len if max_traj_len is not None else mdp.env_spec.timestep_limit\n","print 'Max traj len:', max_traj_len"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gKTCksg1yuUi"},"source":["### #3 : Reward, Value Function, Optimizer 정의하기"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":805},"colab_type":"code","executionInfo":{"elapsed":727,"status":"error","timestamp":1547714148197,"user":{"displayName":"Hyeongjoo Hwang","photoUrl":"","userId":"00454735331078182073"},"user_tz":-540},"id":"Lvg1hZlVxjqa","outputId":"d5d2437b-8c7e-4b2d-c4ca-af7f862b607b"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[95mLoading feedforward net specification\u001b[0m\n","[\n","  {\n","    \"type\": \"fc\",\n","    \"n\": 100\n","  },\n","  {\n","    \"type\": \"nonlin\",\n","    \"func\": \"tanh\"\n","  },\n","  {\n","    \"type\": \"fc\",\n","    \"n\": 100\n","  },\n","  {\n","    \"type\": \"nonlin\",\n","    \"func\": \"tanh\"\n","  }\n","]\n","\u001b[95mAffine(in=4, out=100)\u001b[0m\n","\u001b[95mNonlinearity(func=tanh)\u001b[0m\n","\u001b[95mAffine(in=100, out=100)\u001b[0m\n","\u001b[95mNonlinearity(func=tanh)\u001b[0m\n","\u001b[95mAffine(in=100, out=2)\u001b[0m\n"]},{"ename":"NotImplementedError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-12-e4d74f7dc9f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtime_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestep_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfavor_zero_expert_reward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfavor_zero_expert_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     varscope_name='TransitionClassifier')\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m vf = rl.ValueFunc(\n","\u001b[0;32m<ipython-input-7-339bd1e92a4b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obsfeat_space, action_space, hidden_spec, adam_lr, adam_steps, ent_reg_weight, enable_inputnorm, time_scale, favor_zero_expert_reward, varscope_name)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m################# Implement Here #################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m##################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: "]}],"source":["reward = TransitionClassifier(\n","    hidden_spec=policy_hidden_spec,\n","    obsfeat_space=mdp.obs_space,\n","    action_space=mdp.action_space,    \n","    adam_lr=reward_lr,\n","    adam_steps=reward_steps,\n","    ent_reg_weight=reward_ent_reg_weight,\n","    enable_inputnorm=True,    \n","    time_scale=1./mdp.env_spec.timestep_limit,\n","    favor_zero_expert_reward=bool(favor_zero_expert_reward),\n","    varscope_name='TransitionClassifier')\n","\n","vf = rl.ValueFunc(\n","    hidden_spec=policy_hidden_spec,\n","    obsfeat_space=mdp.obs_space,\n","    enable_obsnorm=obsnorm_mode != 'none',\n","    enable_vnorm=True,\n","    max_kl=vf_max_kl,\n","    damping=vf_cg_damping,\n","    time_scale=1./mdp.env_spec.timestep_limit,\n","    varscope_name='ValueFunc')\n","\n","opt = ImitationOptimizer(\n","    mdp=mdp,\n","    discount=discount,\n","    lam=lam,\n","    policy=policy,\n","    sim_cfg=policyopt.SimConfig(\n","        min_num_trajs=-1, min_total_sa=min_total_sa,\n","        batch_size=sim_batch_size, max_traj_len=max_traj_len),\n","    step_func=rl.TRPO(max_kl=policy_max_kl, damping=policy_cg_damping),\n","    reward_func=reward,\n","    value_func=vf,\n","    policy_obsfeat_fn=lambda obs: obs,\n","    reward_obsfeat_fn=lambda obs: obs,\n","    policy_ent_reg=policy_ent_reg,\n","    ex_obs=exobs_Bstacked_Do,\n","    ex_a=exa_Bstacked_Da,\n","    ex_t=ext_Bstacked)\n","\n","# Set observation normalization\n","if obsnorm_mode == 'expertdata':\n","    policy.update_obsnorm(exobs_Bstacked_Do)\n","    if reward is not None: reward.update_inputnorm(opt.reward_obsfeat_fn(exobs_Bstacked_Do), exa_Bstacked_Da)\n","    if vf is not None: vf.update_obsnorm(opt.policy_obsfeat_fn(exobs_Bstacked_Do))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fXQuPPRjy1iA"},"source":["### #4 : Training\n","\n","`logfilename`에 해당하는 파일이 이미 존재하면, 아래 코드는 실행되지 않습니다. 반복해서 트레이닝을 하고 싶을 경우 파일을 지우거나,  `logfilename`을 바꾸면 됩니다."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"WDo3gQzbk-bI"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","log = nn.TrainingLog( logfilename, [] )\n","for i in xrange(max_iter):\n","    iter_info = opt.step()\n","    log.write(iter_info, print_header=i % (20*print_freq) == 0, display=i % print_freq == 0)\n","    if save_freq != 0 and i % save_freq == 0 and log is not None:\n","        log.write_snapshot(policy, i)\n","\n","    if plot_freq != 0 and i % plot_freq == 0:\n","        # Observation from expert\n","        exdata_N_Doa = exobs_Bstacked_Do # np.concatenate([exobs_Bstacked_Do, exa_Bstacked_Da], axis=1)\n","        # Observation from agent ()\n","        pdata_M_Doa = opt.last_sampbatch.obs.stacked # np.concatenate([opt.last_sampbatch.obs.stacked, opt.last_sampbatch.a.stacked], axis=1)\n","\n","        # Plot reward\n","        _, ax = plt.subplots()\n","        idx1, idx2 = 0,1\n","        range1 = (min(exdata_N_Doa[:,idx1].min(), pdata_M_Doa[:,idx1].min()), max(exdata_N_Doa[:,idx1].max(), pdata_M_Doa[:,idx1].max()))\n","        range2 = (min(exdata_N_Doa[:,idx2].min(), pdata_M_Doa[:,idx2].min()), max(exdata_N_Doa[:,idx2].max(), pdata_M_Doa[:,idx2].max()))\n","        reward.plot(ax, idx1, idx2, range1, range2, n=100)\n","        \n","        # Plot policy samples\n","        ax.scatter(pdata_M_Doa[:,idx1], pdata_M_Doa[:,idx2], color='red',  alpha=0.5, s=1, label='apprentice')\n","        # Plot expert data\n","        ax.scatter(exdata_N_Doa[:,idx1], exdata_N_Doa[:,idx2], color='blue', alpha=0.5, s=1, label='expert')\n","\n","\n","\n","        ax.legend()\n","        plt.show()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VrU3v8nzAXgM"},"source":["## 5. Evaluation : 학습된 Policy 실행시켜보기"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"prFicyZN-0WT"},"outputs":[],"source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()\n","import os\n","os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\n","import gym\n","\n","returns = []\n","lengths = []\n","images = []\n","\n","env = gym.make(env_name)\n","n = 10\n","\n","for i_traj in xrange(n):\n","    obs = env.reset()    \n","    totalr = 0.\n","    l = 0\n","    done = False\n","    imgarr = []\n","    while not done:\n","        a = policy.sample_actions(obs[None], True)[0][0,0]        \n","        obs, r, done, _ = env.step(a)\n","        img = env.render(mode='rgb_array')\n","        imgarr.append(img)\n","        totalr += r\n","        l += 1\n","        if l>=200:\n","          break;\n","          \n","    returns.append(totalr)\n","    lengths.append(l)\n","    images.append(imgarr)\n","    print(\"-- Episode : %2d/%d | Return : %4.3f | Length : %4d \"%(i_traj+1, n, totalr, l))\n","    \n","print(\"**** Return : Avg=%.3f , Std=%.3f\"%(np.array(returns).mean(), np.array(returns).std()))"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"U2_a4m7MF0Jb"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from matplotlib.pyplot import imshow\n","from matplotlib import animation\n","from JSAnimation import IPython_display\n","from IPython.display import display, HTML\n","\n","def plot_movie_mp4(image_array):\n","    dpi = 72.0\n","    xpixels, ypixels = image_array[0].shape[0], image_array[0].shape[1]\n","    fig = plt.figure(figsize=(ypixels/dpi, xpixels/dpi), dpi=dpi)\n","    im = plt.figimage(image_array[0])\n","\n","    def animate(i):\n","        im.set_array(image_array[i])\n","        return (im,)\n","\n","    anim = animation.FuncAnimation(fig, animate, frames=len(image_array))\n","    display(HTML(anim.to_html5_video()))\n","\n","def plot_movie_js(image_array):\n","    dpi = 10.0\n","    xpixels, ypixels = image_array[0].shape[0], image_array[0].shape[1]\n","    fig = plt.figure(figsize=(ypixels/(dpi), xpixels/(dpi)), dpi=dpi)\n","    im = plt.figimage(image_array[0])\n","\n","    def animate(i):\n","        im.set_array(image_array[i])\n","        return (im,)\n","    \n","    anim = animation.FuncAnimation(fig, animate, frames=len(image_array))\n","    display(IPython_display.display_animation(anim))\n","    \n","% matplotlib inline\n","plot_movie_js(images[7])\n","# plot_movie_mp4(images[6])"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"R9etmrRcy-T4"},"source":["## 실습과제\n","\n","1. 현재 실습 자료는 `CartPole-v0` 환경을 학습하도록 되어 있습니다. 먼저 `CartPole` Task가 잘 학습되는지 확인해보세요.\n","\n","2. `MountainCar-v0` 환경을 쓰는 코드를 참고하여 학습하도록 바꿔보세요.\n","\n","> **Hint** : `CartPole-v0`의 Expert Trajectory는 `expert_trajs/trajs_cartpole.h5`에 있습니다.\n","\n","3. 학습이 잘 안된다면, 그 이유는 무엇일지 생각해보고 학습이 잘 되도록 하이퍼파라메터와 코드를 수정해봅시다.\n","\n","> **Hint** : `CartPole` Task는 가능한 한 Episode가 끝나지 않도록 오래 유지해야하는 Task고, `MountainCar` Task는 가능한 한 Episode를 빨리 끝내야 하는 Task라는 점에서 차이가 있습니다.\n"]}]}